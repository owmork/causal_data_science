[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Literature",
    "section": "Overview",
    "text": "Overview\nDistinguishing causal relationships from simple correlation is what commonly used approaches in business analytics often fall short of. In this course, we will provide you with the skill set to answer questions like\n\n\nwhat happens to \\(Y\\) if we do \\(X\\)?\n\n\nwas it \\(X\\) that caused \\(Y\\) to change?\n\n\nIntroducing you to causal inference with the help of data science will allow you to carry out state-of-the-art causal analyses by yourself and extrapolate causal knowledge across different business contexts and various management areas."
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Literature",
    "section": "Objectives",
    "text": "Objectives\nAfter completing this module, students will be able to:\n\nUnderstand the difference between “correlation” and “causation”\nUnderstand the shortcomings of current correlation-based approaches\nDevelop causal knowledge relevant for specific data-driven decisions\nDiscuss the conceptual ideas behind various causal data science tools and algorithms\nCarry out state-of-the-art causal data analyses"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Literature",
    "section": "Instructors",
    "text": "Instructors\n   Lecture: Christoph Ihl\n   Tutorial: Oliver Mork"
  },
  {
    "objectID": "index.html#details",
    "href": "index.html#details",
    "title": "Literature",
    "section": "Details",
    "text": "Details\n   Lecture: Monday, 11.30 - 13.00\n   Tutorial: Tuesday, 15.00 - 16.30 + 16.45 - 18.15"
  },
  {
    "objectID": "index.html#primary",
    "href": "index.html#primary",
    "title": "Literature",
    "section": "Primary",
    "text": "Primary\n\nDing, Peng (2023). A First Course in Causal Inference. arXiv preprint arXiv:2305.18793.\nFacure, Matheus (2023). Causal Inference in Python - Applying Causal Inference in the Tech Industry. O’Reilly Media.\nHuber, Martin (2023). Causal analysis: Impact evaluation and Causal Machine Learning with applications in R. MIT Press, 2023.\nNeal, Brady (2020). Introduction to causal inference from a Machine Learning Perspective. Course Lecture Notes (draft)."
  },
  {
    "objectID": "index.html#secondary",
    "href": "index.html#secondary",
    "title": "Literature",
    "section": "Secondary",
    "text": "Secondary\n\nAngrist, J. D., & Pischke, J. S. (2014). Mastering metrics: The path from cause to effect. Princeton university press.\nCunningham, Scott (2021). Causal Inference: The Mixtape, New Haven: Yale University Press.\nGertler, Paul J., et al. (2016). Impact evaluation in practice. World Bank Publications.\nHernán Miguel A., and Robins James M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nHuntington-Klein, Nick (2021). The effect: An introduction to research design and causality. Chapman and Hall/CRC.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge University Press.\nMullainathan, Sendhil, and Jann Spiess. (2017). Machine Learning: An Applied Econometric Approach. Journal of Economic Perspectives, 31(2): 87–106.\nPearl, Judea, and Dana Mackenzie (2018). The Book of Why. Basic Books, New York, NY.\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell (2016). Causal Inference in Statistics: A Primer. John Wiley & Sons, Inc., New York, NY.\nPeters, Jonas, Dominik Janzing, and Bernhard Schölkopf (2017). Elements of causal inference: foundations and learning algorithms. The MIT Press."
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html",
    "href": "content/course_weeks/week_06/week_6.html",
    "title": "6 - Effect Heterogeneity",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#slides-recap",
    "href": "content/course_weeks/week_06/week_6.html#slides-recap",
    "title": "6 - Effect Heterogeneity",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#practical-example",
    "href": "content/course_weeks/week_06/week_6.html#practical-example",
    "title": "6 - Effect Heterogeneity",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#assignment",
    "href": "content/course_weeks/week_06/week_6.html#assignment",
    "title": "6 - Effect Heterogeneity",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html",
    "href": "content/course_weeks/week_09/week_9.html",
    "title": "9 - Synthetic Control",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Synthetic Control"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#slides-recap",
    "href": "content/course_weeks/week_09/week_9.html#slides-recap",
    "title": "9 - Synthetic Control",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Synthetic Control"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#practical-example",
    "href": "content/course_weeks/week_09/week_9.html#practical-example",
    "title": "9 - Synthetic Control",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Synthetic Control"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#assignment",
    "href": "content/course_weeks/week_09/week_9.html#assignment",
    "title": "9 - Synthetic Control",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Synthetic Control"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html",
    "href": "content/course_weeks/week_05/week_5.html",
    "title": "5 - Double Machine Learning",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#slides-recap",
    "href": "content/course_weeks/week_05/week_5.html#slides-recap",
    "title": "5 - Double Machine Learning",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#practical-example",
    "href": "content/course_weeks/week_05/week_5.html#practical-example",
    "title": "5 - Double Machine Learning",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#assignment",
    "href": "content/course_weeks/week_05/week_5.html#assignment",
    "title": "5 - Double Machine Learning",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html",
    "href": "content/course_weeks/week_03/week_3.html",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "",
    "text": "Slides - Week 3\nLast week, our attention was dedicated to identification with the aid of graphical models. This week, our focus shifts to establishing the connection between causal identification and estimation. This section delves into randomized experiments, widely regarded as the gold standard for causal inference, and linear regression, recognized as the workhorse of causal inference. Randomization is the best way to make treatment and control group comparable, allowing us to go from mere association to causation. Linear regression equips us with the tools to extract statistical information necessary to assess the level of confidence level we have in our estimates.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#slides-recap",
    "href": "content/course_weeks/week_03/week_3.html#slides-recap",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "",
    "text": "Slides - Week 3\nLast week, our attention was dedicated to identification with the aid of graphical models. This week, our focus shifts to establishing the connection between causal identification and estimation. This section delves into randomized experiments, widely regarded as the gold standard for causal inference, and linear regression, recognized as the workhorse of causal inference. Randomization is the best way to make treatment and control group comparable, allowing us to go from mere association to causation. Linear regression equips us with the tools to extract statistical information necessary to assess the level of confidence level we have in our estimates.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#effect-of-introducing-a-new-feature",
    "href": "content/course_weeks/week_03/week_3.html#effect-of-introducing-a-new-feature",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "Effect of introducing a new feature",
    "text": "Effect of introducing a new feature\nLet’s jump right into an example of a randomized experiment to grasp the advantages over observational studies. Imagine you’re part of the business analytics team at a software company. Your task is to analyze the impact of introducing a new feature on the time users spend within the app. While the exact nature of this feature isn’t specified, its objective is to boost users’ daily app engagement, thereby driving up ad revenue.\nAs usual, we need to download and load the data first.\n\ndf &lt;- readRDS(\"new_feature.rds\")\nprint(df)\n\n# A tibble: 1,000 × 3\n   ip              new_feature time_spent\n   &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;\n 1 153.168.98.222            0      71.4 \n 2 119.218.12.199            1      76.1 \n 3 208.197.60.31             0      68.4 \n 4 235.143.38.46             1      73.1 \n 5 86.56.28.141              0      70.4 \n 6 51.111.112.107            0      45.1 \n 7 165.165.131.102           0     106.  \n 8 98.178.26.235             0      14.0 \n 9 63.223.57.208             0       9.22\n10 149.249.67.110            1      79.5 \n# ℹ 990 more rows\n\n\nThe data contains three columns, ip, new_feature and time_spent. Values in the ip column identify the user, new_feature indicates who has the new feature in the app and time_spent is the average daily minutes a user spent on the app following the start of the randomized experiment.\nIn the lecture, you learned that when treatment is randomly assigned, potential outcomes are independent of the treatment, and any systematic difference observed in the outcomes can be attributed solely to the treatment. Consequently, we can simply compare the means of the respective groups to estimate the ATE.\n\\[\n\\hat{\\tau}_{\\text{ATE}} = \\frac{1}{\\sum_i D_i} \\sum_i D_i Y_i - \\frac{1}{\\sum_i (1 - D_i)} \\sum_i (1 - D_i) Y_i\n\\] First, we will go through the manual calculation of all we need to estimate the average treatment effect and asses the statistical significance. Afterwards, we will use linear regression, which saves us a lot of work and will be incredibly important in th weeks to come.\nManual calculation\nUsing the tidyverse allows us a versatile and flexible approach that we can extend later. We group by the treatment column and compute the group-specific means.\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Grouping by the treatment column and calculating the mean of the outcome\n# column by group\ngroup_means &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(mean_time_spent = mean(time_spent)) |&gt; \n  ungroup()\n\n# Show group-specific means\nprint(group_means)\n\n# A tibble: 2 × 2\n  new_feature mean_time_spent\n        &lt;dbl&gt;           &lt;dbl&gt;\n1           0            61.0\n2           1            63.5\n\n\nTo get the difference, we need to add one more step.\n\n# Compute the difference in means\ndiff(group_means$mean_time_spent)\n\n[1] 2.5\n\n\nUp to this point, this should sound familiar. However, although we have an estimate for the ATE, we lack certainty regarding the confidence in this estimate. We also cannot ascertain whether the difference between the mean outcomes of the treatment and control groups is solely due to chance or holds statistical significance.\nStandard deviation and standard error of the mean\nIn addition to the average, another crucial statistical measure, upon which all subsequent methods of statistical inference rely, is the variance. This measure indicates the extent of dispersion/spread in a random variable. Variance of a random variable \\(X\\) calculated by:\n\\[\n\\sigma_X^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left(X_i - \\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2\n\\] The standard deviation obtains as the square root of the variance. Unlike the variance it expressed in the same unit as the data.\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\] Let’s add the standard deviation to our summary table. We can use the built-in function sd(). If we would like to compute the variance, we could use var().\n\n# Grouping by the treatment column and calculating the mean and the standard\n# deviation of the outcome\n# column by group\ngroup_stats &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(\n    mean_time_spent = mean(time_spent),\n    # add standard deviation\n    sd_time_spent = sd(time_spent)\n    ) |&gt; \n  ungroup()\n\n# Show group-specific statistics\nprint(group_stats)\n\n# A tibble: 2 × 3\n  new_feature mean_time_spent sd_time_spent\n        &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1           0            61.0          30.5\n2           1            63.5          28.0\n\n\nKnowing the group-specific means and standard deviations brings us one step closer to answer the question whether the treatment effect is large enough to be considered statistically significant. For each group, we also need to know the standard error of the mean. It can be easily confused with the standard deviation, but there is one large difference: the formula for the standard error takes the sample size into account. A larger sample gives us more confidence in the estimate.\n\n\n\n\n\n\nTip\n\n\n\nStandard deviation measures the dispersion of individual data points from the mean, while standard error of the mean quantifies the precision of the sample mean estimate, reflecting how much the sample mean is likely to vary from the population mean.\n\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt n}\n\\] Because there is no built-in function in R, we can easily define a function (we could use external packages, but for the sake of demonstration, we define it ourselves):\n\n\n\n\n\n\nNote\n\n\n\nIn many cases, you will find functions needed for your analysis in base R or by loading external packages. But sometimes, you want to do something so specific, that no one has done it before. Then, you define your custom function by using function(arg1, arg2, ...).\n\n\n\n# Custom function for the standard error\nse &lt;- function(x) {\n  # Standard deviation divided by the square root of the sample size\n  sd(x) / sqrt(length(x))\n}\n\nWe need to update our table with the statistics by adding the number of observations \\(n\\) per group (using the function n()) and calculating the standard error of the mean.\n\n# Grouping by the treatment column and calculating the mean, standard deviation,\n# sample size, and standard error of the mean\n# column by group\ngroup_stats &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(\n    mean_time_spent = mean(time_spent),\n    # add standard deviation\n    sd_time_spent = sd(time_spent),\n    # add sample size\n    n_obs = n()\n    ) |&gt; \n  ungroup() |&gt; \n  # Add a new column with the standard error of the mean\n  mutate(se_time_spent = sd_time_spent / sqrt(n_obs))\n\n# Show group-specific statistics\nprint(group_stats)\n\n# A tibble: 2 × 5\n  new_feature mean_time_spent sd_time_spent n_obs se_time_spent\n        &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;\n1           0            61.0          30.5   590          1.26\n2           1            63.5          28.0   410          1.38\n\n\nConfidence interval\nA profoundly useful mathematical theorem is the Central Limit Theorem, which states that the distribution of sample means tends towards normality. Understanding the mean and standard error of the mean equips us with all the necessary information to comprehend and illustrate the distribution.\nHere, we plot the standard normal distribution, characterized by a mean of zero and a standard deviation of one. By delineating the mass of the distribution limited by specific values, we can construct confidence intervals around our estimates. These intervals provide a range within which the true mean likely lies, with a chosen probability. While 95% confidence intervals are most commonly reported, values such as 90% or 99% are also frequently used For instance, approximately 95% of the area under the normal distribution falls within approximately two standard deviations below and above the mean.\n\nCodeggplot(data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(-3, 3)) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = ggthemr::swatch()[2],\n                alpha = .3,\n                xlim = c(qnorm(.005), qnorm(.995))) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = ggthemr::swatch()[2],\n                alpha = .5,\n                xlim = c(qnorm(.025), qnorm(.975))) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = ggthemr::swatch()[2],\n                alpha = .9,\n                xlim = c(qnorm(.05), qnorm(.95))) +\n  scale_x_continuous(limits = c(-3, 3), breaks = seq(-3, 3, .5)) +\n  geom_vline(xintercept = qnorm(.005), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.995), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.025), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.975), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.05), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.95), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  theme(panel.grid = element_blank()) +\n  ggtitle(\"Standard Normal Distribution with 90%, 95% and \\n99% Confidence Interval\") + \n  labs(x = \"z-score\", y = \"Density\")\n\n\n\n\n\n\n\nWe can use this directly to compute the confidence intervals around our estimated group means. For the 95% confidence interval, we have \\(alpha = 1 - 0.95 = 0.05\\) and can get z-value using using qnorm(.975)\n\\[\n\\hat{\\mu} \\pm z_{\\alpha/2} * SE\n\\]\n\n\n\n\n\n\nTip\n\n\n\nConfidence level and significance level are sometimes confused.\n\nconfidence level = 1- significance level = 95%\nsignificance level (or alpha) = 1 - confidence level = 5%\n\n\n\nAdding the confidence intervals to our table:\n\n# Grouping by the treatment column and calculating the mean, standard deviation,\n# sample size, and standard error of the mean\n# column by group\ngroup_stats &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(\n    mean_time_spent = mean(time_spent),\n    # add standard deviation\n    sd_time_spent = sd(time_spent),\n    # add sample size\n    n_obs = n()\n    ) |&gt; \n  ungroup() |&gt; \n  # Add a new column with the standard error of the mean\n  mutate(\n    se_time_spent = sd_time_spent / sqrt(n_obs),\n    # adding confidence intervals (lower and upper bound)\n    ci_lb = mean_time_spent - qnorm(.975) * se_time_spent,\n    ci_ub = mean_time_spent + qnorm(.975) * se_time_spent\n    )\n\n# Show group-specific statistics\nprint(group_stats)\n\n# A tibble: 2 × 7\n  new_feature mean_time_spent sd_time_spent n_obs se_time_spent ci_lb ci_ub\n        &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1           0            61.0          30.5   590          1.26  58.6  63.5\n2           1            63.5          28.0   410          1.38  60.8  66.2\n\n\nComparing the confidence intervals across the groups, we see some overlap. However, that does not yet mean that they are not statistically significant, although it is usually a good indication. And, in fact, if they did not overlap at all, you would be able to conclude that they are statistcally different.\n\nCodeu0 &lt;- group_stats[group_stats$new_feature == 0, ]$mean_time_spent\nu1 &lt;- group_stats[group_stats$new_feature == 1, ]$mean_time_spent\n\no0 &lt;- group_stats[group_stats$new_feature == 0, ]$se_time_spent\no1 &lt;- group_stats[group_stats$new_feature == 1, ]$se_time_spent\n\nci_lb0 &lt;- group_stats[group_stats$new_feature == 0, ]$ci_lb\nci_lb1 &lt;- group_stats[group_stats$new_feature == 1, ]$ci_lb\n\nci_ub0 &lt;- group_stats[group_stats$new_feature == 0, ]$ci_ub\nci_ub1 &lt;- group_stats[group_stats$new_feature == 1, ]$ci_ub\n\n\nggplot(data.frame(x = c(55, 70)), aes(x)) +\n  stat_function(fun = dnorm,\n                color = ggthemr::swatch()[3],\n                alpha = .5,\n                args = list(mean = u0, sd = o0),\n                geom = \"line\") +\n    stat_function(fun = dnorm,\n                color = ggthemr::swatch()[2],\n                alpha = .5,\n                args = list(mean = u1, sd = o1),\n                geom = \"line\") +\n  stat_function(fun = dnorm,\n                fill = ggthemr::swatch()[3],\n                alpha = .5,\n                args = list(mean = u0, sd = o0),\n                geom = \"area\",\n                xlim = c(ci_lb0, ci_ub0)) +\n  stat_function(fun = dnorm,\n                fill = ggthemr::swatch()[2],\n                alpha = .5,\n                args = list(mean = u1, sd = o1),\n                geom = \"area\",\n                xlim = c(ci_lb1, ci_ub1)) +\n  ggtitle(\"Distribution of outcomes by group\") + \n  labs(x = \"Minutes\", y = \"Density\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\nHypothesis testing\nTo comprehensively address the question, “Is the difference statistically significant from zero (or any other value)?” we rely on hypothesis testing. Leveraging the mathematical principle that the sum or difference of two independent normal distributions also follows a normal distribution, we can check whether the confidence interval of the ATE contains zero. This occurs because the ATE is derived as the difference between the distributions of the two groups.\nLet’s explicitly formulate what we test:\n\\(H_0: \\mu_1 - \\mu_0 = 0\\)\nExpressed in formulas, the mean of the resulting normal distribution is\n\\[\n\\mu_{diff} = \\mu_1 - \\mu_0\n\\]\nand the standard deviation is (please note, that it is the sum and not the difference)\n\\[\nSE_{diff} = \\sqrt{SE_1^2 + SE_0^2}.\n\\]\n\n# Get means\nmu_0 &lt;- group_stats[group_stats$new_feature==0, ]$mean_time_spent\nmu_1 &lt;- group_stats[group_stats$new_feature==1, ]$mean_time_spent\n\n# Get standard errors\nse_0 &lt;- group_stats[group_stats$new_feature==0, ]$se_time_spent\nse_1 &lt;- group_stats[group_stats$new_feature==1, ]$se_time_spent\n\n\n# Mean\ndiff_mu &lt;- mu_1 - mu_0\ndiff_mu\n\n[1] 2.5\n\n\n\n# Standard error\ndiff_se &lt;- sqrt(se_1^2 + se_0^2)\ndiff_se\n\n[1] 1.9\n\n\nLet’s make another plot: the distribution of the difference. We can already do the visual hypothesis test.\n\nCodeggplot(data.frame(x = c(-10, 10)), aes(x)) +\n  stat_function(fun = dnorm,\n                alpha = .5,\n                args = list(mean = diff_mu, sd = diff_se),\n                geom = \"line\") +\n  stat_function(fun = dnorm,\n                alpha = .5,\n                args = list(mean = diff_mu, sd = diff_se),\n                geom = \"area\",\n                xlim = c(diff_mu - qnorm(.975) * diff_se, diff_mu + qnorm(.975) * diff_se)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  ggtitle(\"Visual hypothesis testing with 95% CI\") + \n  labs(x = \"Minutes\", y = \"Density\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\nWhat we can infer from the plot already, can be confirmed by computing the confidence intervals with the known formula.\n\ndiff_ci_lb = diff_mu - qnorm(.975) * diff_se\ndiff_ci_ub = diff_mu + qnorm(.975) * diff_se\n\nprint(c(\"Lower CI\" = diff_ci_lb, \"Upper CI\" = diff_ci_ub))\n\nLower CI Upper CI \n    -1.2      6.2 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNull hypothesis: the null hypothesis \\(H_0\\) is the claim the the effect being studied does not exists. The effect is a “null” effect.\n\n\nQuestion: Do we reject the null hypothesis?\n\nAnswer\nNo. The ATE is not different from zero.\n[Optional read] p-value\nFrom a slightly different perspective, we can also compute the p-value to tell us the confidence in our estimates. The question we ask is then: What is the lowest significance level that results in rejecting the null hypothesis?\nFirst, we need to compute the t-statistic:\n\\[\n\\begin{align}\nt_{ATE} = \\frac{\\mu_1 - \\mu_0}{\\sqrt{SE_1^2 + SE_0^2}} &= \\frac{\\mu_{diff}}{SE_{diff}}\n\\end{align}\n\\]\n\nt_stat = (diff_mu - 0) / diff_se\n\nKnowing the t-statistic, we provide it in to the following command.\n\n(1 - pnorm(t_stat)) * 2\n\n[1] 0.18\n\n\npnorm(q) returns the integral from \\(-\\infty\\) to \\(q\\), i.e. the mass up to the value \\(q\\). Because we are using the two-sided test, we need to multiply by 2.\nThe p-value expresses the probability with which the null hypothesis is correct. For a rigorous statistical analysis, you usually expect a value of lower than 5% (sometimes 10% or 1%). Please note that this is just a different perspective and the common levels for p-values directly derive from the common values for confidence intervals.\nLinear regression\nAll the steps required to assess the hypothesis regarding the statistical significance of the average treatment effect can be condensed into a single step: running a linear regression. Regression functions as the workhorse for causal inference, serving as the cornerstone for more advanced techniques. Through the simple command lm(), regression seamlessly provides all the necessary statistical output for our analysis. Furthermore, it allows the implementation of more complex analyses beyond single-value treatments with ease.\nThe generic functional form of a singe-valued regression without any covariate is as follows. \\(Y\\) is the outcome variable, \\(D\\) is treatment variable, and \\(e\\) is the error term. We are interested in estimating the coefficients denoted by \\(\\beta\\), particularly \\(\\beta_1\\), which is the treatment effect.\n\\[\nY_i = \\beta_0 + \\beta_1*D+e_i\n\\]\nWe’ve already encountered the syntax before, but if you do not recall: y ~ x, where \\(y\\) is the outcome and \\(x\\) the explaining variable. We specify the respective variables in our data: we want to explain the effect of new_feature on time_spent, fit the model using lm() and assign it to a variable. Then, for a good summary of the estimates and statistical measures, we apply the summary() function to the fitted object.\n\nlr &lt;- lm(time_spent ~ new_feature, data = df)\nsummary(lr)\n\n\nCall:\nlm(formula = time_spent ~ new_feature, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-63.53 -19.83   1.11  19.08  96.01 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    61.04       1.21   50.30   &lt;2e-16 ***\nnew_feature     2.49       1.90    1.32     0.19    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30 on 998 degrees of freedom\nMultiple R-squared:  0.00173,   Adjusted R-squared:  0.000732 \nF-statistic: 1.73 on 1 and 998 DF,  p-value: 0.188\n\n\nTake a look at the output and try to find the statistical measures we have manually calculated before.\n\nGroup means\nTreatment effect\nStandard error of the estimates\nConfidence interval\nHypothesis testing\n\nThe output contains a p-value but does not contain confidence intervals because it is agnostic to the confidence level. However, if you are interested in confidence intervals for a specific confidence level, you can use the confint(model, level) function to calculate them. Or just simply calculate them by hand as you have learned.\n\nconfint(lr, level = .95)\n\n            2.5 % 97.5 %\n(Intercept)  58.7   63.4\nnew_feature  -1.2    6.2\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlthough the average treatment effect is equal, a minor difference in the statistics may arise. This difference stems from approximating the t-distribution with the normal distribution, a common practice when dealing with more than 100 observations. The benefit of utilizing the normal distribution is not needing to specify the degrees of freedom (sample size minus estimated parameters).",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#ab-test-at-streaming-service",
    "href": "content/course_weeks/week_03/week_3.html#ab-test-at-streaming-service",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "A/B test at streaming service",
    "text": "A/B test at streaming service\nConsider the following scenario: you are a data scientist at a music streaming company. Naturally, you’re keen to engage users for as long as possible and encourage them to become avid users of your streaming app. Your colleagues from the department responsible for recommendation algorithms have developed a new fancy algorithm which aims to enhance the daily number of minutes of music listening by offering particularly well tailored recommendations. Following common practice of A/B testing, you randomly assign your users into a treatment group (new algorithm) and a control group (no new algorithm). Download the data here.\n\nAs a pre-check, test whether your random assignment worked well by comparing the daily number of minutes users listened to music per group.\nCompute the average treatment effect using a linear regression.\n\nTake a look at the summary statistics of the regression and answer the following questions:\n\nWhat is the estimated treatment effect?\nWhat is the null hypothesis and do you reject or do you confirm it? At what significance level?\nWhat doest \\(R^2\\) describe?\nWhat is the intercept and what does it describe?\n\n\n\n\nCalculate 90%, 95% and 99% confidence intervals for the treatment effect and compare them. Which intervals are larger? And what question do they answer?\n\nAfter one month, you are asked to take a different outcome into focus - the rate of subscription cancellations. For each user you find the information whether he/she cancelled his/her subscription in the column cancel: 1 if cancelled, and 0 if not. Because other than minutes, cancel is a binary variable (it can only be 0 or 1), you need to apply a different form of regression: the logistic regression. There is lot of theory behind it, which we will not cover now. But feel free to read here about it.\n\nHowever, what you need to understand when using logistic regression is that the interpretation of the coefficients changes because you are acutally fitting\n\\[\nlogit(P(Y=1)) = logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*D + \\epsilon,\n\\]\nwhich is different from the linear regression. We know relate a one-unit increase in \\(D\\) (treated or not-treated) with an increase in \\(logit(p)\\). Therefore, you can read the coefficient as follows:\nIncreasing \\(D\\) by 1 will result in a \\(\\beta_1\\) increase in \\(logit(p)\\), which is approximately an \\(exp(\\beta_1)-1\\) increase in the odds of \\(Y = 1\\).\nInstead of lm() use the following command: glm(formula, family=binomial(link='logit'), data), where you replace formula with the known syntax.\n\nFit the model and return the summary statistics.\nInterpret the coefficients.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#assignment",
    "href": "content/course_weeks/week_03/week_3.html#assignment",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "Assignment",
    "text": "Assignment\nAccept the Week 3 - Assignment and follow the same steps as last week and as described in the organization chapter.\nWill be published 30 April 2024 20:00.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html",
    "href": "content/course_weeks/week_10/week_10.html",
    "title": "10 - Regression Discontinuity",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html#slides-recap",
    "href": "content/course_weeks/week_10/week_10.html#slides-recap",
    "title": "10 - Regression Discontinuity",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html#practical-example",
    "href": "content/course_weeks/week_10/week_10.html#practical-example",
    "title": "10 - Regression Discontinuity",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html#assignment",
    "href": "content/course_weeks/week_10/week_10.html#assignment",
    "title": "10 - Regression Discontinuity",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html",
    "href": "content/course_weeks/week_11/week_11.html",
    "title": "11 - Causal Mediation",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html#slides-recap",
    "href": "content/course_weeks/week_11/week_11.html#slides-recap",
    "title": "11 - Causal Mediation",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html#practical-example",
    "href": "content/course_weeks/week_11/week_11.html#practical-example",
    "title": "11 - Causal Mediation",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html#assignment",
    "href": "content/course_weeks/week_11/week_11.html#assignment",
    "title": "11 - Causal Mediation",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/optional_read/stats.html",
    "href": "content/optional_read/stats.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Now we will delve into some statistical concepts, that are the foundation for statistical modeling processes used in causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to read here.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Statistical Concepts"
    ]
  },
  {
    "objectID": "content/optional_read/stats.html#footnotes",
    "href": "content/optional_read/stats.html#footnotes",
    "title": "Statistical Concepts",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://ourworldindata.org/human-height↩︎",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Statistical Concepts"
    ]
  },
  {
    "objectID": "content/organization/submission.html",
    "href": "content/organization/submission.html",
    "title": "GitHub & Submission",
    "section": "",
    "text": "For each week, there will be assignments for you to solve writing and rendering R programming code in .qmd files using RStudio and uploading these files via GitHub1. We’ll go through it step by step, but briefly summarized, .qmd files allow writing text and code and rendering presentable .html files based on the Quarto2 infrastructure and GitHub is is a hosting platform for so called repositories, which typically consists of data and code. But don’t worry, after the instructions on this page, it will get a lot clearer.\n\n\n\n\n\n\nTip\n\n\n\nIn the following I will guide your through the necessary steps. Some details in the screenshot might not be identical and slightly deviate, but you should be able to follow the general workflow.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#instructions",
    "href": "content/organization/submission.html#instructions",
    "title": "GitHub & Submission",
    "section": "",
    "text": "For each week, there will be assignments for you to solve writing and rendering R programming code in .qmd files using RStudio and uploading these files via GitHub1. We’ll go through it step by step, but briefly summarized, .qmd files allow writing text and code and rendering presentable .html files based on the Quarto2 infrastructure and GitHub is is a hosting platform for so called repositories, which typically consists of data and code. But don’t worry, after the instructions on this page, it will get a lot clearer.\n\n\n\n\n\n\nTip\n\n\n\nIn the following I will guide your through the necessary steps. Some details in the screenshot might not be identical and slightly deviate, but you should be able to follow the general workflow.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#initializing-github-github-desktop",
    "href": "content/organization/submission.html#initializing-github-github-desktop",
    "title": "GitHub & Submission",
    "section": "Initializing GitHub & GitHub Desktop",
    "text": "Initializing GitHub & GitHub Desktop\n\nCreate a free GitHub account. If you already have a GitHub account, you can skip this step.\nDownload and install GitHub Desktop and connect it to your account (Sign into GitHub.com). GitHub Desktop is a graphical user interface, which allows you to sync your local code changes with your online repository.\nCheck if git is installed on your system. It should already be the case but you can check it by opening RStudio, going to the terminal pane and entering which git. It should output the file path to git on your system. If you don’t get the expected result, download and install git.\nAccept the assignment and follow through the steps to enter the virtual classroom. This is the assignment for the first week. For each week, you will be provided with a new assignment, which will be linked in the respective chapter. \nAfter a while (refresh your page), you will get the link to your repository, which is currently free of any content but contains the framework to publish your solutions at a later stage. Click on that link, which is highlighted in blue.\nClone your repository, i.e. you create a local version on your hard drive. Until now, your repository is online hosted on GitHub, but of course, you need a local version to open the files in RStudio and add your solutions and code to your repository. It will open GitHub Desktop (sign into your account if you haven’t done already) and lets you set the path on your local drive. \nAfter setting the path and confirming by clicking Clone, you will be asked how you are planning “to use the fork”. Please make sure to select For my own purposes. This is important, because you do not want to change the original repository, but have your own version of it that you will modify (if you look carefully, you see that the links for the two options differ). \nNow go into your previously specified path and check whether the local version of the repository exists. If not, please read the steps again and check what you missed out.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#implementing-your-solution",
    "href": "content/organization/submission.html#implementing-your-solution",
    "title": "GitHub & Submission",
    "section": "Implementing your solution",
    "text": "Implementing your solution\nYou will find a couple of files in the repository. Let’s discuss some of them:\n\nss24_cds_week_1.Rproj: a R project file. It opens RStudio and mainly sets the correct working environment, which e.g. helps you load and save files.\nplayground.R : a classical R script. Use it to prototype your code and solutions.\nsubmission.qmd : a file that contains a combination of markdown and executable code cells. Here, you will type in your final solutions and render a .html that will be graded.\n.csv files: data you will need for the lab session and assignments\n\nThese files, that are typically found in GitHub repositories, you can ignore for now:\n\nREADME.md : to describe what the purpose of the repository is.\n.gitignore : to set what files should be excluded from pushing changes.\n\nNow, let’s simulate the workflow you will be going through to submit your solutions. We will just make a simple change and update the online version of the repository. But it is the exact workflow needed for all your submissions in the coming weeks.\n\nYou should be already in your local repository. Open the file ss24_cds_week_1.Rproj. This will automatically open RStudio, and your current working environment will be inside this project. That means everything you save will be auto saved to this folder (unless you tell RStudio to save something somewhere else. Have a look at the files tab in the bottom right hand corner. Most files you click will be opened up as text files in the RStudio editor.\nNow open submission.qmd and take a look at the assignments. The first one is already solved, so let’s take a look at the second one. Obviously, the questions are only for the purpose of demonstration, but let’s assume the question would be difficult.\nOpen playground.R to try to find the solution to the question. Here, you will probably quickly come up with the right solution, which is to simply type 2+2 and let R give you the result.\nTransfer the command to submission.qmd and paste it into an executable code chunk. At the top, you find a green C button with a plus to Insert a new code chunk (please also see the shortcut). Save the file.\nThen click on Render which will create the .html file which is your actual submission. On most devices, the file will automatically be opened. If not, find the file in your folder and open it. Check whether you see the intended output. It should look like this file, but with the new solution added. If it looks very different from the example, please check whether in the header of your .qmd file embed-resources: true is enabled.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#uploading-your-solution",
    "href": "content/organization/submission.html#uploading-your-solution",
    "title": "GitHub & Submission",
    "section": "Uploading your solution",
    "text": "Uploading your solution\n\nIf you are happy with your solution and the rendered file, you still have to upload it to GitHub. Otherwise, we can’t see it and are not able to grade it. Therefore, Go back to GitHub Desktop. You should see something similar to the image below. You can see what you changed in the .qmd and the accompanied html should be different as well. There might be a lot more changes that you expect because a lot of stuff ran in the background when rendering.\n\n\n\n\nCommit\n\n\n\nNow you still need to push your changes to GitHub. First, commit your changes at the left bottom by providing a short description of what you have changed and click on Commit to main. Now you can push to origin (you might have to Fetch origin beforehand).\n\n 3. Take a look at your online repository and check if everything was successfully uploaded.\n\n\n\n\n\n\nTip\n\n\n\nFor each assignment, there will be a deadline and you can only push changes to GitHub until that deadline. As long as it is before the deadline, you can make as many changes as you want.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#submission",
    "href": "content/organization/submission.html#submission",
    "title": "GitHub & Submission",
    "section": "Submission",
    "text": "Submission\nSubmit your credentials and GitHub user name via the following form. If you do not submit your information, we won’t be able to evaluate your assignments. Please fill it out by Monday, 22 April 2024.\nWird geladen…\n\n\n\n\n\n\nSummary: How to successfully submit\n\n\n\n\nFill out the form. (once)\nWrite your solutions down in a .qmd file and render.\nCommit and push your changes to your GitHub\n\n\n\n\n\n\nCommit",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#footnotes",
    "href": "content/organization/submission.html#footnotes",
    "title": "GitHub & Submission",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/↩︎\nhttps://quarto.org/↩︎",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/mattermost.html",
    "href": "content/organization/mattermost.html",
    "title": "Mattermost",
    "section": "",
    "text": "In the course of the next chapters, we will do a lot of coding and errors will occur all the time. That is nothing you should be afraid of and in fact, dealing with errors is an elementary component in programming in data science.\nIn most cases, other people from around the world have had similar problems and you will find the right solution to your problem by just googling it. Two great resources to help you are StackOverflow and RStudio Community. Please try to do that as a first step when you run into an error.\nIf you have any questions about the class content, coding problems and other challenges, please use our Mattermost channel, so that everyone can benefit from the discussions. Please help each other, try to answer emerging questions and actively engage in the channel. Questions, that are not directly related to the class content, can be sent to me.\nFollow these steps to join the channel:\n\nGo on https://communicating.tuhh.de/\nClick Click here to sign in\nClick the Button GitLab\nYou may need to login to GitLab with your Kerberos/LDAP data (e.g. cba1020 and your password) on the following page and/or authorize once for Mattermost to access GitLab. You may also need to accept the terms.\nAfter accessing Mattermost, join the team W-11 students\nJoin Causal Data Science Channel (you might need to wait a bit, as I first have to add you)\n\nThere, and in the sessions, I will try to help you as much as possible.\nIn order to keep the discussion efficient and manageable it is necessary that we all follow some basic rules:\n\nPost error message: if you run into an error it is necessary that I know what the error is. Often reading the error message very carefully can also help you to understand where the problem comes from.\nPost the code that caused the error: in order to reproduce the error I need the last command that caused the error. If we need more context we will ask you for that.\nUse the formatting guidelines of Mattermost when you post code. That makes a huge difference in terms of readability. They will also be linked in the channel description. Most important is that using ``` one line above and one line below your code will make it easy to read.\nUse thread function to reply to a discussion. This way a discussion can be easier read. You find the reply button on the right side of a message.\n\nPlaying by these rules makes it a lot easier for everyone to follow the discussion and learn from similar problems and everyone can benefit from the discussions.\nSee in this minimalistic example how little formatting makes your code and error easy to read.\n\n \n```r\nx %&gt;% sum()\n```\n\n**Error:**\nError in x %&gt;% sum() : could not find function \"%&gt;%\"\n\n\n\n\nHow to format your code and error when you ask for help.\n\n\n\n\n\nHow to format your code and error when you ask for help.",
    "crumbs": [
      "Organization",
      "Mattermost"
    ]
  },
  {
    "objectID": "content/organization/intro_R.html",
    "href": "content/organization/intro_R.html",
    "title": "Introduction to R & RStudio IDE",
    "section": "",
    "text": "Before we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -&gt; Global options -&gt; Appearance and switch theme in -&gt; Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab.",
    "crumbs": [
      "Organization",
      "Introduction to R & RStudio IDE"
    ]
  },
  {
    "objectID": "content/organization/intro_R.html#installing-r-rstudio-ide",
    "href": "content/organization/intro_R.html#installing-r-rstudio-ide",
    "title": "Introduction to R & RStudio IDE",
    "section": "",
    "text": "Before we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -&gt; Global options -&gt; Appearance and switch theme in -&gt; Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab.",
    "crumbs": [
      "Organization",
      "Introduction to R & RStudio IDE"
    ]
  },
  {
    "objectID": "content/organization/intro_R.html#introduction-to-r",
    "href": "content/organization/intro_R.html#introduction-to-r",
    "title": "Introduction to R & RStudio IDE",
    "section": "Introduction to R",
    "text": "Introduction to R\nBefore you begin coding, it’s important to remember this final point: do not let the errors, warnings, and other messages that you, like everyone else, are bound to encounter intimidate you. There is no reason to panic just because you see red text in your console and in fact, what is returned will often times already help you to solve the problem and lead you onto the right track.\nThere are three different types of texts that communicate issues or information about the code execution:\n\n\nErrors: this is a legitimate error and most likely your code did not run due to the error. Many of the error messages are very concise and you will directly see what was wrong, what is missing etc. If you do not see what you did wrong at first glance, you can copy the error message and google it. It is very likely someone else has run into the same error before.\n\n\n# Example error\n1 + \"a\"\n\nError in 1 + \"a\": nicht-numerisches Argument für binären Operator\n\n#Error in 1 + \"a\" : non-numeric argument to binary operator\n\n\n\nWarnings: opposed to an error, your code did probably run but there could be something off. However, it is just a warning. You can check it and if you think the warning does not apply to your specific scenario, you can go on.\n\n\n# Example warning\nas.numeric(c(\"18\", \"30\", \"50+\", \"345,678\"))\n\nWarning: NAs durch Umwandlung erzeugt\n\n\n[1] 18 30 NA NA\n\n# Warning message: NAs introduced by coercion \n\n\n\nMessages: these are just friendly texts that provide you with useful information. They do not need immediate attention but can provide useful supplementary information.\n\n\n# Example message\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nInteractive Tutorials:\nBut let’s no more talk about it but instead start coding because the best way to get familiar with R and to code is to just start.\nIn the following chapters, you will learn to code along the way, but to start you will go through some very concise tutorials from the R package swirl. The package provides a whole bunch of tutorials in the console.\nFeel free to complete as many tutorials as you want, but for this class, the following tutorial is of particular use: The R Programming Environment (Chapter 2-12)\nswirl() does not come with R by default but is an optional package. R packages are extensions of the base functionality implemented by default when you download R. Written by users around the world, packages provide additional features and are crucial for data science tasks in practice as you will later see.\nYou need to follow two steps to use an R package:\n\nOnce: install the package. As already mentioned, packages are not installed by default and you have to download it and add it to your library. Once you’ve installed it, you don’t have to repeat this step.\nAlways: load the package. By default, just the base R functionality is loaded and when you want to make use of the additional features provided by a specific package, you have to load it every time you start RStudio.\n\nSo let’s do it for the package swirl:\nFirst, we install the package. This has to be done only once. You can either choose to write your code into the source editor or directly into the console\n\ninstall.packages(\"swirl\")\n\nThen, we load the library into our current our R session.\n\nlibrary(swirl)\n\nNow, the package is loaded and we can start making use of it.\n\ninstall_course(\"The R Programming Environment\")\n\nYou just have to type swirl() into your console and follow the instructions! Please make sure to always use the same name. This way, you can leave the tutorial and start at the same position again later. It’s best to write it down so that you do not forget it.\n\nswirl()\n\nSelect the course you just installed: The R Programming Environment and start with Chapter 2. You should skip Chapter 1 because its irrelevant for you. If you accidentally selected Chapter 1, just quickly go through it and choose No at the last question.\nswirl will ask you to install packages for you that are needed for the tutorial. Please confirm when asked. If you computer is struggling with installing a package named “vctrs”, please type in the following command. If you don’t get such an error, you can ignore it.\n\ninstall.packages(\"vctrs\", repos = \"https://packagemanager.rstudio.com/cran/latest\")\n\nIf, at some point, you want to take a break, you can leave the swirl course by typing bye() or the Esc key. You can return to the course by typing swirl() and hitting Enter. And remember, to use the same name you used the first time.\nYou don’t need to submit anything from this step. Just focus on getting familiar with R by completing the tutorial (I recommend to solve chapter 2-12)!\n\n\n\n\n\n\nNote\n\n\n\nWhenever you want to find out more about a command or you have difficulties understanding what it does, you can click on it and a help page will show up.",
    "crumbs": [
      "Organization",
      "Introduction to R & RStudio IDE"
    ]
  },
  {
    "objectID": "content/organization/quarto.html",
    "href": "content/organization/quarto.html",
    "title": "Quarto [Optional Read]",
    "section": "",
    "text": "Quarto is a scientific publishing tool that allows R, Python, Julia and Observable JS users to create dynamic documents, websites, books and more. In fact, the whole course website is created using Quarto. In case you downloaded RStudio for this course, you do not need to install Quarto anymore. If you have an older RStudio version, you might have to download an install it or update to a new RStudio version. If you are familiar with Markdown or even RMarkdown you will see a lot of similarities.\nYou can explore Quarto’s documentation to learn more about creating documents, websites, blogs, books, slides, etc.\nEach page of your website is created by a q-Markdown file (.qmd). All website pages are plain text file that have the extension .qmd. Notice that the file contains three types of content:\nAn (optional) YAML header surrounded by - - - at the top (there is no need in the beginning to alter it)\nIn the code chunks, you can set different options with #|:\n\n#| eval: false prevents running the code and include its results\n#| include: false prevents code and results from appearing in the finished file. Quarto still runs the code in the chunk, and the results can be used by other chunks\n#| echo: false prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\n#| message: false prevents messages that are generated by code from appearing in the finished file.\n#| warning: false prevents warnings that are generated by code from appearing in the finished.\n#| fig-cap: “…” adds a caption to graphical results.\n\nSee the Quarto Cheat Sheet or the official quarto documentation for further information regarding the markdown syntax. It is necessary, that your code is formatted correctly to be evaluated.",
    "crumbs": [
      "Organization",
      "Quarto [Optional Read]"
    ]
  },
  {
    "objectID": "content/organization/quarto.html#quarto",
    "href": "content/organization/quarto.html#quarto",
    "title": "Quarto [Optional Read]",
    "section": "",
    "text": "Quarto is a scientific publishing tool that allows R, Python, Julia and Observable JS users to create dynamic documents, websites, books and more. In fact, the whole course website is created using Quarto. In case you downloaded RStudio for this course, you do not need to install Quarto anymore. If you have an older RStudio version, you might have to download an install it or update to a new RStudio version. If you are familiar with Markdown or even RMarkdown you will see a lot of similarities.\nYou can explore Quarto’s documentation to learn more about creating documents, websites, blogs, books, slides, etc.\nEach page of your website is created by a q-Markdown file (.qmd). All website pages are plain text file that have the extension .qmd. Notice that the file contains three types of content:\nAn (optional) YAML header surrounded by - - - at the top (there is no need in the beginning to alter it)\nIn the code chunks, you can set different options with #|:\n\n#| eval: false prevents running the code and include its results\n#| include: false prevents code and results from appearing in the finished file. Quarto still runs the code in the chunk, and the results can be used by other chunks\n#| echo: false prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\n#| message: false prevents messages that are generated by code from appearing in the finished file.\n#| warning: false prevents warnings that are generated by code from appearing in the finished.\n#| fig-cap: “…” adds a caption to graphical results.\n\nSee the Quarto Cheat Sheet or the official quarto documentation for further information regarding the markdown syntax. It is necessary, that your code is formatted correctly to be evaluated.",
    "crumbs": [
      "Organization",
      "Quarto [Optional Read]"
    ]
  },
  {
    "objectID": "content/optional_read/motivation.html",
    "href": "content/optional_read/motivation.html",
    "title": "Motivation",
    "section": "",
    "text": "In this course, you will learn about causality in data science with a particular emphasis on business applications. Causal data science methods are increasingly recognized and developed to understand causes and effects. Moving beyond a prediction-based approach in data science, the purpose of causal methods is to understand underlying processes and mechanisms to guide strategic decision-making. Causal methods allow us to answer questions that otherwise could not be addressed.\nA large global survey1 conducted among data science practitioners in the industry in 2020 states the importance of causal data science. 83% of the respondents consider causal inference in data-driven decisions making increasingly important and 44% state that, in their data science project, causal inference already plays an important role. Additionally,\nWhile the primary goal of machine learning is typically the development of algorithms for a high prediction and classification accuracy, causal inference aims to understand and establish cause-and-effect relationships between variables.\nTypical applications in business therefore aim to answer questions like:\nMany successful companies have already recognized the advantages of causal data science. Click on the link to get more details how these companies are using tools from causal inference to generate value within their organizations.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Motivation"
    ]
  },
  {
    "objectID": "content/optional_read/motivation.html#footnotes",
    "href": "content/optional_read/motivation.html#footnotes",
    "title": "Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.causalscience.org/blog/causal-data-science-in-practice/↩︎",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Motivation"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html",
    "href": "content/optional_read/prob.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Feel free to review some basic concepts of probability and statistics. All methods that used in this course are based on statistical models and these require probability theory.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#basic-rules-of-probability",
    "href": "content/optional_read/prob.html#basic-rules-of-probability",
    "title": "Probability Theory",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nConsider the most simple example: flipping coins. We define the outcome of the flip of a coin as a random variable as we are uncertain about what side the coin lands on. To express this uncertainty, we make us of probability theory.\nAfter flipping the coin, we will see what side the coin has landed on and our random variable has taken on of the two possible events \\(\\{H, T\\} \\subseteq \\Omega\\). It will be either Head or Tail.\nSo we have already defined two terms: random variable and events. Now what is a probability? A probability is always linked to an event typically denoted by a capital letter, here either \\(H\\) and \\(T\\), and expresses how likely this event is to happen. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\\[\nP(H) = P(T) = 0.5\n\\]\nExtreme cases: If an event \\(A\\) is impossible, its probability is \\(P(A) = 0\\) and if it is certain to occur, it is \\(P(A)=1\\).\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 1: Probability is a real number greater or equal to 0.\n\n\nWe can also introduce the compliment \\(\\overline{A}\\), which is what happens when \\(A\\) does not happen and consequently, \\(P(A) + P(\\overline{A}) = 1\\). \\(A\\) and \\(\\overline{A}\\) are mutually exclusive, by definition. But there could also be two events \\(A\\) and \\(B\\) that are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\\[\nP(H \\cup T) = P(H) + P(T) = 1\n\\]\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 2: Total probability is equal to 1.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 3: Probability of mutually exclusive events is the sum of the probabilities. (Mutually exclusive: events can’t happen at the same time)\n\n\nTo understand what not mutually exclusive events are, consider events \\(studying\\) and \\(working\\). For a random person, we don’t know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\nThen, the probability of at least one of the events happening is calculated by\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nwith \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the addition rule.\n\n\n\n\n\n\nTip\n\n\n\n\\(\\cup\\) : Union, can be translated as “or”.\n\\(\\cap\\) : Intersection, can be translated as “and”.\n\n\n\n\n\n\nFor mutually exclusive events:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B)\n\\]\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule,\n\\[\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n\\]\nwhere \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened. It is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\nUsing the example with workers and students: without knowing exact numbers, we can assume that students are less likely to work than individuals who are not studying.\n\\[\nP(working|studying) &lt; P(working|\\overline{studying})\n\\]\nEssentially, we are looking at probabilities restricted to a subset of the sample, which in this comparison are the subsamples of studying persons and non-studying persons.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways. Let’s think of rolling a die twice (first roll \\(R_1\\) and second roll \\(R_2\\)).\n\\[\nP(R_2 \\mid R_1) = P(R_2)\n\\]\nThe second roll does not depend on the first one. With each roll the outcomes \\({1, 2, 3, 4, 5, 6}\\) have the same probability likely independent of the previous roll. If we want to compute the probability of both rolls being a \\(6\\), we would just have to multiply the probabilities for each roll.\n\\[\nP(R_1 = 6 \\cap R_2 = 6) = P(R_1 = 6) \\ P(R_2 = 6)\n\\]",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#probability-tree",
    "href": "content/optional_read/prob.html#probability-tree",
    "title": "Probability Theory",
    "section": "Probability Tree",
    "text": "Probability Tree\nLet’s go back to the case where events are dependent on each other. An intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events happens. The probability of two consecutive events is obtained by multiplying the probabilities.\nConsider the following example: you are project manager and based on your are interested in the probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Using historical data about past projects, you come up with the following tree.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#assignment-i",
    "href": "content/optional_read/prob.html#assignment-i",
    "title": "Probability Theory",
    "section": "Assignment I",
    "text": "Assignment I\n\nDefine being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\). (Hint: Check here, if you are not sure what is shown in the probability tree.)\nThen, compute the following probabilities and the sum of all four probabilities.\n\n\\(P(T \\cap S)\\)\n\\(P(T \\cap \\overline{S})\\)\n\\(P(\\overline{T} \\cap S)\\)\n\\(P(\\overline{T} \\cap \\overline{S})\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nWith some browsers and specific operating systems, the compliment probability is not shown correctly (missing the horizontal bar above the letter). In that case it often helps to zoom in or out.\n\n\n\n\n\n\n\n\nOptional assignments!\n\n\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#set-theory",
    "href": "content/optional_read/prob.html#set-theory",
    "title": "Probability Theory",
    "section": "Set Theory",
    "text": "Set Theory\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams that are based on set theory. We already used a simple one above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\nLet’s use an example to understand some other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices: smartphones, tablets and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current usage distribution.\nInstead of using actual data, we simulate the data collection process here. If you are interested how to do it in R, you can expand and check out the code by clicking on Code. But you don’t have to. And don’t worry, if it looks too complicated at this point, just move on.\n\n\n\n\n\n\nNote\n\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R.\n\ntibble() is the most convenient way to create tablets. You specify column name and content and assign your tibble to an object to store it.\n\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\n\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\n\nmutate() is one of the most important functions for data manipulation in tablets. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %&gt;% mutate(new_variable = existing_var / 100).\n\n\n\nCode# Load tidyverse package\nlibrary(tidyverse)\n\n# Number of obervations\nn &lt;- 1000\n\n# Create tibble\napp_usage &lt;- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage &lt;- app_usage %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer. Please note that \\(1\\) indicates usage and \\(0\\) indicates no usage.\n\n\n\n\n\n\nNote\n\n\n\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\n\n\n# Show first ten lines\nhead(app_usage, 10)\n\n\n  \n\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n\n\n\n\n\nNote\n\n\n\nSumming all values by column is done by colSums(table). To sum rows, you can use rowSums(table).\n\n\n\n# Show column sums\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500        589        389        226 \n\n\nThe sum of \\(user\\_id\\) does not really tell us anything. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n\n\n\n\n\nNote\n\n\n\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\n\n\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %&gt;% select(smartphone, tablet, computer) %&gt;% colSums()\n\nsmartphone     tablet   computer \n       589        389        226 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n\nGeneric Venn diagram\n\n\n\n\n\n\n\nNote\n\n\n\n\nwhich() checks a condition and returns the indices.\n\n\n\n# Set of phone, tablet and computer users\nset_phon &lt;- which(app_usage$smartphone == 1)\nset_tabl &lt;- which(app_usage$tablet == 1)\nset_comp &lt;- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\nAssignment II\n\nUsing the Venn diagram above, answer the following questions.\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\n\n\n\n\n\n\n\nOptional assignments!\n\n\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments.\n\n\n\nYou can also use the example to go through the basic probability rules defined above (that does not belong to the assignment anymore).\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - P(T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#bayes-theorem",
    "href": "content/optional_read/prob.html#bayes-theorem",
    "title": "Probability Theory",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMath\nA very important theorem in probability theory is Bayes theorem. In fact, it has been called the most powerful rule of probability and statistics. Let’s quickly go through the math. By reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nand using the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem expresses a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before. With the Bayes theorem you can answer questions like:\n\nHow likely is it that it will rain, when there are clouds in the morning?\nHow likely is it that an email is spam if certain keywords appear?\n\n\n\n\n\n\n\nTip\n\n\n\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]\n\n\nApplication\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) \n\\(P(B|\\overline{A}) = 0.01\\) \n\\(P(A) = 0.04\\) \nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B)\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nAssignment III\n\nCompute\n\n\n\\(P(\\overline{A}|B)\\) (1)\n\n\\(P(A|B)\\) (2)\n\nand fill the gaps in the following sentence:\nThese results show that in case the alarm is triggered, there is a possibility of about __% that the product is flawless and a probability of __% that the product is faulty.\n\n\n\n\n\n\n\nOptional assignments!\n\n\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments.\n\n\n\n\n\nGeneric Venn diagram",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/course_weeks/overview.html",
    "href": "content/course_weeks/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\n\nSession\nDate\nTopic\n\n\n\n\n1\nApril 15 & 16\nIntroduction to Causal Inference\n\n\n2\nApril 22 & 21\nGraphical Causal Models\n\n\n3\nApril 29 & 30\nRandomized Experiments & Linear Regression\n\n\n4\nMay 6 & 7\nMatching\n\n\n5\nMay 13 & 14\nDouble Machine Learning\n\n\n-\nMay 20 & 21\nHoliday\n\n\n6\nMay 27 & 28\nEffect Heterogeneity\n\n\n7\nJune 3 & 4\nUnobserved Confounding & Instrumental Variables\n\n\n8\nJune 10 & 11\nDifference-in-Difference\n\n\n9\nJune 17 & 18\nSynthetic Control\n\n\n10\nJune 24 & 25\nRegression Discontinuity\n\n\n11\nJuly 1 & 2\nCausal Mediation\n\n\n12\nJuly 8 & 9\nFurther Topics in Causal Machine Learning",
    "crumbs": [
      "Content",
      "Overview"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html",
    "href": "content/course_weeks/week_04/week_4.html",
    "title": "4 - Matching",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#slides-recap",
    "href": "content/course_weeks/week_04/week_4.html#slides-recap",
    "title": "4 - Matching",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#practical-example",
    "href": "content/course_weeks/week_04/week_4.html#practical-example",
    "title": "4 - Matching",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#assignment",
    "href": "content/course_weeks/week_04/week_4.html#assignment",
    "title": "4 - Matching",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html",
    "href": "content/course_weeks/week_02/week_2.html",
    "title": "2 - Graphical Causal Models",
    "section": "",
    "text": "Slides - Week 2\nIn this chapter, we focus on the identification of causal effects with the help of graphical models, also known as directed acyclic graphs (DAGs). By graphically modeling and leveraging your theoretical knowledge about the data-generating process, you will be able to understand when association is actually causation.\n\n\n\n\n\n\nTip\n\n\n\nData-generating process (DGP) refers to the underlying mechanisms that generate the data we observe. It represents real-world processes that produce the data points we analyze.\n\n\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come from things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nIt should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#slides-recap",
    "href": "content/course_weeks/week_02/week_2.html#slides-recap",
    "title": "2 - Graphical Causal Models",
    "section": "",
    "text": "Slides - Week 2\nIn this chapter, we focus on the identification of causal effects with the help of graphical models, also known as directed acyclic graphs (DAGs). By graphically modeling and leveraging your theoretical knowledge about the data-generating process, you will be able to understand when association is actually causation.\n\n\n\n\n\n\nTip\n\n\n\nData-generating process (DGP) refers to the underlying mechanisms that generate the data we observe. It represents real-world processes that produce the data points we analyze.\n\n\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come from things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nIt should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#dags",
    "href": "content/course_weeks/week_02/week_2.html#dags",
    "title": "2 - Graphical Causal Models",
    "section": "DAGs",
    "text": "DAGs\nTo discuss and illustrate the notation and terminology of DAGs, we start with a simple and often discussed research problem in causal inference. It deals with the question of whether a university degree increases the salary. While people with university degree tend to earn more than people without university degree, there is the question whether this is caused by the university degree or it is mainly influenced by individuals’ ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree. It is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit. By the way, it helps to imagine association flowing through a graphical model as water flows through a stream.\nDepicted in a graph, we get the following:\n\n\n\n\n\n\n\n\nLet’s discuss some terms:\nQuestion 1: What is/are the descendant(s) of ability?\n\nAnswer\nuni_degree and salary\nQuestion 2: What is/are the parent(s) of salary?\n\nAnswer\nuni_degree and ability\nQuestion 3: What is the treatment and what is the outcome?\n\nAnswer\nuni_degree and salary\nQuestion 4: What does a node depict?\n\nAnswer\nA random variable\nQuestion 5: What does an arrow depict?\n\nAnswer\nFlow of association",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#introduction-dagitty-dagitty-in-r",
    "href": "content/course_weeks/week_02/week_2.html#introduction-dagitty-dagitty-in-r",
    "title": "2 - Graphical Causal Models",
    "section": "Introduction dagitty, dagitty in R",
    "text": "Introduction dagitty, dagitty in R\nOf course, it’s perfectly fine to first sketch DAGs on a sheet of paper, but at some point, when your analysis gets more complex, you are well advised to make use of computational resources. In the following paragraphs, you will be introduced to a few very useful resources.\nA well implemented application is DAGitty, a browser-based environment for creating and analyzing DAGs. Draw your DAG, define treatment and outcome, which variables are observed and unobserved and many other things. Afterwards, you will see what kind of adjustment is necessary to estimate the causal effect of interest. There is also an implementation in R, which is the R package dagitty complemented by ggdag, which provides improved plotting functionality.\n\n# Install dagitty package\ninstall.packages(\"dagitty\")\n\n# Install ggdag\ninstall.packages(\"ggdag\")\n\nNow, we will cover three different ways to create the DAG shown above.\nOption 1: Build on browser-based DAGitty\n…\nOption 2: Build using R package dagitty\n\n\n\n\n\n\nNote\n\n\n\nIn several R functions you will find the syntax y ~ x or y ~ x1 + x2 + ... to specify your functional form. It represents \\(y\\) being dependent on \\(x\\) (of course the names depend on your data/model). Please note, that no parentheses are needed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen drawing the DAG yourself, you can leave out theme_dag_cds() as it is only a custom theme that I have defined for this website. Instead you could use other themes such as theme_dag(), theme_dag_grey(), theme_dag_blank() or proceed without any theme. The other modifications following the theme are also only for the purpose of matching the website theme and can be disregarded. However, you should set text = TRUE in ggdag() in this case.\n\n\n\n# Load packages\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Define dependencies and coordinates\nschooling_dag_1 &lt;- dagify(\n  # Required arguments\n  uni_degree ~ ability,\n  salary ~ ability,\n  salary ~ uni_degree,\n  # Optional arguments\n  exposure = \"uni_degree\",\n  outcome = \"salary\",\n  coords = list(\n    x = c(uni_degree = 1, salary = 3, ability = 2),\n    y = c(uni_degree = 1, salary = 1, ability = 2)\n    ))\n\n# Plot DAG\n# Plot DAG\nggdag(schooling_dag_1, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nOption 3: Copy from dagitty.net to build graph in R\n\n# Define DAG\nschooling_dag_2 &lt;- 'dag {\nability [pos=\"0.5, 0.4\"]\nsalary [outcome,pos=\"0.8, 0\"]\nuni_degree [exposure,pos=\"0.2, 0\"]\nability -&gt; salary\nability -&gt; uni_degree\nuni_degree -&gt; salary\n}'\n\n# Plot DAG\nggdag(schooling_dag_1, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nOne great advantage of having our DAG coded into R or on DAGitty is the possibility to see what we have to adjust for. You can do that by running the command adjustmentSets() which returns all possible sets of variables that you can control for to retrieve a causal effect.\n\n# Get adjustment sets. Because we already specified exposure and outcome when\n# constructing the DAG, we do not need to do again. If they were not specified,\n# you could do so by explicitly providing arguments (\"exposure\", \"outcome\")\nadjustmentSets(schooling_dag_2)\n\n{ ability }",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#types-of-association",
    "href": "content/course_weeks/week_02/week_2.html#types-of-association",
    "title": "2 - Graphical Causal Models",
    "section": "Types of association",
    "text": "Types of association\nMost of the time, DAGs are more complex than the example above. Even in that example, there are many more variables that could be included, like social environment, gender etc. But although in practice DAGs are more complex, they can be disassembled in building blocks that are easier to analyze.\nEffectively, there are only three different types of association we need to focus on:\nChain: \\(X \\rightarrow Z \\rightarrow Y\\)\nConfounder: \\(X \\leftarrow Z \\rightarrow Y\\)\nCollider: \\(X \\rightarrow Z \\leftarrow Y\\)\nKnowing their characteristics and idiosyncrasies allows us to identify valid strategies to estimate causal effects. Then, you know which variables you have to include in your analysis and which ones you have to leave out. Because in case you include or exclude the wrong variables, you will end up with a biased results and you are not able to interpret your estimate causally.\nIt is important to understand (conditional) (in-)dependencies between, and with particular focus on conditional independence. An important concept you learned in the lecture is d-separation:\n\n\n\n\n\n\nNote\n\n\n\nd-separation\nTwo nodes \\(X\\) and \\(Y\\) are d-separated by a set of nodes \\(Z\\) if all of the paths between \\(X\\) and \\(Y\\) are blocked by \\(Z\\).\n\n\nChain\nOne element is a chain of random variables where the causal effect flows in one direction.\n\nCode# Specify chain structure\nchain &lt;- dagify(\n  Y ~ Z,\n  Z ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 0, X = 0))\n)\n\n# Plot DAG\nggdag(chain) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nConsider the following variables:\n\n\n\\(X\\): Investment in R&D\n\n\\(Z\\): Innovative products\n\n\\(Y\\): Market share\n\nThis mechanism is also sometimes called mediation, because \\(Z\\) mediates the effect of \\(X\\) on \\(Y\\).\nQuestion: Describe the dependencies in the DAG above.\n\nAnswer\n\n\n\\(X\\) and \\(Z\\): dependent, as indicated by the arrow. If you invest more into R&D, you develop more innovative products.\n\n\\(Z\\) and \\(Y\\): dependent, as indicated by the arrow. If you develop more innovative products, your market share increases.\n\n\\(X\\) and \\(Y\\): dependent, as indicated by the arrow (going through \\(Z\\)). More investment in R&D leads to more innovative products and an increased market share.\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, because when we condition on innovative products, that means we hold the quantity and quality of innovative products fixed at a particular level, then there is no effect from investment in R&D to market share as there is no direct effect, but only an effect through the development of innovative products.\nRule: Two variables, \\(X\\) and \\(Y\\), are conditionally independent given \\(Z\\), if there is only one unidirectional path between \\(X\\) and \\(Y\\) and \\(Z\\) is any set of variables that intercepts that path.\nWe can also check using the function dseperated(). It returns TRUE when two nodes are d-separated, which means that they are independent of each other. You need to provide \\(X\\) and \\(Y\\), which are the focal nodes and \\(Z\\), your set that you condition on.\n\n# Check d-separation between X and Y\ndseparated(chain, X = \"X\", Y = \"Y\", Z = c())\n\n[1] FALSE\n\n\n\n# Check d-separation between X and Y conditional on Z\ndseparated(chain, X = \"X\", Y = \"Y\", Z = c(\"Z\"))\n\n[1] TRUE\n\n\nFork\nAnother mechanism is the fork, also called common cause.\n\nCode# Fork\nfork &lt;- dagify(\n  X ~ Z,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0))\n)\n\n# Plot DAG\nggdag(fork) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nThe reason it is called common cause is that, as depicted above, both \\(X\\) and \\(Y\\) are caused by \\(Z\\).\nTo illustrate it, consider the following scenario: \\(Z\\) represents the temperature in a particular town and \\(X\\) and \\(Y\\) represent ice cream sales and number of crimes in that same town, respectively.\n\n\n\\(X\\): ice cream sales\n\n\\(Z\\): temperature\n\n\\(Y\\): number of crimes\n\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\nQuestion: Describe the dependencies in the DAG above.\n\nAnswer\n\n\n\\(Z\\) and \\(X\\): dependent, as indicated by arrow. Higher temperature leads to more ice cream sales.\n\n\\(Z\\) and \\(Y\\): dependent, as indicated by arrow. Higher temperature leads to more crimes.\n\n\\(X\\) and \\(Y\\): dependent, as both are influenced by \\(Z\\). \\(X\\) and \\(Y\\) change both with variation in \\(Z\\). Variation in temperature affects ice cream sales and number of crimes simultaneously.\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, as for a fixed level of temperature, there is no association anymore.\nRule: If variable \\(Z\\) is a common cause of variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\)are independent conditional on X.\nAgain, we also check using R.\n\n# Check d-separation between X and Y conditional on Z\ndseparated(fork, X = \"X\", Y = \"Y\", Z = c(\"Z\"))\n\n[1] TRUE\n\n\n\n# Check d-separation between X and Y\ndseparated(fork, X = \"X\", Y = \"Y\", Z = c())\n\n[1] FALSE\n\n\nCollision\nThe last mechanism is the collision, which is also called common effect.\n\nCode# Collider\ncollider &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1))\n)\n\n# Plot DAG\nggdag(collider) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nIt is the reflection of the fork and both \\(X\\) and \\(Y\\) have a common effect on the collision node \\(Z\\).\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n\n\\(X\\) and \\(Z\\): dependent, as indicated by arrow.\n\n\\(Y\\) and \\(Z\\): dependent, as indicated by arrow.\n\n\\(X\\) and \\(Y\\): independent, there is no path between \\(X\\) and \\(Y\\).\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson’s paradox.\nFor example, imagine the variables to be:\n\n\n\\(X\\): programming skills\n\n\\(Y\\): social skills\n\n\\(Z\\): hired by renowned tech company\n\nFirst of all, in the general population, there is no correlation between programming skills and social skills (3rd dependency). Second, having either programming or social skills will land you a job a a tech company (1st and 2nd dependency).\nBut what about the last dependency? Why are programming and social skills suddenly correlated when conditioned on e.g. being hired by a tech company? That is because when you know someone works for a tech company and has no programming skill, the likelihood that he/she has social skills increases because otherwise he/she would have likely not be hired. Vice versa, if you know someone has been hired and has no social skills, he/she is probably a talented programmer.\nRule: If a variable \\(Z\\) is the collision node between two variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are unconditionally independent but are dependent conditional on \\(Z\\) (and any descendants of \\(Z\\)).\nSame procedure as above, checking using R.\n\n# Check d-separation between X and Y\ndseparated(collider, X = \"X\", Y = \"Y\", Z = c())\n\n[1] TRUE\n\n\n\n# Check d-separation between X and Y conditional on Z\ndseparated(collider, X = \"X\", Y = \"Y\", Z = c(\"Z\"))\n\n[1] FALSE\n\n\nQuiz\n\nLook at the following DAG and by hand, describe what nodes are d-separated and which ones are not.\n\n\nCode# create DAG from dagitty\ndag_model &lt;- 'dag {\nbb=\"0,0,1,1\"\nX [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nM [pos=\"0.2,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nX -&gt; M\nM -&gt; Y\nX -&gt; Z3\nZ1 -&gt; X\nZ1 -&gt; Y\nZ2 -&gt; Y\nZ2 -&gt; Z3\nZ3 -&gt; Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nFor each of the following scenarios, explain whether the nodes \\(X\\) and \\(Y\\) are d-separated.\n\nno conditioning\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c())\n\n[1] FALSE\n\n\n\nconditioning on \\((M, Z1)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z1\"))\n\n[1] TRUE\n\n\n\nconditioning on \\((M, Z3)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z3\"))\n\n[1] FALSE\n\n\n\nconditioning on \\((M, Z1, Z4)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z1\", \"Z4\"))\n\n[1] FALSE\n\n\n\nconditioning on \\((M, Z1, Z2, Z4)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z1\", \"Z2\", \"Z4\"))\n\n[1] TRUE\n\n\n\nIn R, draw a DAG of a randomized controlled trial. Assume the following variables: the treatment \\(D\\) and a variable \\(Z\\) that affects the outcome \\(Y\\).\n\n\n# RCT\nrct &lt;- dagify(\n  Y ~ X,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 2, X = 1))\n)\n\n# Plot DAG\nggdag(rct) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#practical-examples",
    "href": "content/course_weeks/week_02/week_2.html#practical-examples",
    "title": "2 - Graphical Causal Models",
    "section": "Practical examples",
    "text": "Practical examples\nEffect of hiring consultants\nConsider this scenario: You aim to determine the effectiveness of hiring top consultants for your company and quantify this impact. You have access to a large company database containing profit data from both the previous and current years, along with information on whether companies have hired top consultants.\nFirst, we are going to load the data (You’ll need to download the data first).\n\n\n\n\n\n\nNote\n\n\n\nThe .rds format is a specific R format preferred over other format like .csv when you are working only in R. It preserves R-specific attributes and structure and loads and saves objects efficiently with readRDS() and saveRDS(), respectively.\n\n\n\n# Load the data\nprofits &lt;- readRDS(\"profits.rds\")\nprint(profits)\n\n# A tibble: 1,000 × 4\n   company previous_profit consultant profit\n     &lt;int&gt;           &lt;dbl&gt;      &lt;int&gt;  &lt;dbl&gt;\n 1       1            7.46          1   7.44\n 2       2            5.92          0   5.03\n 3       3            7.47          1   7.32\n 4       4            8.73          1   8.39\n 5       5            6.65          0   5.75\n 6       6            2.47          0   2.13\n 7       7            4.59          0   4.06\n 8       8            5.80          1   5.99\n 9       9            5.83          1   5.9 \n10      10            4.39          0   3.72\n# ℹ 990 more rows\n\n\nIdentification\nBy now, you have learned that the first step in causal inference - before estimating the treatment effect - is the identification and in this chapter you have learned a useful graphical tool: DAGs. Let’s draw the DAG that shows your understanding of the scenario and find out how we are able to recover the causal effect of hiring consultants on profits.\nLet’s discuss each possible dependency:\n\n\n\\(consultant\\) - \\(profit\\)\n\n\n\\(consultant\\) - \\(previous\\_profit\\)\n\n\n\\(profit\\) - \\(previous\\_profit\\)\n\n\n\n# Construct DAG\nprofits_dag &lt;- dagify(\n  consultant ~ previous_profit,\n  profit ~ previous_profit,\n  profit ~ consultant,\n  coords = list(x = c(consultant = 1, profit = 3, previous_profit = 2),\n                y = c(consultant = 1, profit = 1, previous_profit = 2))\n)\n\n# Plot DAG\nggdag(profits_dag, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nBy now, you are probably already able to see it on a first glance because you know what a confounder is and how to adjust for it. But for the purpose of demonstration, let’s dagitty tell us what we need to adjust for.\n\n# Adjustment set\nadjustmentSets(profits_dag, exposure = \"consultant\", outcome = \"profit\")\n\n{ previous_profit }\n\n\nAs expected, it is the backdoor adjustment which we need to perform in our estimation. We need to control for the previous profits because more previously profitable companies are more likely to hire top consultants and are more likely to make profits again.\nEstimation\nIn the lecture, you have learned the following formula to satisfy the backdoor criterion: condition on the values of your adjustment set and average over the joint distribution.\n\\[\nP(Y|do(D)) = \\sum\\nolimits_W P(Y|D=d, W=w) P(W=w)\n\\] We slightly rewrite the so-called adjustment formula to highlight what we need to do in the estimation step.\n\\[\n\\begin{align}\nATE &= E_W[E(Y|D=1) - E(Y|D=0)] \\\\\nATE &= \\sum_{w \\in W} [E(Y|D=1, W=w) - E(Y|D=0, W=w)] P(W=w) \\\\\n&= \\sum_{w \\in W} [E(Y|D=1, W=w)P(W=w) - E(Y|D=0, W=w)P(W=w)]\n\\end{align}\n\\] When dealing with only a few values of \\(W\\), you condition by examining each group \\(w \\in W\\), comparing treated and untreated observations within each group, and then averaging the outcomes, with the group sizes serving as weights. Remember, you assume that within the groups the treatment must look as if it was as good as randomly assigned.\nHowever, when we look at our adjustment set, which is the previous profit of a company, we see that there are not only a couple of values, but theoretically, previous profits could take any real number.\n\n\n\n\n\n\nNote\n\n\n\nsummary() provides a concise summary of the statistical properties of data objects such as data frames, vectors or even models.\n\n\n\n# Statistical summary of column of previous profits\nsummary(profits$previous_profit)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -1.0     4.0     5.6     5.6     7.1    13.5 \n\n\nIn the following weeks, we will learn sophisticated methods to deal with continuous confounders, but today, we refer to a simpler method. We assign each company to a group determined by the value of previous profits. The function ntile() does the job: it splits the data into \\(n\\) equally sized groups ordered by size.\n\n\n\n\n\n\nNote\n\n\n\nIn R, pipes |&gt; are used to chain together multiple operations in a readable and concise manner. The |&gt; operator (shortcut: cmd/ctrl + shift + m) passes the output of one function as the first argument to the next function. This makes code easier to understand by avoiding nested function calls and enhancing code readability.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn R, group_by() is used to group data by one or more variables, summarise() computes summary statistics within each group, and ungroup() removes grouping structure. These functions are commonly used together in data manipulation workflows and belong to the tidyverse (dplyr). n() is a function specifically for such grouping operations. It does not take any arguments and counts the number of rows in each group.\n\n\n\n# Split group into n equally sized groups \nprofits$group &lt;- ntile(profits$previous_profit, n = 4)\n\n# Compute mean value and number of observations by group\nprofits_by_group &lt;- profits |&gt; \n  # What columns to group by\n  group_by(group, consultant) |&gt; \n  # What columns to use for aggregation and what kind of aggregations\n  summarise(\n    # Average profit by group\n    mean_profit = mean(profit),\n    # Number of observations by group\n    nobs = n()\n  ) |&gt; \n  # Remove grouping structure\n  ungroup()\n\n# Show table\nprint(profits_by_group)\n\n# A tibble: 8 × 4\n  group consultant mean_profit  nobs\n  &lt;int&gt;      &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1     1          0        2.45   181\n2     1          1        3.44    69\n3     2          0        4.05   189\n4     2          1        5.02    61\n5     3          0        5.31    67\n6     3          1        6.33   183\n7     4          0        7.03    58\n8     4          1        8.28   192\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn R, pivot_wider() is a function included in the tidyverse (tidyr) used to convert data from long to wide format. It reshapes data by spreading values from a column into multiple columns, with each unique value in that column becoming a new column. This is particularly useful for creating summary tables or reshaping data for analysis and visualization.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmutate() in R’s dplyr (incl. in tidyverse) adds new columns to a data frame, allowing you to transform or calculate values based on existing columns.\n\n\n\n# Convert to wide format to compute effect\nte_by_group &lt;- profits_by_group |&gt; \n  # Convert column values of consultant to headers and take values from\n  # mean_profit\n  pivot_wider(\n    # For each group, we want one row\n    id_cols = \"group\", \n    # Values of consultants to headers\n    names_from = \"consultant\",\n    # Take values from 'mean_profit'\n    values_from = \"mean_profit\", \n    # Change names of headers for better readability\n    names_prefix = \"consultant_\"\n    ) |&gt; \n  # Compute group treatment effect by subtracting mean of untreated units from\n  # mean of treated units\n  mutate(te = consultant_1 - consultant_0)\n\n# Show table\nprint(te_by_group)\n\n# A tibble: 4 × 4\n  group consultant_0 consultant_1    te\n  &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1     1         2.45         3.44 0.993\n2     2         4.05         5.02 0.964\n3     3         5.31         6.33 1.01 \n4     4         7.03         8.28 1.24 \n\n\nBecause all groups have the same size the weighted average is equal to the average and we can simply take the average to obtain the ATE:\n\n# Taking the average of the treatment effect column. Same as weighted mean,\n# because group sizes are identical.\nmean(te_by_group$te)\n\n[1] 1.1\n\n\n\n# With weighted mean\nweighted.mean(te_by_group$te, c(250, 250, 250, 250))\n\n[1] 1.1\n\n\nOne assumption that needs to be fulfilled for the treatment effect to be valid is the positivity assumption we discussed in the last week. Let’s recall it (slightly rewritten to match the notation of our example):\n\n\n\n\n\n\nNote\n\n\n\nAssumption 6: “Positivity / Overlap / Common Support”.\nFor all values of covariates \\(w\\) present in the population of interest (i.e. \\(w\\) such that \\(P(W=w) &gt; 0\\)), we have \\(0 &lt; P(D=1|W=w) &lt; 1\\).\n\n\nQuestion: Is the positivity assumption satisfied?\n\nAnswer\nYes, because in each group, we have observations from both groups. Take a look at the column nobs in the table profits_by_group.\nFor the sake of demonstration, let’s see what would have happened, had we not adjusted for the previous profits:\n\n# Naive estimate\ny1 &lt;- mean(profits[profits$consultant == 1, ]$profit)\ny0 &lt;- mean(profits[profits$consultant == 0, ]$profit)\ny1 - y0\n\n[1] 2.5\n\n\n\n\n\n\n\n\n\n\n[Optional read] Effect of new feature\nImagine you’re at a software company, introducing a new feature and keen to gauge its impact. To ensure unbiased results, you conduct a randomized rollout: 10% of customers receive the new feature randomly, while the rest do not. Your aim is to assess if this feature enhances customer satisfaction, indirectly measured through Net Promoter Score (NPS). You distribute surveys to both the treated (with the new feature) and control groups, asking if they would recommend your product. Upon analysis, you observe higher NPS scores among customers with the new feature. But can you attribute this difference solely to the feature? To delve into this, start with a graph illustrating this scenario.\nIdentification\nGraphically modeling the described scenario, you’ll end up with:\n\nCoderollout_dag &lt;- 'dag {\nbb=\"0,0,1,1\"\n\"Customer satisfaction\" [pos=\"0, 1\"]\n\"New feature\" [exposure,pos=\"0, 0\"]\nNPS [outcome,pos=\"1,1\"]\nResponse [adjusted,pos=\".8, 0\"]\n\"Customer satisfaction\" -&gt; NPS\n\"Customer satisfaction\" -&gt; Response\n\"New feature\" -&gt; \"Customer satisfaction\"\n\"New feature\" -&gt; Response\nResponse -&gt; NPS\n}'\n\nggdag(rollout_dag, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nArgue, what needs to be adjusted for to compute the effect of the new feature on the net promoter score (NPS).",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#assignment",
    "href": "content/course_weeks/week_02/week_2.html#assignment",
    "title": "2 - Graphical Causal Models",
    "section": "Assignment",
    "text": "Assignment\n\nAccept the Week 2 - Assignment and follow the same steps as last week and as described in the organization chapter.\nSolve the following assignment:\nLoad the data health_program.rds and first take a look at the data.\nImagine the following scenario: you are manager of a company and want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently. Some time after implementing the health_program, you’ll observe the sick_days for each employee and want to estimate the average treatment effect. You know that because of the voluntary basis of participating in the program, other variables might confound your estimate if you compute a naive estimate of the average treatment effect. So you might apply what you have learned in the last weeks about confounding, adjusting etc.\nFirst, load the data health_program.rds and first take a look at the data.For the purpose of this task, we assume that all variables we need for a valid estimation are included in the data.\n\nTake a look at all variables that are included in the data. List all pairs of variables and argue whether there might be a dependency and what direction you assume for that relationship if present.\nMap your explanations from the previous task into a DAG using dagitty and ggdag. Define exposure and treatment for that DAG.\nBased on your DAG, what variables do you need to control for? Explain in a short paragraph.\nEstimate the average treatment effect based on your previous explanations. (Hint: id_cols in pivot_wider can take more than one value by providing a vector id_cols = c(\"var1\", \"var2\")).\nExplain, whether the positivity assumption is satisfied.\nEstimate the average treatment effect without conditioning on any variables and compare to the treatment effect computed in 4. Explain the difference.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#further-optional-reading",
    "href": "content/course_weeks/week_02/week_2.html#further-optional-reading",
    "title": "2 - Graphical Causal Models",
    "section": "Further optional reading",
    "text": "Further optional reading\nIn this chapter, we already used some functions from the tidyverse. Functions from the tidyverse, are often preferred over base R functions for several reasons:\n\nReadability and Expressiveness: Tidyverse functions use a consistent and expressive syntax, making code easier to read and write. This can enhance collaboration and reduce errors.\nPiping: Tidyverse functions work well with the |&gt; pipe operator, allowing for a more fluid and readable data manipulation workflow.\nTidy Data Principles: Tidyverse functions are designed around the principles of tidy data, which promotes a standardized way of organizing data that facilitates analysis and visualization.\n\nFor an overview of base R vs tidyverse functions, you can read here.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html",
    "href": "content/course_weeks/week_07/week_7.html",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#slides-recap",
    "href": "content/course_weeks/week_07/week_7.html#slides-recap",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#practical-example",
    "href": "content/course_weeks/week_07/week_7.html#practical-example",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#assignment",
    "href": "content/course_weeks/week_07/week_7.html#assignment",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html",
    "href": "content/course_weeks/week_08/week_8.html",
    "title": "8 - Difference-in-Difference",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Difference-in-Difference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#slides-recap",
    "href": "content/course_weeks/week_08/week_8.html#slides-recap",
    "title": "8 - Difference-in-Difference",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Difference-in-Difference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#practical-example",
    "href": "content/course_weeks/week_08/week_8.html#practical-example",
    "title": "8 - Difference-in-Difference",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Difference-in-Difference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#assignment",
    "href": "content/course_weeks/week_08/week_8.html#assignment",
    "title": "8 - Difference-in-Difference",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Difference-in-Difference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_01/week_1.html",
    "href": "content/course_weeks/week_01/week_1.html",
    "title": "1 - Introduction to Causal Inference",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure to read and follow the instructions in Organization before reading this section.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "1 - Introduction to Causal Inference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_01/week_1.html#christmas-example",
    "href": "content/course_weeks/week_01/week_1.html#christmas-example",
    "title": "1 - Introduction to Causal Inference",
    "section": "Christmas example",
    "text": "Christmas example\nConsider this scenario: you’re the owner of an online marketplace company that small and medium-sized businesses use to sell advertise and sell their products. The businesses act autonomously regarding prices, advertising etc. Because your revenue depends on the prosperity of these businesses, you want to support them by offering guidance when to implement sales campaigns featuring temporary price drops. From a business perspective, a price drop is beneficial when the increase in number of sold units compensates for the lower price. Therefore,it is important to know the number of additional units sold after a price reduction. For simplicity, we only focus on one particular category, socks, and we examine the weeks leading up to and including Christmas week.\nQuestion: It helps to make oneself as clear as possible what the research question is. So, try to formulate a question that entails very clearly what effect we are interested in.\nAnswer\nWhat is the impact of a price reduction on the number of units sold?\nNow, let’s take a look at some data that you collected which could help you to answer your research question.\n\n\n\n\n\n\nNote\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R. Here, we load the library tidyverse, which is, in fact, a collection of many useful libraries specifically developed for data science tasks.\nIf you only need one particular function from a library, you can pick it by the following syntax: library::function().\nAll libraries/packages you want to use need to be installed first by running install.packages(\"library_to_install\").\n\n\n\n# Load packages from the tidyverse\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# Change the path if needed\nsales &lt;- read_csv(\"xmas_sales.csv\")\n# Currently coded as 0/1, we convert to FALSE/TRUE\nsales$is_on_sale &lt;- as.logical(sales$is_on_sale)\n\nThe data consists of the following columns:\n\n\nstore: unique identifier of store\n\nweeks_to_xmas: weekly data for each store leading up to Christmas\n\navg_week_sales: historical average of sales indicating business size\n\nis_on_sale: sale/price reduction indicator\n\nweekly_amount_sold: average weekly sales during that week\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many other ways to get a first look at your data instead of simply entering the variable name or use print(). Often times you will see head(data, n) to see the first \\(n\\) lines. Just the same you can use tail(data, n) to see the \\(n\\) last lines. If it is mainly numeric data, summary() provides a good overview. If you have many columns, glimpse() from the dplyr package contained in the tidyverse helps you.\n\n\n\n# Simply enter the table name to show its content\nprint(sales)\n\n# A tibble: 2,000 × 5\n   store weeks_to_xmas avg_week_sales is_on_sale weekly_amount_sold\n   &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt; &lt;lgl&gt;                   &lt;dbl&gt;\n 1     1             3           13.0 TRUE                    220. \n 2     1             2           13.0 TRUE                    185. \n 3     1             1           13.0 TRUE                    146. \n 4     1             0           13.0 FALSE                   102. \n 5     2             3           19.9 FALSE                   103. \n 6     2             2           19.9 FALSE                    53.7\n 7     2             1           19.9 FALSE                    13.8\n 8     2             0           19.9 FALSE                     0  \n 9     3             3           18.5 FALSE                    97.0\n10     3             2           18.5 FALSE                    54.7\n# ℹ 1,990 more rows\n\n\nLet’ connect the data from the table with the formula notation you have learned in the lecture. First of all, it is important to state that our units of analysis \\(i\\) are stores.\n\\(D_i\\) denotes the treatment for unit \\(i\\) and for our example it can take either one of the two values:\n\\[\nD_i=\\begin{cases}1 \\ \\text{if unit i received treatment}\\\\0 \\ \\text{otherwise}\\\\\\end{cases}\n\\]\nDon’t be confused by the term treatment, it is not a medical treatment (but can be) and other terms used are intervention or manipulation. Here, the treatment \\(D_i\\) is whether a store dropped its prices in a particular week.\n\n\n\n\n\n\nTip\n\n\n\nSometimes, you will encounter \\(T_i\\) instead of \\(D_i\\). But because in R, T is used to abbreviate the boolean value TRUE and in many other applications, \\(T\\) is reserved for time, we will use \\(D\\) instead.\n\n\nQuestion: What is the data equivalent for the outcome \\(Y_i\\)?\nAnswer\nIt is weekly_amount_sold.\nLet’s revisit the initial research question. We are interested in the effect a price reduction has on sales. We can express it as either\n\nthe effect of \\(T\\) on \\(Y\\)\n\nthe is_on_sale on weekly_amount_sold.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "1 - Introduction to Causal Inference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_01/week_1.html#fundamental-problem-of-causal-inference",
    "href": "content/course_weeks/week_01/week_1.html#fundamental-problem-of-causal-inference",
    "title": "1 - Introduction to Causal Inference",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nTo compute the effect, we would ideally know for each observation the counterfactual outcome, i.e. if it was on sale, how would have been sales if it was not on sales or if it was not on sale, how would have been sales if it was on sale? However, due to the fundamental problem of causal inference, it is impossible to observe both states.\nTherefore, at first, you are often tempted to compare the observations that were on sale (“treated”) with the observations that were not on sale (“control”). Because that’s easy to calculate, let’s plot the result. A good way to compare and plot observations of two groups is a box plot.\n\n\n\n\n\n\nNote\n\n\n\nFor plots, we make use of the package ggplot2 which is included in the tidyverse you already loaded. Both plots show the same data and convey the same message, but because ggplot2 is a package dedicated to data visualization it has some advantages regarding aesthetics and the efficiency in creating plots.\nIf you are interested in an introduction to ggplot2, I recommend this resource written by Megan Hall.\nBy the way, don’t be confused when your plot appears different in terms of colors, fonts etc. compared to the one shown here. It is adjusted to match the website theme.\n\n\n\n# Box plot in base R\n# boxplot(weekly_amount_sold ~ is_on_sale, data = sales)\n\n# Box plot in ggplot2\nggplot(\n  data = sales, # first, provide the data\n  aes( # then, provide the aesthetics (at least X and Y)\n    x = is_on_sale, \n    y = weekly_amount_sold\n    )) +\n  geom_boxplot() # with a \"+\" add what type of plot\n\n\n\n\n\n\n\nWhat do we see? Stores that dropped their prices sell more. It confirms our intuition that people buy more on sale. However, the difference seems very high. Let’s calculate it.\n\n\n\n\n\n\nNote\n\n\n\nTo access a data frame, there are several ways in R. In the following chapters, we will use other ways to extract data but here we use the following syntax: dataframe[condition, ]$column. First you take the data frame that contains the data you want to extract or subset. Then you provide a condition on how to subset. If you just want to have a specific column, you can extract it using the $ operator. Please note that if your condition refers to a column in the same data frame, you need to call the data frame another time like dataframe$filter_column.\nTo compute the average of a column (or more precise: a vector), we use the function mean().\nFor printing several variables, in notebooks, you can just collect them in a vector by c(var1, var2, ...). If you want to name the elements provide a name in quotation marks: c(\"name1\" = var1, ...).\n\n\n\n# Outcome for all observations on sale\nY1 &lt;- sales[sales$is_on_sale == TRUE, ]$weekly_amount_sold\nY1_mean &lt;- mean(Y1)\n# Outcome for all observations not on sale\nY0 &lt;- sales[sales$is_on_sale == FALSE, ]$weekly_amount_sold\nY0_mean &lt;- mean(Y0)\n\n# Show both outcomes and their difference\nc(\n  \"Avg. outcome on sale\" = Y1_mean,\n  \"Avg. outcome not on sale\" = Y0_mean,\n  \"Difference\" = Y1_mean  - Y0_mean\n)\n\n    Avg. outcome on sale Avg. outcome not on sale               Difference \n                     141                       63                       78 \n\n\nThe difference is even higher than the outcome for stores that did not implement a sales campaign. At this point, the alarm bells should ring - a simple comparison is very unlikely to yield a valid result.\nQuestion: What explanations can you think of that distort the relationship between the treatment variable and the outcome? Think back to what has been discussed in the lecture.\nAnswer\nThere might be one or several common causes (confounders). Two causes that affect both is_on_sale and weekly_amount_sold are the (1) business size (or: avg_week_sales) and the (2) time distance to Christmas (weeks_to_xmas). Because (1) larger businesses are more likely to implement sales campaigns and naturally sell more and because (2) sales are often implemented close to Christmas when customers buy anyway.\nSummarizing, there is no way to know the true causal effect of price cuts on units sold as we do not observe both worlds for all units: the world with price cuts and the world without price cuts. That’s what the fundamental problem of causal inference states. Throughout the whole course, we will come up with ways and methods to deal with this problem and get as close to the causal effect as possible.\nUnrealistic scenario\nFor now, let’s imagine the impossible and assume we can actually see both worlds and know both states for each observation. In potential outcomes (PO) notation, that means we can see both \\(Y_{i1}\\) and \\(Y_{i0}\\), where \\(0\\) and \\(1\\) refer to the treatment states for unit \\(i\\).\n\\[\nY_i=\\begin{cases}Y_{i1} \\ \\text{if unit i received treatment}\\\\Y_{i0} \\ \\text{otherwise}\\\\\\end{cases}\n\\]\nWhen you take a look at the table, you see the observed outcome y and both potential outcomes y0 and y1, one of which is the observed and the other one the counterfactual outcome. You also see a store identifier, the treatment status t, a covariate x and the individual treatment effect (\\(ITE\\)) te.\n\n# Read data  \"unrealistic\" scenario\nsales_unreal &lt;- read_csv(\"sales_unreal.csv\")\n\n# Print table\nprint(sales_unreal)\n\n# A tibble: 6 × 7\n      i    y0    y1     t     x     y    te\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   200   220     0     0   200    20\n2     2   120   140     0     0   120    20\n3     3   300   400     0     1   300   100\n4     4   450   500     1     0   500    50\n5     5   600   600     1     0   600     0\n6     6   600   800     1     1   800   200\n\n\nThe ITEs are computed as:\n\\[\n\\tau_i = \\text{ITE}_i = Y_{i1} - Y_{i0}\n\\]\nIn this unrealistic scenario, knowing all states, it is easy to compute the average treatment effect (\\(ATE\\)). For each unit, we subtract the untreated outcome from the treated outcome and take the average. Or even easier, because there are already computed ITEs in the data, we take the average of those.\n\nATE &lt;- mean(sales_unreal$y1 - sales_unreal$y0) # equivalent to: mean(sales_unreal$te)\nATE\n\n[1] 65\n\n\nThe true average causal treatment effect is 65. In formula notation, we calculated the sample equivalent of\n\\[\n\\text{ATE} = E[\\tau_i] = E[Y_{i1} - Y_{i0}] \\,.\n\\]\nWithout any problems, you could also calculate the conditional average treatment effect (\\(CATE\\)), i.e. the average treatment effect for units where the \\(X\\) takes on the specified value \\(x\\).\n\\[\nCATE = E[Y_{i1} - Y_{i0} | X = x]\n\\]\nRealistic scenario\nBut now let’s get back to the actual scenario: we just observe one outcome and we do not what would have happened in a different world. Consequently, the data looks like this:\n\n# Read \"realistic\" scenario\nsales_real &lt;- read_csv(\"sales_real.csv\")\n\n# Print table\nprint(sales_real)\n\n# A tibble: 6 × 7\n      i    y0    y1     t     x     y te   \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n1     1   200    NA     0     0   200 NA   \n2     2   120    NA     0     0   120 NA   \n3     3   300    NA     0     1   300 NA   \n4     4    NA   500     1     0   500 NA   \n5     5    NA   600     1     0   600 NA   \n6     6    NA   800     1     1   800 NA   \n\n\nRemember, the true causal effect is 65. Let’s check, how close we get when we try to estimate the ATE by comparing the treated observations to the untreated observations.\n\n# Average outcome for treated observations\ny1 &lt;- mean(sales_real[sales_real$t == 1, ]$y)\n\n# Average outcome for not treated observations\ny0 &lt;- mean(sales_real[sales_real$t == 0, ]$y)\n\n# Show both outcomes and their difference\nc(\n  \"Avg. treated outcomes\" = y1,\n  \"Avg. not treated outcomes\" = y0,\n  \"Difference\" = y1 - y0\n)\n\n    Avg. treated outcomes Avg. not treated outcomes                Difference \n                      633                       207                       427 \n\n\nIt’s \\(426.667\\) and thus very far off. Again, it proves the danger of taking naive averages and the inequality of association and causation. The reason here is that businesses engaged in sales are different from those that did not and would have sold more regardless of price cut.\nBias\nThe difference is also called bias and with full knowledge of all states can be calculated by:\n\\[\n\\begin{align}\nE[Y_1 - Y_0] &= E[Y|D=1] - E[Y|D=0] \\\\ &= E[Y_1|D=1] - E[Y_0|D=0] + E[Y_0|D=1] - E[Y_0|D=1] \\\\\n&= \\underbrace{E[Y_1 - Y_0|D=1]}_{ATT} + \\underbrace{\\{ E[Y_0|D=1] - E[Y_0|D=0] \\}}_{BIAS}\n\\end{align}\n\\]\nYou see that there is a bias when \\(E[Y_0|D=1]\\) is not equal to \\(E[Y_0|D=1]\\). That means, for no bias to occur, treated and untreated units only differ in their treatment status and is called the ignorability or exchangeability assumption.\nGoing back to the dataset with many observations, what can you infer from this plot? Does the plot indicate any violations of the assumptions?\n\n# Plot business size vs outcome\nggplot(\n  data = sales,\n  aes(x = avg_week_sales, \n      y = weekly_amount_sold, \n      color = is_on_sale) # different color depending on value of 'is_on_sale'\n  ) + geom_point(alpha = .5) # with alpha, we control point transparency",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "1 - Introduction to Causal Inference"
    ]
  }
]