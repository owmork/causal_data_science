[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Literature",
    "section": "Overview",
    "text": "Overview\nDistinguishing causal relationships from simple correlation is what commonly used approaches in business analytics often fall short of. In this course, we will provide you with the skill set to answer questions like\n\n\nwhat happens to \\(Y\\) if we do \\(X\\)?\n\n\nwas it \\(X\\) that caused \\(Y\\) to change?\n\n\nIntroducing you to causal inference with the help of data science will allow you to carry out state-of-the-art causal analyses by yourself and extrapolate causal knowledge across different business contexts and various management areas."
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Literature",
    "section": "Objectives",
    "text": "Objectives\nAfter completing this module, students will be able to:\n\nUnderstand the difference between “correlation” and “causation”\nUnderstand the shortcomings of current correlation-based approaches\nDevelop causal knowledge relevant for specific data-driven decisions\nDiscuss the conceptual ideas behind various causal data science tools and algorithms\nCarry out state-of-the-art causal data analyses"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Literature",
    "section": "Instructors",
    "text": "Instructors\n   Lecture: Christoph Ihl\n   Tutorial: Oliver Mork"
  },
  {
    "objectID": "index.html#details",
    "href": "index.html#details",
    "title": "Literature",
    "section": "Details",
    "text": "Details\n   Lecture: Monday, 11.30 - 13.00\n   Tutorial: Tuesday, 15.00 - 16.30 + 16.45 - 18.15"
  },
  {
    "objectID": "index.html#primary",
    "href": "index.html#primary",
    "title": "Literature",
    "section": "Primary",
    "text": "Primary\n\nDing, Peng (2023). A First Course in Causal Inference. arXiv preprint arXiv:2305.18793.\nFacure, Matheus (2023). Causal Inference in Python - Applying Causal Inference in the Tech Industry. O’Reilly Media.\nHuber, Martin (2023). Causal analysis: Impact evaluation and Causal Machine Learning with applications in R. MIT Press, 2023.\nNeal, Brady (2020). Introduction to causal inference from a Machine Learning Perspective. Course Lecture Notes (draft)."
  },
  {
    "objectID": "index.html#secondary",
    "href": "index.html#secondary",
    "title": "Literature",
    "section": "Secondary",
    "text": "Secondary\n\nAngrist, J. D., & Pischke, J. S. (2014). Mastering metrics: The path from cause to effect. Princeton university press.\nCunningham, Scott (2021). Causal Inference: The Mixtape, New Haven: Yale University Press.\nGertler, Paul J., et al. (2016). Impact evaluation in practice. World Bank Publications.\nHernán Miguel A., and Robins James M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nHuntington-Klein, Nick (2021). The effect: An introduction to research design and causality. Chapman and Hall/CRC.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge University Press.\nMullainathan, Sendhil, and Jann Spiess. (2017). Machine Learning: An Applied Econometric Approach. Journal of Economic Perspectives, 31(2): 87–106.\nPearl, Judea, and Dana Mackenzie (2018). The Book of Why. Basic Books, New York, NY.\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell (2016). Causal Inference in Statistics: A Primer. John Wiley & Sons, Inc., New York, NY.\nPeters, Jonas, Dominik Janzing, and Bernhard Schölkopf (2017). Elements of causal inference: foundations and learning algorithms. The MIT Press."
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html",
    "href": "content/course_weeks/week_06/week_6.html",
    "title": "6 - Effect Heterogeneity",
    "section": "",
    "text": "Up until now, we have primarily discussed the general impact of a treatment. Let’s shift our focus to how treatments can affect different units in various ways. This involves the concept of personalized treatment effects and the improved assignment of treatments, especially in situations where it’s not feasible to treat everyone and prioritization is necessary. This topic is closely related to predictive problems, cross-validation, and model selection, with effect validation being more challenging than for a simple predictive model.\nIn recent weeks, we have occasionally estimated Conditional Average Treatment Effects (CATE) by manually assuming group-level heterogeneity.\n\\[\n\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]\n\\]",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#slides-recap",
    "href": "content/course_weeks/week_06/week_6.html#slides-recap",
    "title": "6 - Effect Heterogeneity",
    "section": "",
    "text": "Up until now, we have primarily discussed the general impact of a treatment. Let’s shift our focus to how treatments can affect different units in various ways. This involves the concept of personalized treatment effects and the improved assignment of treatments, especially in situations where it’s not feasible to treat everyone and prioritization is necessary. This topic is closely related to predictive problems, cross-validation, and model selection, with effect validation being more challenging than for a simple predictive model.\nIn recent weeks, we have occasionally estimated Conditional Average Treatment Effects (CATE) by manually assuming group-level heterogeneity.\n\\[\n\\tau(\\mathbf{x}) = \\mathbb{E}[Y_i(1) - Y_i(0) | \\mathbf{X_i = x}]\n\\]",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#metalearners",
    "href": "content/course_weeks/week_06/week_6.html#metalearners",
    "title": "6 - Effect Heterogeneity",
    "section": "Metalearners",
    "text": "Metalearners\nNow predict individualized treatment effects based on all covariates \\(X_i\\) under the assumption of observed confounding using metalearners.\n\n\n\n\n\n\nTip\n\n\n\nNo proper cross-fitting\nIn the next steps, you are going to code the metalearners by hand. For simplicity, you don’t need to perform proper cross-fitting because the focus is on understanding the mechanisms of metalearners.\n\n\nWe will focus on the R-learner and the DR-learner, which have the advantage of minimizing only one loss function. However, there are also other types of metalearners, such as the S-learner, T-learner, and X-learner.\nIn general, metalearners combine multiple supervised machine learning steps into a pipeline that outputs predicted Conditional Average Treatment Effects (CATEs). The typical process involves:\n\nEstimating nuisance parameters.\nPlugging these estimates into a minimization problem targeting CATE.\nSolving the minimization problem.\nUsing the model learned in step 3 to predict CATE.\n\nR-learner\nA robust and flexible approach to estimating heterogeneous treatment effects is the R-learner, which we will now examine. We will explore two specifications: the partially linear R-learner and the generic machine learning (ML) R-learner.\nPartially linear R-learner\nRelated to the FWL theorem and the double machine learning with partially linear regression, which we discussed last time, the R-learner slightly adjusts the minimization problem by allowing the treatment effects to vary with the covariates \\(mathbf{X}\\).\n\\[\n\\underbrace{Y_i - \\overbrace{\\mathbb{E}[Y_i \\mid \\mathbf{X_i}]}^{\\mu(\\mathbf{X_i})}}_{\\text{outcome residual}} = \\tau(\\mathbf{X_i}) \\underbrace{( T_i - \\overbrace{\\mathbb{E}[T_i \\mid \\mathbf{X_i}]}^{e(\\mathbf{X_i})})}_{\\text{treatment residual}} + \\epsilon_{Y_i}\n\\] The minimization problem is the same except for the potentially heterogeneous treatment effect \\(\\tau(\\mathbf{X_i})\\) instead of the homogeneous treatment effect \\(\\tau)\\).\n\\[\n\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) = \\arg \\min_{\\tau} \\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2\n\\] With some modifications, we can see that by using a modified covariate \\(\\mathbf{X_i}\\), we can estimate with a linear model that minimize squares and solves the minimization problem in the final step.\n\\[\n\\begin{align*}\n\\hat{\\beta}_{RL} &= \\underset{\\beta}{\\operatorname{arg\\,min}} \\sum_{i=1}^N( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\mathbf{\\beta'} \\underbrace{(T_i - \\hat{e}(\\mathbf{X_i})) \\mathbf{X_i}}_{=\\mathbf{\\tilde{X}_i}})^2 \\\\\n&= \\underset{\\beta}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\left( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\mathbf{\\beta'} \\mathbf{\\tilde{X}_i} \\right)^2\n\\end{align*}\n\\] For estimating the outcome model \\(\\hat{\\mu}(\\mathbf{X_i})\\) and the exposure model \\(\\hat{e}(\\mathbf{X_i})\\), we can use well suited ML functions.\n\n\n\n\n\n\nTip\n\n\n\nOf course, there are packages and functions to just specify your data, treatment, outcome and covariates and you’ll get the result. But to better understand what’s going on under the hood, we are going to perform it manually here.\n\n\nImagine the following situation. You are operating an online shop, and a year ago, introduced a plus membership aimed at binding customers and driving revenue growth. The plus memberships comes at a small cost for the customers, which is why not all of the customers subscribed. Now you want to examine whether binding customers by this membership program in fact increases your sales with subscribed customers. However, it’s essential to consider potential confounding variables such as age, gender, or previous average purchase amounts. And this time you also measure a bunch of other variables which might let ML methods be more advantageous due to their way of handling high-dimensional data.\nEach row in your dataset corresponds to a different customer, with accompanying demographic details, their average purchase sum before the introduction of the plus membership, their current average purchase, an indication of their participation in the plus membership program, and the categories they have bought in.\n\nlibrary(tidyverse)\n\nmembership &lt;- readRDS(\"membership.rds\")\nglimpse(membership)\n\nRows: 1,000\nColumns: 27\n$ age                &lt;dbl&gt; 32, 42, 23, 67, 27, 54, 40, 27, 29, 35, 22, 37, 25, 41, 37, 60, 32, 31, 30, 41, 34, 48, 42, 41, 38, 29, 37, 27, 26, 27, 5…\n$ sex                &lt;int&gt; 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0…\n$ pre_avg_purch      &lt;dbl&gt; 22, 32, 32, 47, 30, 31, 38, 26, 26, 47, 36, 55, 32, 26, 48, 41, 23, 23, 33, 30, 31, 25, 43, 37, 22, 40, 33, 24, 23, 37, 3…\n$ card               &lt;int&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ avg_purch          &lt;dbl&gt; 45.45, 72.73, 64.20, 55.66, 80.08, 35.34, 10.22, 45.32, 0.56, 69.84, 53.71, 55.46, 53.50, 44.00, 29.72, 33.08, 32.08, 37.…\n$ vehicle            &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ food               &lt;int&gt; 1, 2, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 1, 2, 1, 0, 0, 1, 1, 2, 1, 2, 4, 1, 0, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 2, 1, 0, 1, 1, 1…\n$ beverage           &lt;int&gt; 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 3, 0, 0, 1, 0, 1, 4, 0, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 0…\n$ art                &lt;int&gt; 0, 2, 1, 2, 1, 0, 0, 1, 1, 1, 3, 0, 0, 3, 1, 3, 1, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 0, 1, 2, 3, 0, 2, 0, 2, 0, 1, 0, 1, 2, 0…\n$ baby               &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 2, 3, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 2, 0, 1, 1…\n$ personal_care      &lt;int&gt; 0, 0, 2, 1, 1, 1, 3, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, 2, 1, 0, 2, 0, 0, 3, 2, 1, 1, 1, 2, 0, 1, 0, 3, 1, 2, 1…\n$ toys               &lt;int&gt; 0, 0, 3, 2, 1, 1, 1, 2, 2, 0, 0, 1, 2, 3, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 2, 3, 0, 1, 0, 1, 1, 1, 1, 1…\n$ clothing           &lt;int&gt; 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 5, 1, 1, 1, 1, 3, 0, 1, 2, 2, 5, 2, 3, 1, 2, 1, 1, 0, 0, 1, 0, 0, 2, 5, 1, 0, 3, 1, 1, 2, 1…\n$ decor              &lt;int&gt; 0, 0, 0, 3, 2, 2, 0, 1, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 3, 1, 2, 2, 2, 0, 0, 1, 1, 2…\n$ cell_phones        &lt;int&gt; 1, 2, 4, 1, 1, 2, 4, 4, 3, 1, 5, 1, 3, 2, 3, 4, 3, 3, 1, 4, 3, 5, 2, 4, 5, 5, 5, 4, 2, 4, 2, 2, 2, 3, 5, 3, 3, 2, 3, 4, 1…\n$ construction       &lt;int&gt; 1, 2, 1, 1, 1, 4, 1, 0, 3, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 2, 1, 1, 3…\n$ home_appliances    &lt;int&gt; 0, 2, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 2, 0, 3, 0, 1, 4, 1, 1, 1, 0, 1, 2, 1, 2, 2…\n$ electronics        &lt;int&gt; 1, 2, 4, 0, 2, 2, 1, 2, 2, 3, 1, 3, 3, 0, 4, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 4, 0, 0, 2, 4, 2, 2, 1, 0, 2, 1, 1, 0, 3, 2…\n$ sports             &lt;int&gt; 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 4, 0, 2, 0, 3, 2, 0, 1, 0, 1, 2, 3, 4, 0, 2, 1, 1, 0, 1, 0, 1, 2…\n$ tools              &lt;int&gt; 3, 2, 1, 1, 1, 2, 1, 0, 4, 2, 1, 0, 0, 2, 0, 1, 1, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0…\n$ games              &lt;int&gt; 4, 1, 3, 1, 2, 1, 3, 1, 0, 3, 0, 1, 1, 7, 1, 3, 0, 1, 2, 3, 0, 2, 2, 1, 2, 6, 3, 1, 2, 1, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2…\n$ industry           &lt;int&gt; 0, 0, 2, 0, 0, 1, 0, 0, 3, 2, 1, 2, 1, 0, 0, 2, 1, 0, 3, 0, 0, 0, 3, 1, 3, 0, 1, 3, 2, 0, 2, 0, 2, 1, 1, 0, 0, 2, 0, 2, 1…\n$ pc                 &lt;int&gt; 4, 2, 2, 2, 3, 0, 2, 1, 3, 0, 3, 2, 3, 3, 2, 1, 0, 0, 3, 2, 3, 2, 1, 1, 3, 1, 3, 2, 2, 0, 2, 4, 2, 1, 0, 0, 4, 2, 3, 2, 0…\n$ jewel              &lt;int&gt; 0, 1, 0, 0, 0, 2, 0, 0, 0, 4, 1, 0, 0, 1, 3, 1, 0, 3, 1, 4, 0, 1, 2, 0, 1, 2, 2, 2, 2, 0, 1, 1, 0, 2, 0, 0, 1, 4, 0, 1, 1…\n$ books              &lt;int&gt; 2, 1, 0, 0, 1, 0, 0, 0, 4, 2, 1, 2, 1, 2, 0, 1, 0, 0, 0, 3, 0, 3, 1, 2, 2, 2, 2, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 3, 0, 2, 1…\n$ music_books_movies &lt;int&gt; 1, 3, 3, 2, 2, 1, 1, 0, 2, 2, 0, 0, 2, 3, 2, 0, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 0, 1, 2, 0, 1, 0, 2, 0, 2, 1, 2, 0, 1, 0, 0…\n$ health             &lt;int&gt; 0, 0, 2, 2, 6, 0, 4, 2, 2, 2, 2, 0, 3, 1, 0, 2, 3, 2, 1, 3, 4, 2, 3, 1, 2, 1, 5, 3, 2, 2, 5, 1, 5, 2, 1, 0, 0, 4, 5, 4, 3…\n\n\nAgain, we are going to rely on the mlr3 packages for ML models.\n\n# Load packages\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nIn the first step, we train the models for the nuisance parameters \\(\\hat{\\mu}(\\mathbf{X_i})\\) and \\(\\hat{e}(\\mathbf{X_i})\\).\nQ1: Specify the nuisance models. Then, train and predict to obtain the residuals.\n\nCode## Specify\n# Prediction model for the treatment/exposure\ntask_e &lt;- as_task_classif(membership |&gt; select(-avg_purch), target = \"card\")\nlrnr_e &lt;- lrn(\"classif.log_reg\", predict_type = \"prob\")\n\n# Prediction model for the outcome\ntask_m &lt;- as_task_regr(membership |&gt; select(-card), target = \"avg_purch\")\nlrnr_m &lt;- lrn(\"regr.lm\")\n\n## Train\nlrnr_m$train(task_m)\nlrnr_e$train(task_e)\n\n## Predict\nm_pred_tbl &lt;- lrnr_m$predict(task_m)\ne_pred_tbl &lt;- lrnr_e$predict(task_e)\n\n## Residualize\n# True values\nD &lt;- membership$card\nY &lt;- membership$avg_purch\n\n# Residuals\nY_res &lt;- Y - m_pred_tbl$response\nD_res &lt;- D - e_pred_tbl$prob[, 2]\n\n\nNow, you can construct the modified covariates as obtained in the minimization problem.\n\\[\n\\mathbf{\\tilde{X}_i} = (T_i - \\hat{e}(\\mathbf{X_i})) \\mathbf{X_i}\n\\] Please note, that you also have to include the intercept column with all values equal to 1, e.g. by using rep(1, n) or mutate(intercept = 1).\nQ2: Construct a data frame or matrix that contains the modified covariates and the intercept.\n\nCode# Get matrix of unmodified covariates\nX &lt;- membership |&gt; select(-avg_purch, -card) |&gt; mutate(intercept = 1)\nX &lt;- as.matrix(X)\n\n# Modify by multiplying with residuals\nX_tilde &lt;- X * D_res\n\n\nNow, you need to solve the minimization problem. Since we are focusing on a linear model, you can use the lm() function. Alternatively, you can apply linear shrinkage estimators, such as Lasso.\nQ3: Run a regression of the residualized outcome on the modified covariates and report the results.\n\nCode# Partially linear R-Learner\nrl_pl &lt;- lm(Y_res ~ 0 + X_tilde)\nsummary(rl_pl)\n\n\nCall:\nlm(formula = Y_res ~ 0 + X_tilde)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46.41 -10.92   0.22  10.49  39.31 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \nX_tildeage                 -1.1424     0.0842  -13.56  &lt; 2e-16 ***\nX_tildesex                  6.6572     1.9958    3.34  0.00088 ***\nX_tildepre_avg_purch       -0.1253     0.1014   -1.24  0.21698    \nX_tildevehicle              0.5023     3.2101    0.16  0.87568    \nX_tildefood                -0.5471     1.0029   -0.55  0.58550    \nX_tildebeverage            -0.2443     1.0019   -0.24  0.80743    \nX_tildeart                  0.7067     0.9858    0.72  0.47365    \nX_tildebaby                -0.9090     1.0073   -0.90  0.36707    \nX_tildepersonal_care       -1.1645     0.9598   -1.21  0.22533    \nX_tildetoys                 1.2214     0.9722    1.26  0.20928    \nX_tildeclothing            -0.0405     0.7201   -0.06  0.95517    \nX_tildedecor                0.6599     0.9482    0.70  0.48663    \nX_tildecell_phones         -0.1243     0.5666   -0.22  0.82637    \nX_tildeconstruction         0.2585     0.9667    0.27  0.78921    \nX_tildehome_appliances      0.3960     1.0720    0.37  0.71192    \nX_tildeelectronics         -0.2746     0.6926   -0.40  0.69184    \nX_tildesports              -0.1140     0.9825   -0.12  0.90769    \nX_tildetools               -0.9351     1.0366   -0.90  0.36723    \nX_tildegames               -0.4941     0.6857   -0.72  0.47133    \nX_tildeindustry             0.8194     1.0548    0.78  0.43744    \nX_tildepc                   1.1932     0.7115    1.68  0.09384 .  \nX_tildejewel                0.8425     0.9792    0.86  0.38978    \nX_tildebooks               -0.3454     0.9799   -0.35  0.72454    \nX_tildemusic_books_movies   0.9734     0.9791    0.99  0.32035    \nX_tildehealth               0.4330     0.7162    0.60  0.54560    \nX_tildeintercept           52.0186     6.3475    8.20  7.8e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15 on 974 degrees of freedom\nMultiple R-squared:  0.26,  Adjusted R-squared:  0.24 \nF-statistic: 13.1 on 26 and 974 DF,  p-value: &lt;2e-16\n\n\nHaving trained the model, you obtain coefficients which you can multiply with \\(\\mathbf{X}\\) (not \\(\\mathbf{\\tilde{X}}\\)) to get the \\(CATE\\).\n\n\n\n\n\n\nNote\n\n\n\nMatrix multiplication in R is done using mn %*% nm. Make sure to use the correct order.\n\n\n\\[\n\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) = \\mathbf{\\hat{\\beta}_{RL} x} \\neq \\mathbf{\\hat{\\beta}_{RL} \\tilde{x}}\n\\] \nQ4: Get the predicted conditional average treatment effects CATE.\n\nCode# Multiply covariates with coefficient vector\nrl_pl_CATE &lt;- X %*% rl_pl$coefficients\nhist(rl_pl_CATE, breaks = 30)\n\n\n\n\n\n\n\nGeneric ML R-learner\nIn the previous estimation, we imposed linearity on the CATE. However, sometimes you might not want to assume linearity. By rearranging the minimization problem, you can see that you can use any machine learning method capable of handling weighted minimization.\n$$ \\[\\begin{align*}\n\\hat{\\tau}_{\\text{RL}}(\\mathbf{x}) &= \\arg \\min_{\\tau} \\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tau(\\mathbf{X_i}) ( T_i - \\hat{e}(\\mathbf{X_i})))^2 \\\\\n\n&= ... \\\\\n&= \\arg \\min_{\\tau} \\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2 \\bigg(\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i})}{ T_i - \\hat{e}(\\mathbf{X_i})} - \\tau(\\mathbf{X_i})\\bigg)^2 \\\\\n\\end{align*}\\] $$\nThe estimation procedure again requires the outcome and treatment model estimated as a first step to obtain residuals. Then, however, you proceed with the\n\nweights\n\n\\[\n(T_i - \\hat{e}(\\mathbf{X_i}))^2,\n\\]\n\na pseudo-outcome\n\n\\[\\frac{Y_i - \\hat{\\mu}(\\mathbf{X_i})}{ T_i - \\hat{e}(\\mathbf{X_i})}\\] and 3. unmodified covariates \\(X\\).\nQ5: Construct the weights and the pseudo-outcome.\n\nCode# Pseudo - outcome\nY_pseudo_rl &lt;- Y_res / D_res\n\n# Weight\nweight_rl &lt;- D_res^2\n\n# Add pseudo-outcome, weights and unmodified covariates to a matrix/data frame\ndata_rl &lt;- cbind(X, Y_pseudo_rl, weight_rl)\n\n\n\n\n\n\n\n\nTip\n\n\n\nHints: ML methods capable of performing weighted minimization are e.g. lrn(\"regr.xgboost\") or lrn(\"regr.randomForest\"). To specify the weight, you can use: task_rl$set_col_roles(col = \"weight_rl\", roles = \"weight\").\n\n\nQ6: Specify a ML method capable of performing weighted minimization. Then, train and predict.\n\nCode## Specify task and learner\n# Task\ntask_rl &lt;- as_task_regr(data_rl, target = \"Y_pseudo_rl\")\n# Assign weight column\ntask_rl$set_col_roles(col = \"weight_rl\", roles = \"weight\")\n\n# Specify learner\nlrnr_rl &lt;- lrn(\"regr.xgboost\")\n\n# Train\nlrnr_rl$train(task_rl)\n\n# Predict CATEs\nrl_ml_CATE &lt;- lrnr_rl$predict(task_rl)$response\nhist(rl_ml_CATE, breaks = 30)\n\n\n\n\n\n\n\nDR-learner\nThe doubly robust learner we discussed in the previous chapter is also a metalearner we can exploit to estimate heterogeneous effects.\n\\[\n\\tau_{\\text{DR}}(\\mathbf{x}) = \\mathbb{E}\\bigg[ \\underbrace{\\hat{\\mu}(1, \\mathbf{X_i}) - \\hat{\\mu}(0, \\mathbf{X_i}) + \\frac{T_i(Y_i - \\hat{\\mu}(1, \\mathbf{X_i}))}{ \\hat{e}_1(\\mathbf{X_i})} - \\frac{(1-T_i)(Y_i - \\hat{\\mu}(0, \\mathbf{X_i})}{\\hat{e}_0(\\mathbf{X_i}))}}_{\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}} \\bigg| \\mathbf{X_i= x} \\bigg]\n\\]\nYou might remember that we predicted both potential outcomes and obtained estimates of the individual treatment effect.\n\\[\n\\tau_{\\text{DR}}(\\mathbf{x}) = \\mathbb{E}\\bigg[ \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} \\bigg| \\mathbf{X_i= x} \\bigg]\n\\]\nThe only step left is then to regress these estimates on the assumed drivers of heterogeneity in effects.\n\\[\n\\hat{\\tau}_{RL}(\\mathbf{x}) = \\underset{\\tau}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\left( \\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}} - \\tau(\\mathbf{X_i})\\right)^2\n\\]\nQ7: Specify your nuisance parameters. Then, train and predict to obtain m0_hat, m1_hat and e_hat.\n\nCode## Train outcome models\n# Y0 ~ X\nlrnr_m$train(task_m, row_ids = which(membership$card == 0))\nm0_hat &lt;- lrnr_m$predict(task_m)$response\n\n# Y1 ~ X\nlrnr_m$train(task_m, row_ids = which(membership$card == 1))\nm1_hat &lt;- lrnr_m$predict(task_m)$response\n\n# D ~ X (already trained)\ne_hat &lt;- e_pred_tbl$prob[, 2]\n\n\nQ8: Construct the pseudo-outcome used to compute the ATE \\({\\text{ATE}}^{\\text{AIPW}}\\). Regress it on \\(X\\) and obtain the fitted values.\n\nCode# Pseudo-outcome\ntau_dr &lt;- m1_hat - m0_hat + D * (Y - m1_hat) / e_hat - (1 - D) * (Y - m0_hat) / (1-e_hat)\n\n# Fit regression\ndrl_mod &lt;- lm(tau_dr ~ 0 + X)\n# \"Predict\"\ndrl_CATE &lt;- drl_mod$fitted.values\n\n# Plot histogram\nhist(drl_CATE, breaks = 30)\n\n\n\n\n\n\n\nComparison\n\n# Store and plot predictions\nresults &lt;- tibble(\n  \"R-learner (partially linear)\" = rl_pl_CATE,\n  \"R-learner (ML)\" = rl_ml_CATE,\n  \"DR-learner\" = drl_CATE\n  )\n\n\n# Correlation of CATEs by different methods\ncor(results, method = \"pearson\")\n\n                             R-learner (partially linear) R-learner (ML) DR-learner\nR-learner (partially linear)                          1.0            0.8        1.0\nR-learner (ML)                                        0.8            1.0        0.8\nDR-learner                                            1.0            0.8        1.0\n\n\n\n# Summary statistics\nsummary(results)\n\n R-learner (partially linear).V1 R-learner (ML)    DR-learner \n Min.   :-57                     Min.   :-12.2   Min.   :-57  \n 1st Qu.: -5                     1st Qu.: -2.0   1st Qu.: -5  \n Median :  8                     Median :  1.9   Median :  8  \n Mean   :  7                     Mean   :  2.3   Mean   :  7  \n 3rd Qu.: 20                     3rd Qu.:  5.7   3rd Qu.: 20  \n Max.   : 45                     Max.   : 12.4   Max.   : 46  \n\n\n\nCode# Reshape data to long format\nresults_long &lt;- results %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Method\", values_to = \"Value\") |&gt; \n  mutate(Method = factor(Method, levels = c(\"R-learner (partially linear)\", \"R-learner (ML)\", \"DR-learner\")))\n\n# Create the histograms\nggplot(results_long, aes(x = Value)) +\n  geom_histogram(binwidth = 0.5, alpha = 0.75) + # Adjust binwidth as needed\n  facet_wrap(~ Method, scales = \"free_x\") +\n  theme_minimal() +\n  labs(title = \"Histograms of CATE Estimates by Method\",\n       x = \"CATE\",\n       y = \"Frequency\")",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#evaluation-of-effect-heterogeneity",
    "href": "content/course_weeks/week_06/week_6.html#evaluation-of-effect-heterogeneity",
    "title": "6 - Effect Heterogeneity",
    "section": "Evaluation of effect heterogeneity",
    "text": "Evaluation of effect heterogeneity\nWhile we have learned to estimate the CATE and observe variation in the estimated individual response to the treatment, we have not yet determined whether this heterogeneity is systematic or merely noise. Evaluating our estimated CATEs is challenging because we cannot use the typical out-of-sample approach due to the missing counterfactual. Additionally, using ML methods in the final step provides no statistical inference.\nTo address this, we can derive summary statistics for the distribution of CATEs and jointly test for the presence of heterogeneity and our ability to detect it. Three methods we will explore are:\n\n\nBLP: Best Linear Predictor\n\nGATES: High-vs-low Sorted Group Average Treatment Effect\n\nCLAN: Classification Analysis\n\nFor the further analysis, we will rely on the GenericML package, which provides the functions get_BLP(), get_GATES() and getCLAN() which we can apply on a GenericML object.\n\ninstall.packages(\"GenericML\")\n\n\n# Load package\nlibrary(GenericML)\n\nIt also builds upon the mlr3 package and we can again specify our learners. Here, we can also specify more than one learner and let GenericML figure out which one performs best.\n\n# Specify set of learners\nlrnr &lt;- c(\"mlr3::lrn('svm')\", 'mlr3::lrn(\"ranger\", num.trees = 100)')\n\nADD good explanation\n\n# Specify covariates to use\nX1 &lt;- setup_X1(funs_Z = c(\"S\", \"B\", \"p\"))\n\n\n\n\n\n\n\nNote\n\n\n\nYou might get a warning saying that some of your propensity scores are outside the interval [0.35, 0.65]. Generally, it is only recommended to use the package when having randomized data. For the sake of demonstration, we will ignore the warning here.\n\n\n\ngen_ML &lt;- GenericML(\n  # data\n  Z = X, \n  D = D,\n  Y = Y,\n  # learners\n  learners_GenericML = lrnr,\n  learner_propensity_score = \"lasso\",\n  # algorithm\n  num_splits = 10,\n  quantile_cutoffs = c(0.2, 0.4, 0.6, 0.8),\n  # regression setup\n  X1_BLP = X1,\n  X1_GATES = X1,\n  # computational issues\n  parallel = TRUE, \n  num_cores = 6, \n  seed = 11\n)\n\nBest linear predictor\nFirst, we take a look at the best linear predictor (BLP). It aims to provide the solution of the hypothetical regression of the true CATE on the demeaned predicted CATE. Thereby, it attempts to answer whether heterogeneity is present and we are able to detect it. It is a straightforward way to check the quality of our predictions.\n\\[\n\\mathbb{E}[\\tau(\\mathbf{X_i}) | \\hat{\\tau}(\\mathbf{X_i}) ] := \\beta_1 + \\beta_2\\underbrace{(\\hat{\\tau}(\\mathbf{X_i}) - \\mathbb{E}[\\hat{\\tau}(\\mathbf{X_i})])}_{\\text{demeaned prediction}}\n\\] Because we do not know the actual true treatment effect, we use a pseudo-response that we regress on the demeaned CATE.\nUsing GenericML, we just have to run:\n\nget_BLP(gen_ML, plot = TRUE)\n\nBLP generic targets\n---\n       Estimate CI lower CI upper p value\nbeta.1    6.383    3.560    9.206       0\nbeta.2    1.180    0.938    1.423       0\n---\nConfidence level of confidence interval [CI lower, CI upper]: 90 %\n\n\n\n\n\n\n\n\nQuestion: What does \\(\\beta_1\\) represent?\nQuestion: What does \\(\\beta_2\\) represent? What, if \\(\\beta_2 = 0\\)?\nSorted group average treatment effects\nSorted group average treatment effects (GATES) offers insights into the distribution and heterogeneity of treatment effects. The procedure involves sorting and slicing the distribution of the predicted treatment effects \\(\\hat{\\tau}(\\mathbf{X_i})\\) into \\(K\\) parts and then compute \\(ATE\\)s for each of the slices. When these slices are different from each other, we can identify subgroups which response differently to the treatment.\nMathematically, we ran\n\\[\n\\gamma_k := \\mathbb{E}[ \\tau(\\mathbf{X_i}) | G_k].\n\\]\nWhen the estimator approximates well, we expect to see monotonicity in estimates:\n\\[\n\\gamma_1 \\leq \\gamma_2 \\leq \\ldots \\leq \\gamma_K\n\\] Using GenericML, we just have to run:\n\nget_GATES(gen_ML, plot = TRUE)\n\nGATES generic targets\n---\n                Estimate CI lower CI upper p value\ngamma.1          -14.179  -21.268   -7.208    0.00\ngamma.2            1.759   -4.916    8.786    0.55\ngamma.3            5.966   -0.635   12.567    0.04\ngamma.4           15.179    8.566   21.791    0.00\ngamma.5           22.919   16.526   29.377    0.00\ngamma.5-gamma.1   37.383   27.915   46.900    0.00\n---\nConfidence level of confidence interval [CI lower, CI upper]: 90 %\n\n\n\n\n\n\n\n\nQuestion: Discuss the output. Do you see the assumed monotonicity in the estimates?\nClassification analysis\nThe purpose of classification analysis (CLAN) is to explore what drives the heterogeneous effects. Instead of regressing a pseudo-outcome as in GATES, covariates, which are potential drivers of heterogeneity in treatment effect are regressed upon. In other words, CLAN is a simple mean comparison of covariates in groups split by the size of their treatment effect.\n\nget_CLAN(gen_ML, variable = \"age\", plot = TRUE)\n\nCLAN generic targets for variable 'age' \n---\n                Estimate CI lower CI upper p value\ndelta.1             59.6     57.8     61.4       0\ndelta.2             46.2     44.7     47.7       0\ndelta.3             39.5     38.1     40.8       0\ndelta.4             32.9     31.3     34.5       0\ndelta.5             26.5     25.3     27.7       0\ndelta.5-delta.1    -33.3    -35.4    -31.2       0\n---\nConfidence level of confidence interval [CI lower, CI upper]: 90 %\n\n\n\n\n\n\n\n\n\nget_CLAN(gen_ML, variable = \"sex\", plot = TRUE)\n\nCLAN generic targets for variable 'sex' \n---\n                Estimate CI lower CI upper p value\ndelta.1           0.4700   0.3717   0.5683    0.00\ndelta.2           0.4600   0.3618   0.5582    0.00\ndelta.3           0.4700   0.3717   0.5683    0.00\ndelta.4           0.5150   0.4115   0.6085    0.00\ndelta.5           0.5800   0.4828   0.6772    0.00\ndelta.5-delta.1   0.1000  -0.0386   0.2386    0.12\n---\nConfidence level of confidence interval [CI lower, CI upper]: 90 %\n\n\n\n\n\n\n\n\nQuestion: Interpret the plots.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_06/week_6.html#assignment",
    "href": "content/course_weeks/week_06/week_6.html#assignment",
    "title": "6 - Effect Heterogeneity",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\nAccept the Week 6 - Assignment and follow the same steps as last week and as described in the organization chapter.\nFor the assignments you are going to use music.rds. Consider the following scenario: you are a data scientist at a music streaming company. Naturally, you’re keen to engage users for as long as possible and encourage them to become avid users of your streaming app. Your colleagues from the department responsible for recommendation algorithms have developed a new fancy algorithm which aims to enhance the daily number of minutes of music listening by offering particularly well tailored recommendations. Following common practice of A/B testing, you randomly assign your users into a treatment group (new algorithm) and a control group (no new algorithm). The outcome variable is minutes_spent and the treatment variable is new_algo.\n\nAs you’ve learned, in an A/B test you just need to regress the outcome on the treatment. Follow that and interpret the estimated ATE.\nTake a look at the two formula rearrangements for the R-learner. For both approaches, construct the predicted treatment effects and coefficients and show the equivalence. (You don’t need to use cross-fitting)\nUsing the DoubleML package, run the DR-learner and check for CATEs based on your available covariates. Do you find heterogeneous treatment effects? If yes, interpret them (also with regard to your coefficient in the first task). Please note that to extract the doubly robust \\(\\tilde{\\tau_i}_{\\text{ATE}}\\), you need to access dr_mod$psi_b.\nFor the covariate app_usage_frequency, use the package np to check for non-linear heterogeneous treatment effects. Check lecture slide 12 for more details. Discuss the result.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "6 - Effect Heterogeneity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html",
    "href": "content/course_weeks/week_09/week_9.html",
    "title": "9 - Difference-in-Differences",
    "section": "",
    "text": "So far, we have primarily discussed cross-sectional data, where each unit is observed only once. However, sometimes you can observe units over multiple time periods, resulting in panel data. This allows us to see how a unit changes before and after a treatment. When randomization isn’t feasible, panel data is the best alternative for identifying causal effects. The most well-known method for estimating this effect is called difference-in-differences (DiD).",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#slides-recap",
    "href": "content/course_weeks/week_09/week_9.html#slides-recap",
    "title": "9 - Difference-in-Differences",
    "section": "",
    "text": "So far, we have primarily discussed cross-sectional data, where each unit is observed only once. However, sometimes you can observe units over multiple time periods, resulting in panel data. This allows us to see how a unit changes before and after a treatment. When randomization isn’t feasible, panel data is the best alternative for identifying causal effects. The most well-known method for estimating this effect is called difference-in-differences (DiD).",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#panel-data",
    "href": "content/course_weeks/week_09/week_9.html#panel-data",
    "title": "9 - Difference-in-Differences",
    "section": "Panel Data",
    "text": "Panel Data\nIn marketing, randomization is often impractical because you cannot always control who receives the treatment. For instance, let’s say you decide to place billboards in a city to advertise your app, aiming to measure the resulting number of downloads attributed to these billboards. You will advertise in some cities but not in others, creating a geo experiment. Additionally, you will track several time-invariant covariates:\n\n\n\\(age_i\\): age average in city \\(i\\)\n\n\n\\(population\\_size_i\\): integer indicating population size of city \\(i\\)\n\n\n\\(user\\_share_i\\): initial share of app users prior to observation period in city \\(i\\)\n\n\n\\(competitor\\_price_i\\): average of competitor app price in that given area\n\nUnlike your competitor, your app is free to download, and you plan to generate revenue within the app. Our observed units are cities \\(i\\) over multiple time periods \\(t\\). Let’s have a look at the data.\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Read data\ndf &lt;- readRDS(\"mkt_panel.rds\")\n\n# Print first lines\nprint(df |&gt; head(10))\n\n   city month post downloads ever_treated age user_share population_size competitor_price\n1     1     1    0       418            1  44       0.23              15              2.2\n2     1     2    0       437            1  44       0.23              15              2.2\n3     1     3    0       472            1  44       0.23              15              2.2\n4     1     4    0       460            1  44       0.23              15              2.2\n5     1     5    0       509            1  44       0.23              15              2.2\n6     1     6    0       524            1  44       0.23              15              2.2\n7     1     7    0       568            1  44       0.23              15              2.2\n8     1     8    0       605            1  44       0.23              15              2.2\n9     1     9    1       640            1  44       0.23              15              2.2\n10    2     1    0       461            1  45       0.23               8              3.1\n\n\nWe see that for city \\(i=1\\), we have eight months with \\(post = 0\\) and 1 month with \\(post = 1\\). The outcome changes over time but the covariates are time invariant. Please also note that the treatment indicator indicates whether a unit was “ever” treated.\nWhen counting the number of control and treatment units per period, we’ll see that we have a complete panel without any missing data.\n\n# Panel data overview\ndf_piv &lt;- df |&gt;\n  count(month, ever_treated, post) |&gt; \n  pivot_wider(\n    names_from = \"ever_treated\",\n    values_from = \"n\",\n    names_prefix = \"ever_treated_\"\n    )\n\n# Print\nprint(df_piv)\n\n# A tibble: 9 × 4\n  month  post ever_treated_0 ever_treated_1\n  &lt;int&gt; &lt;dbl&gt;          &lt;int&gt;          &lt;int&gt;\n1     1     0             91            109\n2     2     0             91            109\n3     3     0             91            109\n4     4     0             91            109\n5     5     0             91            109\n6     6     0             91            109\n7     7     0             91            109\n8     8     0             91            109\n9     9     1             91            109",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#basic-differences-in-differences",
    "href": "content/course_weeks/week_09/week_9.html#basic-differences-in-differences",
    "title": "9 - Difference-in-Differences",
    "section": "Basic differences-in-differences",
    "text": "Basic differences-in-differences\nNot always you will have several pre-treatment periods and DiD already works when you only observe one period before and one period after the treatment. So let’s image that scenario and later re-introduce the other pre-treatment periods.\n\n# Last pre-treatment and post-treatment period\ndf_2p &lt;- df |&gt; filter(month %in% 8:9)\n\n# Print\nprint(df_2p |&gt; count(month, ever_treated, post))\n\n  month ever_treated post   n\n1     8            0    0  91\n2     8            1    0 109\n3     9            0    1  91\n4     9            1    1 109\n\n\nBecause we observe both control and treated units before and after the treatment, we can compute the treatment effect \\(\\tau_{\\text{DiD}}\\) using the difference-in-differences method. This involves first calculating the difference in outcomes between the treated and control groups after the treatment, then subtracting the difference in outcomes between these groups before the treatment. This double differencing helps isolate the treatment effect.\n\\[\n\\begin{aligned}\n\\tau_{\\text{DiD}} = \\underbrace{\\mathbb{E}[Y_{i,1} | D_i=1] - \\mathbb{E}[Y_{i,1} | D_i=0]}_{\\text{difference in post-treatment}} \\\\ - \\underbrace{(\\mathbb{E}[Y_{i,0} | D_i=1] - \\mathbb{E}[Y_{i,0} | D_i=0])}_{\\text{difference in pre-treatment}}\n\\end{aligned}\n\\]\nIn a slightly different and cleaned up notation, we see that the difference we obtain is only attributed to the treatment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nTime\nOutcome\n1st Difference\nDiD\n\n\n\nTreatment (D=1)\n0\n\\(Y= Y_{T=0, D=1}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0,D=1} + T + D\\)\n\\(T +D\\)\n\n\n\n\n\n\n\n\\(D\\)\n\n\nControl (D=0)\n0\n\\(Y = Y_{T=0, D=0}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0, D=0} + T\\)\n\\(T\\)\n\n\n\n\n\n\n\n\n\nTask 1: With the help of the formula/table, compute \\(\\tau_{\\text{DiD}}\\). What is the interpretation with regard to our example?\n\n# ...\n\n\n# Step 2: Extract values and doubly differencing\nwith(did_p2, {\n  y_00 &lt;- y[ever_treated == 0 & post == 0]\n  y_01 &lt;- y[ever_treated == 0 & post == 1]\n  y_10 &lt;- y[ever_treated == 1 & post == 0]\n  y_11 &lt;- y[ever_treated == 1 & post == 1]\n  \n  (y_11 - y_10) - (y_01 - y_00)\n})\n\n[1] 15\n\n\nAlternatively, we can use regression-based approaches to estimate the treatment effect. Two different approaches yielding equivalent results are:\n\nTwo-way fixed effects (TWFE).\n\n\\[\nY_{i,t} = \\alpha_i + \\gamma_t + \\tau_{\\text{DiD}} (D_i \\times t) + \\epsilon_{i,t}\n\\]\n\nRegression with treatment and time dummy, as well as interaction of treatment and time.\n\n\\[\nY_{i,t} = \\alpha + \\beta D_i + \\gamma t + \\tau_{\\text{DiD}} (D_i \\times t) + \\epsilon_{i,t}\n\\] Task 2: Perform both approaches and report the estimated coefficient.\n\n# ...\n\n\n# ...\n\nFor two-way fixed effects estimation, we can generally also use the lm() command which we have used for so many applications already. However, it is recommended to use the fixest package due to its increased speed and focus on fixed effects. As it is the workhorse of many other packages, let’s get it installed.\n\ninstall.packages(\"fixest\")\n\nThe formula notation is slightly different as we separate the fixed effects using the | operator. This approach allows the algorithm to converge faster, and the fixed effects will not appear in the regression summary. Since fixed effects are typically not of primary interest and we often have a large number of them, this simplification is quite convenient.\n\nCode# Faster way for (1)\nlibrary(fixest)\nsummary(feols(downloads ~ ever_treated:post | city + month, data = df_2p))\n\nOLS estimation, Dep. Var.: downloads\nObservations: 400\nFixed-effects: city: 200,  month: 2\nStandard-errors: Clustered (city) \n                  Estimate Std. Error t value  Pr(&gt;|t|)    \never_treated:post       15          3       5 1.399e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 10.4     Adj. R2: 0.98258 \n             Within R2: 0.112107",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#parallel-trends",
    "href": "content/course_weeks/week_09/week_9.html#parallel-trends",
    "title": "9 - Difference-in-Differences",
    "section": "Parallel trends",
    "text": "Parallel trends\nHere, where we only have two time period, we need to rely on one assumption to hold: the parallel trends assumption. Opposed to methods where we just know one outcome - the “after” outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential outcomes are equal \\((Y_1(0)-Y_0(0)) \\perp\\!\\!\\!\\perp T\\). That is a big difference, because do not have to assume that observation units are similar in all their characteristics. Instead DiD hinges on a different assumption, the parallel trends assumption. It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but only the change of outcomes from before to after. It implies that there is no factor that has only an impact on just one of the groups.\n\\[\n\\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | D_i=1] = \\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | D_i=0]\n\\]\nWhile the parallel trends assumption is generally untestable, in the case with only two periods, we are particularly in the dark.\nGraphically, what we assume is that the treated line would have evolved similarly to the control line - as depicted by the dotted line. What can we do to increase the plausibility of this assumption?\n\nCodey10 &lt;- did_p2[did_p2$ever_treated==1 & did_p2$post==0, ]$y\ny01 &lt;- did_p2[did_p2$ever_treated==0 & did_p2$post==1, ]$y\ny00 &lt;- did_p2[did_p2$ever_treated==0 & did_p2$post==0, ]$y\n\ndata_cf &lt;- tibble(\n  x = c(0, 1),\n  y = c(y10, y10 + (y01-y00))\n)\n\nggplot(did_p2, aes(x = post, y = y, group = ever_treated, color = as.factor(ever_treated))) +\n  geom_point() +\n  geom_line() +\n  geom_line(data = data_cf, aes(x=x, y=y, group = NA), color=\"#5AFFC5\", linetype = \"dotted\") +\n  geom_vline(xintercept = .5, linetype = \"dashed\") +\n  scale_x_continuous(breaks=c(0,1)) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#testing-for-parallel-trends",
    "href": "content/course_weeks/week_09/week_9.html#testing-for-parallel-trends",
    "title": "9 - Difference-in-Differences",
    "section": "Testing for parallel trends",
    "text": "Testing for parallel trends\nThere is some remedy when we are able to observe multiple periods prior to the treatment. By comparing trends before treatment between the treatment and control groups, researchers aim to demonstrate that there was no significant difference prior to the treatment. The logic is that if there was no difference before treatment, any difference observed after the treatment is likely due to the treatment itself.\nHowever, even that cannot provide full certainty about the parallel trends assumption. There may still be unobserved factors that could affect the treatment. Nevertheless, event studies are a useful tool for arguing that the treatment and control groups are comparable.\nLet’s now switch back to the dataset with multiple pre-treatment periods and begin by plotting the observed outcomes over time. This will help us determine if there are converging or diverging trends between the treatment and control groups. By visualizing these trends, we can already assess whether the parallel trends assumption holds before the treatment.\n\nCode# Outcomes across time by group\ndf |&gt; \n  ggplot(aes(\n    x = month,\n    y = downloads,\n    fill = as.factor(ever_treated),\n    color = as.factor(ever_treated)\n  )) + \n  stat_summary(fun.data = mean_se, geom = \"point\", alpha = .8) +\n  stat_summary(fun.data = mean_se, geom = \"line\", alpha = .8) +\n  stat_summary(fun.data = mean_se, geom = \"ribbon\", alpha = .2, color = NA) +\n  geom_vline(xintercept = 8.5, linetype = \"dashed\") +\n  scale_x_continuous(breaks=1:9) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAt the first glance, prior to the treatment, the outcomes seem to evolve in parallel. However, a better way is to actually perform statistical tests and check the hypothesis that the evolution of both groups is parallel. For each difference between two pre-treatment periods, the should be no difference in the evolution of the outcomes across the treatment groups.\nTask 3a: Subset the data so you can perform test for pre-trends between the pre-treatment months 7 and 8 using the TWFE approach from above.\n\n# ...\n\nTask 3b: Subset the data so you can perform test for pre-trends between the pre-treatment months 3 and 8 using the TWFE approach from above.\n\n# ...\n\nWith this approach, you would have to iterate through quite a lot of combinations. Another approach would be to add “fake” post columns to the data frame and interact them with the treatment indicator.\nTask 4: Subset the data so you can perform test for pre-trends between ALL two arbitrary pre-treatment periods using the TWFE approach from above. You’ll need to create new columns with e.g. post_t = if_else(month == t, 1, 0), where t is a treatment period and introduce new interactions into the formula.\n\n# ...\n\nAn even more programmatic way to test for pre-trends is to run an event study to perform the tests all in one command. We only have to slightly change the input. Instead of the single treatment indicator ever_treated:post, which is only active for the treated group after treatment, we include several interactions which cover all periods for the treated group.\nWe can do so with the help of i() from the fixest package and a newly created variable that measures the time distance to the treatment period.\nUsing iplot() we can visualize the multiple treatment effects. Any treatment effect prior to the treatment is a big cause of concern with regards to the parallel trends assumption.\n\n# Plot event study\niplot(evt_stdy)",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#conditional-parallel-trends",
    "href": "content/course_weeks/week_09/week_9.html#conditional-parallel-trends",
    "title": "9 - Difference-in-Differences",
    "section": "Conditional parallel trends",
    "text": "Conditional parallel trends\nNow, we have seen that we cannot assume parallel trends - there are significant differences in the evolution of outcomes. But what can we do to increase the plausibility of assuming parallel trends? One approach that is widely been used in practice is the inclusion of covariates. But as you have learned in the lecture, you have to be extremely careful.\nWhat you obtain then is the conditional parallel trends assumption, which is the assumption of parallel trends conditional on covariates.\n\\[\n\\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | T_i=1, \\mathbf{X_i}] = \\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | T_i=0, \\mathbf{X_i}] \\quad\n\\]\nTask 5: Include the covariates into the event study and consider that they have time-varying effects.\n\n# ...\n\n\n# Plot event study\niplot(evt_stdy_cond_2)",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#att-conditional-on-covariates",
    "href": "content/course_weeks/week_09/week_9.html#att-conditional-on-covariates",
    "title": "9 - Difference-in-Differences",
    "section": "ATT conditional on covariates",
    "text": "ATT conditional on covariates\nTWFE\nTask 6: Use the TWFE approach to estimate the ATT by under the assumption of parallel trend conditional on the covariates.\n\n# ...\n\nDoubly robust estimator\nDuring the last weeks we have already learned the advantages of doubly robust estimation and in fact, there is a doubly robust DiD estimator proposed by Sant’Anna and Zhao, 2020 and implemented in the DRDID package.\n\ninstall.packages(\"DRDID\")\n\n\nlibrary(DRDID)\n\ndrdid(\n  yname = \"downloads\",\n  tname = \"post\",\n  idname = \"city\",\n  dname = \"ever_treated\",\n  xformla = ~age+user_share+population_size+competitor_price,\n  data = df_2p\n)\n\n Call:\ndrdid(yname = \"downloads\", tname = \"post\", idname = \"city\", dname = \"ever_treated\", \n    xformla = ~age + user_share + population_size + competitor_price, \n    data = df_2p)\n------------------------------------------------------------------\n Further improved locally efficient DR DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n 18.6812     3.2087     5.822        0       12.3922    24.9703  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: weighted least squares.\n Propensity score est. method: inverse prob. tilting.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.\n\n\nTask 7: Following the same syntax, use the outcome regression adjustment with ordid() and the inverse probability weighting approach ipwdid() to estimate the treatment effect.\n\n# ...\n\nOutcome regression\nInverse probability weighting regression",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9.html#assignment",
    "href": "content/course_weeks/week_09/week_9.html#assignment",
    "title": "9 - Difference-in-Differences",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "9 - Difference-in-Differences"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html",
    "href": "content/course_weeks/week_07/week_7.html",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "",
    "text": "Assumptions:\n\n\nRelevance: \\(Z\\) is significantly correlated with \\(T\\), i.e. \\(Cov(Z, T) \\neq 0\\). Path 1 must exist.\n\nExclusion Restriction: \\(Z\\) affects \\(Y\\) only through \\(T\\). A direct path 2 must not exist.\n\nUnconfoundedness (Exogeneity, Validity): \\(Z\\) is independent of \\(U\\), i.e. \\(Cov(Z, \\epsilon_Y) = 0\\). Conditioning on \\(\\mathbf{X}\\) required in some contexts (will cover this later). Path 3 must not exist.\n\nFor our practical example, imagine the following situation: you have developed an app and you are already having an active user base. Of course, some users are more active than other users. Also, users might use the app for different purposes. In general, user behavior likely depends on a lot of unobserved characteristics.\nObviously, your goal is to keep users as long as possible on the app to maximize your ad revenues. To do that, you want to introduce a new feature and see how it affects time spent on the app. Simply comparing users who use the newly introduced feature to users who don’t would result in a biased estimate due to the unobserved confounders regarding their activity and willingness to use a new feature.\nTherefore, you perform a so called randomized encouragement trial, where for a random selection of users, a popup appears when opening the app and encourages these users to test new feature. The users who are not randomly selected don’t get a popup message but could also use the new feature.\nAfter a while you collect data on users’ activity and also if they were encouraged and if they used the new feature. Download the data and load it.\n\n\nCausal Effect: The impact of a new feature on the time spent in the app.\n\nTreatment: Usage of the new feature.\n\nOutcome: Time spent in the app.\n\nInstrument: Randomized encouragement (e.g., a popup message encouraging users to try the new feature).\n\nThis setup avoids running an A/B test on the feature itself by using an encouragement design, which is often more acceptable from a business perspective, particularly when there is a desire to roll out a feature to all users.\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Load data and print\ndf &lt;- readRDS(\"rand_enc.rds\")\nprint(df)\n\n# A tibble: 10,000 × 3\n   popup used_ftr time_spent\n   &lt;int&gt;    &lt;int&gt;      &lt;dbl&gt;\n 1     0        0       65.5\n 2     0        0       77.5\n 3     1        0       70.8\n 4     0        0       58.5\n 5     0        1       86.1\n 6     1        1       67.7\n 7     0        0       68.1\n 8     0        0       63.3\n 9     1        1       69.2\n10     0        0       60.9\n# ℹ 9,990 more rows\n\n\nTask 1: Evaluate the IV assumptions.\nRelevance\n\n\nQuestion: Does the randomized encouragement (popup message) affect the likelihood of using the new feature?\n\nEvaluation: It is plausible that the encouragement message will increase the probability that users try the new feature, satisfying the relevance condition.\nExclusion Restriction\n\n\nQuestion: Does the randomized encouragement affect the time spent in the app only through its effect on the usage of the new feature?\n\nEvaluation: Assuming the only reason the encouragement affects the time spent in the app is by increasing the use of the new feature, this condition holds. The popup message should not directly affect time spent in the app other than by leading users to use the new feature.\nUnconfoundedness\n\n\nQuestion: Is the random assignment of encouragement independent of potential outcomes and treatments?\n\nEvaluation: Since the encouragement is randomized, it should be independent of other factors that could influence both the usage of the new feature and the time spent in the app, satisfying this condition.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#slides-recap",
    "href": "content/course_weeks/week_07/week_7.html#slides-recap",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "",
    "text": "Assumptions:\n\n\nRelevance: \\(Z\\) is significantly correlated with \\(T\\), i.e. \\(Cov(Z, T) \\neq 0\\). Path 1 must exist.\n\nExclusion Restriction: \\(Z\\) affects \\(Y\\) only through \\(T\\). A direct path 2 must not exist.\n\nUnconfoundedness (Exogeneity, Validity): \\(Z\\) is independent of \\(U\\), i.e. \\(Cov(Z, \\epsilon_Y) = 0\\). Conditioning on \\(\\mathbf{X}\\) required in some contexts (will cover this later). Path 3 must not exist.\n\nFor our practical example, imagine the following situation: you have developed an app and you are already having an active user base. Of course, some users are more active than other users. Also, users might use the app for different purposes. In general, user behavior likely depends on a lot of unobserved characteristics.\nObviously, your goal is to keep users as long as possible on the app to maximize your ad revenues. To do that, you want to introduce a new feature and see how it affects time spent on the app. Simply comparing users who use the newly introduced feature to users who don’t would result in a biased estimate due to the unobserved confounders regarding their activity and willingness to use a new feature.\nTherefore, you perform a so called randomized encouragement trial, where for a random selection of users, a popup appears when opening the app and encourages these users to test new feature. The users who are not randomly selected don’t get a popup message but could also use the new feature.\nAfter a while you collect data on users’ activity and also if they were encouraged and if they used the new feature. Download the data and load it.\n\n\nCausal Effect: The impact of a new feature on the time spent in the app.\n\nTreatment: Usage of the new feature.\n\nOutcome: Time spent in the app.\n\nInstrument: Randomized encouragement (e.g., a popup message encouraging users to try the new feature).\n\nThis setup avoids running an A/B test on the feature itself by using an encouragement design, which is often more acceptable from a business perspective, particularly when there is a desire to roll out a feature to all users.\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Load data and print\ndf &lt;- readRDS(\"rand_enc.rds\")\nprint(df)\n\n# A tibble: 10,000 × 3\n   popup used_ftr time_spent\n   &lt;int&gt;    &lt;int&gt;      &lt;dbl&gt;\n 1     0        0       65.5\n 2     0        0       77.5\n 3     1        0       70.8\n 4     0        0       58.5\n 5     0        1       86.1\n 6     1        1       67.7\n 7     0        0       68.1\n 8     0        0       63.3\n 9     1        1       69.2\n10     0        0       60.9\n# ℹ 9,990 more rows\n\n\nTask 1: Evaluate the IV assumptions.\nRelevance\n\n\nQuestion: Does the randomized encouragement (popup message) affect the likelihood of using the new feature?\n\nEvaluation: It is plausible that the encouragement message will increase the probability that users try the new feature, satisfying the relevance condition.\nExclusion Restriction\n\n\nQuestion: Does the randomized encouragement affect the time spent in the app only through its effect on the usage of the new feature?\n\nEvaluation: Assuming the only reason the encouragement affects the time spent in the app is by increasing the use of the new feature, this condition holds. The popup message should not directly affect time spent in the app other than by leading users to use the new feature.\nUnconfoundedness\n\n\nQuestion: Is the random assignment of encouragement independent of potential outcomes and treatments?\n\nEvaluation: Since the encouragement is randomized, it should be independent of other factors that could influence both the usage of the new feature and the time spent in the app, satisfying this condition.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#parametric-identification-of-ate",
    "href": "content/course_weeks/week_07/week_7.html#parametric-identification-of-ate",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "Parametric identification of ATE",
    "text": "Parametric identification of ATE\nAs in our practical example, we will derive the ATE for the binary case, i.e. a binary instrument and a binary treatment variable. However, the continuous case is very similar. For the parametric identification, we assume linearity. In other words, we assume a homogeneous treatment effect and exclude effect heterogeneity with regard to unobserved variables.\n\\[\nY_i = \\beta_0 + \\tau T_i + \\beta_U U_i + \\epsilon_i\n\\] Please note, that there is no \\(Z\\) in this formula due to the exclusion restriction.\nLet’s go through the identification procedure step by step. You will see and use the other assumptions there, as well.\nWe start with the associational difference for \\(Y\\) under different values for \\(Z\\). It’s not the treatment effect \\(\\tau\\), but because we can observe this expression, we take it as a starting point.\n\\[\n\\begin{align*}\n& \\mathbb{E}[Y_i | Z_i = 1] - \\mathbb{E}[Y_i | Z_i = 0] \\\\\n\\end{align*}\n\\]\nExplanation\nAssumption of linearity.\n\\[\n\\begin{align*}\n&= \\mathbb{E}[\\beta_0 + \\tau T_i + \\beta_U U_i + \\epsilon_i | Z_i = 1] - \\mathbb{E}[\\beta_0 + \\tau T_i + \\beta_U U_i + \\epsilon_i | Z_i = 0] \\\\\n\\end{align*}\n\\]\nExplanation\nLinearity of expectation.\n\\[\n\\begin{align*}\n&= \\beta_0 + \\tau \\mathbb{E}[T_i | Z_i = 1] + \\beta_u \\mathbb{E}[U_i | Z_i = 1] + \\mathbb{E}[\\epsilon_i | Z_i = 1] - \\beta_0 - \\tau \\mathbb{E}[T_i | Z_i = 0] - \\beta_u \\mathbb{E}[U_i | Z_i = 0] - \\mathbb{E}[\\epsilon_i | Z_i = 0] \\\\\n\\end{align*}\n\\]\nExplanation\nRe-arranging, removing \\(\\epsilon_i\\) and \\(\\beta_0\\).\n\\[\n\\begin{align*}\n&= \\tau (\\mathbb{E}[T_i | Z_i = 1] - \\mathbb{E}[T_i | Z_i = 0]) + \\beta_u (\\mathbb{E}[U_i | Z_i = 1] - \\mathbb{E}[U_i | Z_i = 0]) \\\\\n\\end{align*}\n\\]\nExplanation\nUnconfoundedness assumption.\n\\[\n\\begin{align*}\n&= \\tau (\\mathbb{E}[T_i | Z_i = 1] - \\mathbb{E}[T_i | Z_i = 0]) + \\beta_u (\\mathbb{E}[U_i] - \\mathbb{E}[U_i]) \\\\\n\\end{align*}\n\\]\nExplanation\nRe-arranging/removing terms.\n\\[\n\\begin{align*}\n&= \\tau (\\mathbb{E}[T_i | Z_i = 1] - \\mathbb{E}[T_i | Z_i = 0])\n\\end{align*}\n\\]\nTask 2: Solve for \\(\\tau\\) and compute sample version of the Wald estimand.\n\n# Numerator\nnum &lt;- mean(df[df$popup == 1, ]$time_spent) - mean(df[df$popup == 0, ]$time_spent)\n\n# Denominator\nden &lt;- mean(df[df$popup == 1, ]$used_ftr) - mean(df[df$popup == 0, ]$used_ftr)\n\n# Wald estimate\nnum/den\n\n[1] 9.3",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#sls-two-stage-least-squares",
    "href": "content/course_weeks/week_07/week_7.html#sls-two-stage-least-squares",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "2SLS (Two-stage least squares)",
    "text": "2SLS (Two-stage least squares)\nA more practical way of estimating the treatment effect is the two-stage least squares estimator (2SLS) which returns an estimate equivalent to the one just computed. 2SLS splits the estimation procedure in two stages:\n\n1st stage: regress \\(T\\) on \\(Z\\) to obtain \\(\\hat{T}\\), where \\(\\hat{T}\\) are the fitted values.\n2nd stage: regress \\(Y\\) on \\(\\hat{T}\\) to obtain the coefficient \\(\\hat{\\tau}\\), which is the treatment effect.\n\nIn the graphs below, you can see how the 2SLS estimator works. First, \\(U\\) is an unobserved confounder of \\(T\\) on \\(Y\\). When replacing \\(T\\) with \\(\\hat{T}\\), there is no arrow from \\(U\\) on the adjusted treatment, because there is no dependence.\n\nTask 3: Compute the treatment effect using the 2SLS estimator as described. (1) First, do it by running the two regressions. (2) Then, use the R package AER and the function ivreg(). The syntax for the formula is: Y ~ T | Z. (Note: for method (1) your standard errors will be wrong. You can ignore that for now.)\n\n# (a) with wrong standard errors\n\n# First stage\nstage_1 &lt;- lm(used_ftr ~ popup, data = df)\nsummary(stage_1)\n\n\nCall:\nlm(formula = used_ftr ~ popup, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.659 -0.309 -0.309  0.341  0.691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.30941    0.00667    46.4   &lt;2e-16 ***\npopup        0.35002    0.00937    37.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.47 on 9998 degrees of freedom\nMultiple R-squared:  0.123, Adjusted R-squared:  0.122 \nF-statistic: 1.4e+03 on 1 and 9998 DF,  p-value: &lt;2e-16\n\n# Second stage\ndf$used_ftr_hat &lt;- stage_1$fitted.values\nstage_2 &lt;- lm(time_spent ~ used_ftr_hat, data = df)\nsummary(stage_2)\n\n\nCall:\nlm(formula = time_spent ~ used_ftr_hat, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-41.66  -7.36   0.28   7.87  31.94 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    62.273      0.322   193.2   &lt;2e-16 ***\nused_ftr_hat    9.309      0.623    14.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11 on 9998 degrees of freedom\nMultiple R-squared:  0.0218,    Adjusted R-squared:  0.0217 \nF-statistic:  223 on 1 and 9998 DF,  p-value: &lt;2e-16\n\n\n\ninstall.packages(\"AER\") # (Applied Economics with R)\n\n\n# (b) with correct standard errors\n\n# Load package\nlibrary(AER)\n\n# Two-stage least squares\ntsls &lt;- ivreg(time_spent ~ used_ftr | popup, data = df)\nsummary(tsls, vcov = vcovHC) # with robust standard errors\n\n\nCall:\nivreg(formula = time_spent ~ used_ftr | popup, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-35.52  -6.03   0.62   6.63  27.28 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   62.273      0.270   230.5   &lt;2e-16 ***\nused_ftr       9.309      0.524    17.8   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.2 on 9998 degrees of freedom\nMultiple R-Squared: 0.308,  Adjusted R-squared: 0.308 \nWald test:  316 on 1 and 9998 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#nonparametric-identification-of-local-ate",
    "href": "content/course_weeks/week_07/week_7.html#nonparametric-identification-of-local-ate",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "Nonparametric identification of Local ATE",
    "text": "Nonparametric identification of Local ATE\nThe linearity assumption is a very strong parametric assumption, as it e.g. requires homogeneous treatment effects. Therefore, we would like to nonparametrically identify the treatment effect, and in fact, we can. However, we are not able to identify the average treatment effect ATE, but instead a local version of the average treatment effect, known as LATE. While we can drop the linearity assumption, now the assumption of monotonicity is needed. But first, we have to introduce a bit of new notation to express the relationship between instrument and treatment. The notation is similar to the potential outcome notation.\nPrincipal stratification\nWe segment population into four segments. E.g. \\(T_i(Z = 1) = 1\\) represents unit’s \\(i\\) decision to “get the treatment” when being encouraged. Based on all possible combinations, four different exist:\n\n\nCompliers always take the treatment that they’re encouraged to take: \\(T_i(1) = 1\\) and \\(T_i(0) = 0\\).\n\nAlways-Takers always take the treatment, regardless of encouragement: \\(T_i(1) = 1\\) and \\(T_i(0) = 1\\).\n\nNever-Takers never take the treatment, regardless of encouragement: \\(T_i(1) = 0\\) and \\(T_i(0) = 0\\).\n\nDefiers always take the opposite treatment that they’re encouraged to take: \\(T_i(1) = 0\\) and \\(T_i(0) = 1\\).\n\nApplied to our example, usere who receive the encouragement (popup message) and use it are compliers.\nTask 4: We are not able to observe whether a unit is a complier, always-taker, never-taker or defier. For each observed combination of \\(Z\\) and \\(T\\), there are always two options. For the following combinations, answer what segments could be possible.\n\\(Z=0, T=0\\)\nCompliers or never-takers\n\\(Z=0, T=1\\)\nDefiers or always-takers\n\\(Z=1, T=0\\)\nDefiers or never-takers\n\\(Z=1, T=1\\)\nCompliers or always-takers\nTask 5: Draw the DAGs for complier and never-takers.\nLocal ATE\nIn case of unobserved confounding, we will nonparametrically identify the local average treatment effect (LATE), also known as the complier average causal affect (CACE). It is the average tratment effect among compliers.\n\\[\n\\mathbb{E}[Y_i(T_i=1) - Y_i(T_i=0) | T_i(Z_i=1) = 1, T_i(Z_i=0) = 0]\n\\]\nInstead of linearity, we assume monotonicity: a unit encouraged to take the treatment (\\(Z=1\\)), is either more or equally likely to take it then without encouragement (\\(Z=0\\)).\n\\[\nT_i(Z_i=1) \\geq T_i(Z_i=0) \\,\\, \\forall i\n\\]\nThe monotonicity assumption implies that there are no defiers because they actually always act differently than the compliers.\nIn our example, monotonicity means that the popup uniformly increase the likelihood of using the new feature, without making any users less likely to use it. And while the assumption is not testable, we could argue that the popup message is unlikely to decrease the likelihood that users try the new feature. Therefore, this condition is likely satisfied.\nFor the estimation of the LATE, if \\(Z_i\\) and \\(T_i\\) are binary instrument and treatment variable, respectively, and monotonicity holds, it follows\n\\[\n\\mathbb{E}[Y_i(1) - Y_i(0) | T_i(1) = 1, T_i(0) = 0] = \\frac{\\mathbb{E}[Y_i | Z_i = 1] - \\mathbb{E}[Y_i | Z_i = 0]}{\\mathbb{E}[T_i | Z_i = 1] - \\mathbb{E}[T_i | Z_i = 0]}\n\\]\nThe numerator term is also called Intention-to-Treat (ITT) Effect, while the denominator is 1st-stage effect or Complier Share.\nTask 6: Compute ITT and complier share for our example. How would the LATE vary with a lower or higher complier share?",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#identification-examples",
    "href": "content/course_weeks/week_07/week_7.html#identification-examples",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "Identification examples",
    "text": "Identification examples\nTask 7: Finding a good instrument is often times quite difficult due to the requirements discussed above. Read the following identification strategies and evalute their validity.\n\n\nCase 1: Effect of R&D Expenditure on Innovation Output\n\n\nVariable\nDescription\n\n\n\nTreatment\nR&D expenditure\n\n\nOutcome\nNumber of patents filed\n\n\nInstrument\nFirm size\n\n\n\nEvaluation\nFirm size affects R&D expenditure (relevance) but also likely directly affects innovation output due to economies of scale and other factors, violating the exclusion restriction. [invalid]\nCase 2: Effect of Training Programs on Employee Productivity\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nTreatment\nParticipation in a training program\n\n\nOutcome\nEmployee productivity measured by output per hour\n\n\nInstrument\nDistance from the employee’s home to the training center\n\n\n\nEvaluation\nDistance from home to the training center is likely to influence participation in the training program (relevance) but should not directly affect productivity except through participation in the training program (exclusion restriction). [valid]\nCase 3: Effect of Marketing Spend on Sales\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nTreatment\nMarketing expenditure by a firm\n\n\nOutcome\nSales revenue\n\n\nInstrument\nIntroduction of a new marketing budget rule that allocates more funds to marketing based on predefined criteria\n\n\n\nEvaluation\nThe new budget rule affects marketing expenditure (relevance) and is assumed to affect sales only through changes in marketing expenditure (exclusion restriction). Independence holds if the rule is exogenous to other factors affecting sales. [valid]\nCase 4: Effect of Employee Benefits on Job Satisfaction\n\n\nVariable\nDescription\n\n\n\nTreatment\nGenerosity of employee benefits\n\n\nOutcome\nJob satisfaction score\n\n\nInstrument\nFirm’s overall profitability\n\n\n\nEvaluation\nFirm profitability may influence the ability to offer generous benefits (relevance), but it also directly affects job satisfaction through other channels like job security and working conditions, violating the exclusion restriction. [invalid]",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_07/week_7.html#assignment",
    "href": "content/course_weeks/week_07/week_7.html#assignment",
    "title": "7 - Unobserved Confounding & Instrumental Variables",
    "section": "Assignment",
    "text": "Assignment\n\nAccept the Week 7 - Assignment and follow the same steps as last week and as described in the organization chapter.\n\n\nTake the example from the tutorial.\n\nRegress \\(Y\\) on \\(T\\).\nRegress \\(Y\\) on \\(Z\\).\nCompare the estimates to the one obtained by instrumental variable regression and argue why the size differs. Why are the two regressions not valid estimates of the treatment effect?\n\n\nCompute the different bounds presented in the lecture and for each bound, explain in one sentence, in your own words, what is assumed.\n\nUsing the data task_3.rds, perform a sensitivity analysis using the package sensemakr as presented in the lecture. The outcome and treatment is the same as in the tutorial, but you have one additional covariate, age. With the results from the sensitivity analysis, answer the following questions.\n\nHow sensitive is your estimate to a potential unobserved confounder which is 1x, 2x, or 3x as strong as the variable age?\nInterpret the robustness value and explain, in your own words, what it expresses.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "7 - Unobserved Confounding & Instrumental Variables"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html",
    "href": "content/course_weeks/week_02/week_2.html",
    "title": "2 - Graphical Causal Models",
    "section": "",
    "text": "In this chapter, we focus on the identification of causal effects with the help of graphical models, also known as directed acyclic graphs (DAGs). By graphically modeling and leveraging your theoretical knowledge about the data-generating process, you will be able to understand when association is actually causation.\n\n\n\n\n\n\nTip\n\n\n\nData-generating process (DGP) refers to the underlying mechanisms that generate the data we observe. It represents real-world processes that produce the data points we analyze.\n\n\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come from things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nIt should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#slides-recap",
    "href": "content/course_weeks/week_02/week_2.html#slides-recap",
    "title": "2 - Graphical Causal Models",
    "section": "",
    "text": "In this chapter, we focus on the identification of causal effects with the help of graphical models, also known as directed acyclic graphs (DAGs). By graphically modeling and leveraging your theoretical knowledge about the data-generating process, you will be able to understand when association is actually causation.\n\n\n\n\n\n\nTip\n\n\n\nData-generating process (DGP) refers to the underlying mechanisms that generate the data we observe. It represents real-world processes that produce the data points we analyze.\n\n\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come from things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nIt should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#dags",
    "href": "content/course_weeks/week_02/week_2.html#dags",
    "title": "2 - Graphical Causal Models",
    "section": "DAGs",
    "text": "DAGs\nTo discuss and illustrate the notation and terminology of DAGs, we start with a simple and often discussed research problem in causal inference. It deals with the question of whether a university degree increases the salary. While people with university degree tend to earn more than people without university degree, there is the question whether this is caused by the university degree or it is mainly influenced by individuals’ ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree. It is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit. By the way, it helps to imagine association flowing through a graphical model as water flows through a stream.\nDepicted in a graph, we get the following:\n\n\n\n\n\n\n\n\nLet’s discuss some terms:\nQuestion 1: What is/are the descendant(s) of ability?\n\nAnswer\nuni_degree and salary\nQuestion 2: What is/are the parent(s) of salary?\n\nAnswer\nuni_degree and ability\nQuestion 3: What is the treatment and what is the outcome?\n\nAnswer\nuni_degree and salary\nQuestion 4: What does a node depict?\n\nAnswer\nA random variable\nQuestion 5: What does an arrow depict?\n\nAnswer\nFlow of association",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#introduction-dagitty-dagitty-in-r",
    "href": "content/course_weeks/week_02/week_2.html#introduction-dagitty-dagitty-in-r",
    "title": "2 - Graphical Causal Models",
    "section": "Introduction dagitty, dagitty in R",
    "text": "Introduction dagitty, dagitty in R\nOf course, it’s perfectly fine to first sketch DAGs on a sheet of paper, but at some point, when your analysis gets more complex, you are well advised to make use of computational resources. In the following paragraphs, you will be introduced to a few very useful resources.\nA well implemented application is DAGitty, a browser-based environment for creating and analyzing DAGs. Draw your DAG, define treatment and outcome, which variables are observed and unobserved and many other things. Afterwards, you will see what kind of adjustment is necessary to estimate the causal effect of interest. There is also an implementation in R, which is the R package dagitty complemented by ggdag, which provides improved plotting functionality.\n\n# Install dagitty package\ninstall.packages(\"dagitty\")\n\n# Install ggdag\ninstall.packages(\"ggdag\")\n\nNow, we will cover three different ways to create the DAG shown above.\nOption 1: Build on browser-based DAGitty\n…\nOption 2: Build using R package dagitty\n\n\n\n\n\n\nNote\n\n\n\nIn several R functions you will find the syntax y ~ x or y ~ x1 + x2 + ... to specify your functional form. It represents \\(y\\) being dependent on \\(x\\) (of course the names depend on your data/model). Please note, that no parentheses are needed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen drawing the DAG yourself, you can leave out theme_dag_cds() as it is only a custom theme that I have defined for this website. Instead you could use other themes such as theme_dag(), theme_dag_grey(), theme_dag_blank() or proceed without any theme. The other modifications following the theme are also only for the purpose of matching the website theme and can be disregarded. However, you should set text = TRUE in ggdag() in this case.\n\n\n\n# Load packages\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Define dependencies and coordinates\nschooling_dag_1 &lt;- dagify(\n  # Required arguments\n  uni_degree ~ ability,\n  salary ~ ability,\n  salary ~ uni_degree,\n  # Optional arguments\n  exposure = \"uni_degree\",\n  outcome = \"salary\",\n  coords = list(\n    x = c(uni_degree = 1, salary = 3, ability = 2),\n    y = c(uni_degree = 1, salary = 1, ability = 2)\n    ))\n\n# Plot DAG\n# Plot DAG\nggdag(schooling_dag_1, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nOption 3: Copy from dagitty.net to build graph in R\n\n# Define DAG\nschooling_dag_2 &lt;- 'dag {\nability [pos=\"0.5, 0.4\"]\nsalary [outcome,pos=\"0.8, 0\"]\nuni_degree [exposure,pos=\"0.2, 0\"]\nability -&gt; salary\nability -&gt; uni_degree\nuni_degree -&gt; salary\n}'\n\n# Plot DAG\nggdag(schooling_dag_1, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nOne great advantage of having our DAG coded into R or on DAGitty is the possibility to see what we have to adjust for. You can do that by running the command adjustmentSets() which returns all possible sets of variables that you can control for to retrieve a causal effect.\n\n# Get adjustment sets. Because we already specified exposure and outcome when\n# constructing the DAG, we do not need to do again. If they were not specified,\n# you could do so by explicitly providing arguments (\"exposure\", \"outcome\")\nadjustmentSets(schooling_dag_2)\n\n{ ability }",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#types-of-association",
    "href": "content/course_weeks/week_02/week_2.html#types-of-association",
    "title": "2 - Graphical Causal Models",
    "section": "Types of association",
    "text": "Types of association\nMost of the time, DAGs are more complex than the example above. Even in that example, there are many more variables that could be included, like social environment, gender etc. But although in practice DAGs are more complex, they can be disassembled in building blocks that are easier to analyze.\nEffectively, there are only three different types of association we need to focus on:\nChain: \\(X \\rightarrow Z \\rightarrow Y\\)\nConfounder: \\(X \\leftarrow Z \\rightarrow Y\\)\nCollider: \\(X \\rightarrow Z \\leftarrow Y\\)\nKnowing their characteristics and idiosyncrasies allows us to identify valid strategies to estimate causal effects. Then, you know which variables you have to include in your analysis and which ones you have to leave out. Because in case you include or exclude the wrong variables, you will end up with a biased results and you are not able to interpret your estimate causally.\nIt is important to understand (conditional) (in-)dependencies between, and with particular focus on conditional independence. An important concept you learned in the lecture is d-separation:\n\n\n\n\n\n\nNote\n\n\n\nd-separation\nTwo nodes \\(X\\) and \\(Y\\) are d-separated by a set of nodes \\(Z\\) if all of the paths between \\(X\\) and \\(Y\\) are blocked by \\(Z\\).\n\n\nChain\nOne element is a chain of random variables where the causal effect flows in one direction.\n\nCode# Specify chain structure\nchain &lt;- dagify(\n  Y ~ Z,\n  Z ~ X,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 0, X = 0))\n)\n\n# Plot DAG\nggdag(chain) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nConsider the following variables:\n\n\n\\(X\\): Investment in R&D\n\n\\(Z\\): Innovative products\n\n\\(Y\\): Market share\n\nThis mechanism is also sometimes called mediation, because \\(Z\\) mediates the effect of \\(X\\) on \\(Y\\).\nQuestion: Describe the dependencies in the DAG above.\n\nAnswer\n\n\n\\(X\\) and \\(Z\\): dependent, as indicated by the arrow. If you invest more into R&D, you develop more innovative products.\n\n\\(Z\\) and \\(Y\\): dependent, as indicated by the arrow. If you develop more innovative products, your market share increases.\n\n\\(X\\) and \\(Y\\): dependent, as indicated by the arrow (going through \\(Z\\)). More investment in R&D leads to more innovative products and an increased market share.\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, because when we condition on innovative products, that means we hold the quantity and quality of innovative products fixed at a particular level, then there is no effect from investment in R&D to market share as there is no direct effect, but only an effect through the development of innovative products.\nRule: Two variables, \\(X\\) and \\(Y\\), are conditionally independent given \\(Z\\), if there is only one unidirectional path between \\(X\\) and \\(Y\\) and \\(Z\\) is any set of variables that intercepts that path.\nWe can also check using the function dseperated(). It returns TRUE when two nodes are d-separated, which means that they are independent of each other. You need to provide \\(X\\) and \\(Y\\), which are the focal nodes and \\(Z\\), your set that you condition on.\n\n# Check d-separation between X and Y\ndseparated(chain, X = \"X\", Y = \"Y\", Z = c())\n\n[1] FALSE\n\n\n\n# Check d-separation between X and Y conditional on Z\ndseparated(chain, X = \"X\", Y = \"Y\", Z = c(\"Z\"))\n\n[1] TRUE\n\n\nFork\nAnother mechanism is the fork, also called common cause.\n\nCode# Fork\nfork &lt;- dagify(\n  X ~ Z,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0))\n)\n\n# Plot DAG\nggdag(fork) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nThe reason it is called common cause is that, as depicted above, both \\(X\\) and \\(Y\\) are caused by \\(Z\\).\nTo illustrate it, consider the following scenario: \\(Z\\) represents the temperature in a particular town and \\(X\\) and \\(Y\\) represent ice cream sales and number of crimes in that same town, respectively.\n\n\n\\(X\\): ice cream sales\n\n\\(Z\\): temperature\n\n\\(Y\\): number of crimes\n\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\nQuestion: Describe the dependencies in the DAG above.\n\nAnswer\n\n\n\\(Z\\) and \\(X\\): dependent, as indicated by arrow. Higher temperature leads to more ice cream sales.\n\n\\(Z\\) and \\(Y\\): dependent, as indicated by arrow. Higher temperature leads to more crimes.\n\n\\(X\\) and \\(Y\\): dependent, as both are influenced by \\(Z\\). \\(X\\) and \\(Y\\) change both with variation in \\(Z\\). Variation in temperature affects ice cream sales and number of crimes simultaneously.\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, as for a fixed level of temperature, there is no association anymore.\nRule: If variable \\(Z\\) is a common cause of variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\)are independent conditional on X.\nAgain, we also check using R.\n\n# Check d-separation between X and Y conditional on Z\ndseparated(fork, X = \"X\", Y = \"Y\", Z = c(\"Z\"))\n\n[1] TRUE\n\n\n\n# Check d-separation between X and Y\ndseparated(fork, X = \"X\", Y = \"Y\", Z = c())\n\n[1] FALSE\n\n\nCollision\nThe last mechanism is the collision, which is also called common effect.\n\nCode# Collider\ncollider &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 0, X = 1))\n)\n\n# Plot DAG\nggdag(collider) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nIt is the reflection of the fork and both \\(X\\) and \\(Y\\) have a common effect on the collision node \\(Z\\).\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n\n\\(X\\) and \\(Z\\): dependent, as indicated by arrow.\n\n\\(Y\\) and \\(Z\\): dependent, as indicated by arrow.\n\n\\(X\\) and \\(Y\\): independent, there is no path between \\(X\\) and \\(Y\\).\n\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson’s paradox.\nFor example, imagine the variables to be:\n\n\n\\(X\\): programming skills\n\n\\(Y\\): social skills\n\n\\(Z\\): hired by renowned tech company\n\nFirst of all, in the general population, there is no correlation between programming skills and social skills (3rd dependency). Second, having either programming or social skills will land you a job a a tech company (1st and 2nd dependency).\nBut what about the last dependency? Why are programming and social skills suddenly correlated when conditioned on e.g. being hired by a tech company? That is because when you know someone works for a tech company and has no programming skill, the likelihood that he/she has social skills increases because otherwise he/she would have likely not be hired. Vice versa, if you know someone has been hired and has no social skills, he/she is probably a talented programmer.\nRule: If a variable \\(Z\\) is the collision node between two variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are unconditionally independent but are dependent conditional on \\(Z\\) (and any descendants of \\(Z\\)).\nSame procedure as above, checking using R.\n\n# Check d-separation between X and Y\ndseparated(collider, X = \"X\", Y = \"Y\", Z = c())\n\n[1] TRUE\n\n\n\n# Check d-separation between X and Y conditional on Z\ndseparated(collider, X = \"X\", Y = \"Y\", Z = c(\"Z\"))\n\n[1] FALSE\n\n\nQuiz\n\nLook at the following DAG and by hand, describe what nodes are d-separated and which ones are not.\n\n\nCode# create DAG from dagitty\ndag_model &lt;- 'dag {\nbb=\"0,0,1,1\"\nX [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nM [pos=\"0.2,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nX -&gt; M\nM -&gt; Y\nX -&gt; Z3\nZ1 -&gt; X\nZ1 -&gt; Y\nZ2 -&gt; Y\nZ2 -&gt; Z3\nZ3 -&gt; Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nFor each of the following scenarios, explain whether the nodes \\(X\\) and \\(Y\\) are d-separated.\n\nno conditioning\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c())\n\n[1] FALSE\n\n\n\nconditioning on \\((M, Z1)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z1\"))\n\n[1] TRUE\n\n\n\nconditioning on \\((M, Z3)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z3\"))\n\n[1] FALSE\n\n\n\nconditioning on \\((M, Z1, Z4)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z1\", \"Z4\"))\n\n[1] FALSE\n\n\n\nconditioning on \\((M, Z1, Z2, Z4)\\)\n\n\n\ndseparated(dag_model, X = \"X\", Y = \"Y\", Z = c(\"M\", \"Z1\", \"Z2\", \"Z4\"))\n\n[1] TRUE\n\n\n\nIn R, draw a DAG of a randomized controlled trial. Assume the following variables: the treatment \\(D\\) and a variable \\(Z\\) that affects the outcome \\(Y\\).\n\n\n# RCT\nrct &lt;- dagify(\n  Y ~ X,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 1, Z = 2, X = 1))\n)\n\n# Plot DAG\nggdag(rct) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#practical-examples",
    "href": "content/course_weeks/week_02/week_2.html#practical-examples",
    "title": "2 - Graphical Causal Models",
    "section": "Practical examples",
    "text": "Practical examples\nEffect of hiring consultants\nConsider this scenario: You aim to determine the effectiveness of hiring top consultants for your company and quantify this impact. You have access to a large company database containing profit data from both the previous and current years, along with information on whether companies have hired top consultants.\nFirst, we are going to load the data (You’ll need to download the data first).\n\n\n\n\n\n\nNote\n\n\n\nThe .rds format is a specific R format preferred over other format like .csv when you are working only in R. It preserves R-specific attributes and structure and loads and saves objects efficiently with readRDS() and saveRDS(), respectively.\n\n\n\n# Load the data\nprofits &lt;- readRDS(\"profits.rds\")\nprint(profits)\n\n# A tibble: 1,000 × 4\n   company previous_profit consultant profit\n     &lt;int&gt;           &lt;dbl&gt;      &lt;int&gt;  &lt;dbl&gt;\n 1       1            7.46          1   7.44\n 2       2            5.92          0   5.03\n 3       3            7.47          1   7.32\n 4       4            8.73          1   8.39\n 5       5            6.65          0   5.75\n 6       6            2.47          0   2.13\n 7       7            4.59          0   4.06\n 8       8            5.80          1   5.99\n 9       9            5.83          1   5.9 \n10      10            4.39          0   3.72\n# ℹ 990 more rows\n\n\nIdentification\nBy now, you have learned that the first step in causal inference - before estimating the treatment effect - is the identification and in this chapter you have learned a useful graphical tool: DAGs. Let’s draw the DAG that shows your understanding of the scenario and find out how we are able to recover the causal effect of hiring consultants on profits.\nLet’s discuss each possible dependency:\n\n\n\\(consultant\\) - \\(profit\\)\n\n\n\\(consultant\\) - \\(previous\\_profit\\)\n\n\n\\(profit\\) - \\(previous\\_profit\\)\n\n\n\n# Construct DAG\nprofits_dag &lt;- dagify(\n  consultant ~ previous_profit,\n  profit ~ previous_profit,\n  profit ~ consultant,\n  coords = list(x = c(consultant = 1, profit = 3, previous_profit = 2),\n                y = c(consultant = 1, profit = 1, previous_profit = 2))\n)\n\n# Plot DAG\nggdag(profits_dag, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nBy now, you are probably already able to see it on a first glance because you know what a confounder is and how to adjust for it. But for the purpose of demonstration, let’s dagitty tell us what we need to adjust for.\n\n# Adjustment set\nadjustmentSets(profits_dag, exposure = \"consultant\", outcome = \"profit\")\n\n{ previous_profit }\n\n\nAs expected, it is the backdoor adjustment which we need to perform in our estimation. We need to control for the previous profits because more previously profitable companies are more likely to hire top consultants and are more likely to make profits again.\nEstimation\nIn the lecture, you have learned the following formula to satisfy the backdoor criterion: condition on the values of your adjustment set and average over the joint distribution.\n\\[\nP(Y|do(D)) = \\sum\\nolimits_W P(Y|D=d, W=w) P(W=w)\n\\] We slightly rewrite the so-called adjustment formula to highlight what we need to do in the estimation step.\n\\[\n\\begin{align}\nATE &= E_W[E(Y|D=1) - E(Y|D=0)] \\\\\nATE &= \\sum_{w \\in W} [E(Y|D=1, W=w) - E(Y|D=0, W=w)] P(W=w) \\\\\n&= \\sum_{w \\in W} [E(Y|D=1, W=w)P(W=w) - E(Y|D=0, W=w)P(W=w)]\n\\end{align}\n\\] When dealing with only a few values of \\(W\\), you condition by examining each group \\(w \\in W\\), comparing treated and untreated observations within each group, and then averaging the outcomes, with the group sizes serving as weights. Remember, you assume that within the groups the treatment must look as if it was as good as randomly assigned.\nHowever, when we look at our adjustment set, which is the previous profit of a company, we see that there are not only a couple of values, but theoretically, previous profits could take any real number.\n\n\n\n\n\n\nNote\n\n\n\nsummary() provides a concise summary of the statistical properties of data objects such as data frames, vectors or even models.\n\n\n\n# Statistical summary of column of previous profits\nsummary(profits$previous_profit)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -1.0     4.0     5.6     5.6     7.1    13.5 \n\n\nIn the following weeks, we will learn sophisticated methods to deal with continuous confounders, but today, we refer to a simpler method. We assign each company to a group determined by the value of previous profits. The function ntile() does the job: it splits the data into \\(n\\) equally sized groups ordered by size.\n\n\n\n\n\n\nNote\n\n\n\nIn R, pipes |&gt; are used to chain together multiple operations in a readable and concise manner. The |&gt; operator (shortcut: cmd/ctrl + shift + m) passes the output of one function as the first argument to the next function. This makes code easier to understand by avoiding nested function calls and enhancing code readability.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn R, group_by() is used to group data by one or more variables, summarise() computes summary statistics within each group, and ungroup() removes grouping structure. These functions are commonly used together in data manipulation workflows and belong to the tidyverse (dplyr). n() is a function specifically for such grouping operations. It does not take any arguments and counts the number of rows in each group.\n\n\n\n# Split group into n equally sized groups \nprofits$group &lt;- ntile(profits$previous_profit, n = 4)\n\n# Compute mean value and number of observations by group\nprofits_by_group &lt;- profits |&gt; \n  # What columns to group by\n  group_by(group, consultant) |&gt; \n  # What columns to use for aggregation and what kind of aggregations\n  summarise(\n    # Average profit by group\n    mean_profit = mean(profit),\n    # Number of observations by group\n    nobs = n()\n  ) |&gt; \n  # Remove grouping structure\n  ungroup()\n\n# Show table\nprint(profits_by_group)\n\n# A tibble: 8 × 4\n  group consultant mean_profit  nobs\n  &lt;int&gt;      &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1     1          0        2.45   181\n2     1          1        3.44    69\n3     2          0        4.05   189\n4     2          1        5.02    61\n5     3          0        5.31    67\n6     3          1        6.33   183\n7     4          0        7.03    58\n8     4          1        8.28   192\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn R, pivot_wider() is a function included in the tidyverse (tidyr) used to convert data from long to wide format. It reshapes data by spreading values from a column into multiple columns, with each unique value in that column becoming a new column. This is particularly useful for creating summary tables or reshaping data for analysis and visualization.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmutate() in R’s dplyr (incl. in tidyverse) adds new columns to a data frame, allowing you to transform or calculate values based on existing columns.\n\n\n\n# Convert to wide format to compute effect\nte_by_group &lt;- profits_by_group |&gt; \n  # Convert column values of consultant to headers and take values from\n  # mean_profit\n  pivot_wider(\n    # For each group, we want one row\n    id_cols = \"group\", \n    # Values of consultants to headers\n    names_from = \"consultant\",\n    # Take values from 'mean_profit'\n    values_from = \"mean_profit\", \n    # Change names of headers for better readability\n    names_prefix = \"consultant_\"\n    ) |&gt; \n  # Compute group treatment effect by subtracting mean of untreated units from\n  # mean of treated units\n  mutate(te = consultant_1 - consultant_0)\n\n# Show table\nprint(te_by_group)\n\n# A tibble: 4 × 4\n  group consultant_0 consultant_1    te\n  &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1     1         2.45         3.44 0.993\n2     2         4.05         5.02 0.964\n3     3         5.31         6.33 1.01 \n4     4         7.03         8.28 1.24 \n\n\nBecause all groups have the same size the weighted average is equal to the average and we can simply take the average to obtain the ATE:\n\n# Taking the average of the treatment effect column. Same as weighted mean,\n# because group sizes are identical.\nmean(te_by_group$te)\n\n[1] 1.1\n\n\n\n# With weighted mean\nweighted.mean(te_by_group$te, c(250, 250, 250, 250))\n\n[1] 1.1\n\n\nOne assumption that needs to be fulfilled for the treatment effect to be valid is the positivity assumption we discussed in the last week. Let’s recall it (slightly rewritten to match the notation of our example):\n\n\n\n\n\n\nNote\n\n\n\nAssumption 6: “Positivity / Overlap / Common Support”.\nFor all values of covariates \\(w\\) present in the population of interest (i.e. \\(w\\) such that \\(P(W=w) &gt; 0\\)), we have \\(0 &lt; P(D=1|W=w) &lt; 1\\).\n\n\nQuestion: Is the positivity assumption satisfied?\n\nAnswer\nYes, because in each group, we have observations from both groups. Take a look at the column nobs in the table profits_by_group.\nFor the sake of demonstration, let’s see what would have happened, had we not adjusted for the previous profits:\n\n# Naive estimate\ny1 &lt;- mean(profits[profits$consultant == 1, ]$profit)\ny0 &lt;- mean(profits[profits$consultant == 0, ]$profit)\ny1 - y0\n\n[1] 2.5\n\n\n\n\n\n\n\n\n\n\n[Optional read] Effect of new feature\nImagine you’re at a software company, introducing a new feature and keen to gauge its impact. To ensure unbiased results, you conduct a randomized rollout: 10% of customers receive the new feature randomly, while the rest do not. Your aim is to assess if this feature enhances customer satisfaction, indirectly measured through Net Promoter Score (NPS). You distribute surveys to both the treated (with the new feature) and control groups, asking if they would recommend your product. Upon analysis, you observe higher NPS scores among customers with the new feature. But can you attribute this difference solely to the feature? To delve into this, start with a graph illustrating this scenario.\nIdentification\nGraphically modeling the described scenario, you’ll end up with:\n\nCoderollout_dag &lt;- 'dag {\nbb=\"0,0,1,1\"\n\"Customer satisfaction\" [pos=\"0, 1\"]\n\"New feature\" [exposure,pos=\"0, 0\"]\nNPS [outcome,pos=\"1,1\"]\nResponse [adjusted,pos=\".8, 0\"]\n\"Customer satisfaction\" -&gt; NPS\n\"Customer satisfaction\" -&gt; Response\n\"New feature\" -&gt; \"Customer satisfaction\"\n\"New feature\" -&gt; Response\nResponse -&gt; NPS\n}'\n\nggdag(rollout_dag, text = FALSE) +\n  theme_dag_cds() + # custom theme, can be left out\n  geom_dag_point(color = ggthemr::swatch()[2]) +\n  geom_dag_text(color = NA) +\n  geom_dag_label_repel(aes(label = name), size = 4) +\n  geom_dag_edges(edge_color = ggthemr::swatch()[7])\n\n\n\n\n\n\n\nArgue, what needs to be adjusted for to compute the effect of the new feature on the net promoter score (NPS).",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#assignment",
    "href": "content/course_weeks/week_02/week_2.html#assignment",
    "title": "2 - Graphical Causal Models",
    "section": "Assignment",
    "text": "Assignment\n\nAccept the Week 2 - Assignment and follow the same steps as last week and as described in the organization chapter.\nSolve the following assignment:\nLoad the data health_program.rds and first take a look at the data.\nImagine the following scenario: you are manager of a company and want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently. Some time after implementing the health_program, you’ll observe the sick_days for each employee and want to estimate the average treatment effect. You know that because of the voluntary basis of participating in the program, other variables might confound your estimate if you compute a naive estimate of the average treatment effect. So you might apply what you have learned in the last weeks about confounding, adjusting etc.\nFirst, load the data health_program.rds and first take a look at the data.For the purpose of this task, we assume that all variables we need for a valid estimation are included in the data.\n\nTake a look at all variables that are included in the data. List all pairs of variables and argue whether there might be a dependency and what direction you assume for that relationship if present.\nMap your explanations from the previous task into a DAG using dagitty and ggdag. Define exposure and treatment for that DAG.\nBased on your DAG, what variables do you need to control for? Explain in a short paragraph.\nEstimate the average treatment effect based on your previous explanations. (Hint: id_cols in pivot_wider can take more than one value by providing a vector id_cols = c(\"var1\", \"var2\")).\nExplain, whether the positivity assumption is satisfied.\nEstimate the average treatment effect without conditioning on any variables and compare to the treatment effect computed in 4. Explain the difference.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_02/week_2.html#further-optional-reading",
    "href": "content/course_weeks/week_02/week_2.html#further-optional-reading",
    "title": "2 - Graphical Causal Models",
    "section": "Further optional reading",
    "text": "Further optional reading\nIn this chapter, we already used some functions from the tidyverse. Functions from the tidyverse, are often preferred over base R functions for several reasons:\n\nReadability and Expressiveness: Tidyverse functions use a consistent and expressive syntax, making code easier to read and write. This can enhance collaboration and reduce errors.\nPiping: Tidyverse functions work well with the |&gt; pipe operator, allowing for a more fluid and readable data manipulation workflow.\nTidy Data Principles: Tidyverse functions are designed around the principles of tidy data, which promotes a standardized way of organizing data that facilitates analysis and visualization.\n\nFor an overview of base R vs tidyverse functions, you can read here.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "2 - Graphical Causal Models"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html",
    "href": "content/course_weeks/week_04/week_4.html",
    "title": "4 - Matching",
    "section": "",
    "text": "Last week, we discussed the significant advantage of employing a randomized treatment assignment: it allowed straightforward comparisons of group averages or the application of univariate linear regressions (i.e., with only one variable on the right-hand side), thereby easing statistical inference.\nHowever, what if the treatment isn’t randomized? We’ve discovered that when developing our identification strategy, we must consider confounding variables and implement measures to control for them. How do we do this? In this chapter, you’ll delve into (1) a regression-based approach, (2) a matching-based approach, and (3) a weighting-based approach to address this challenge.\n\n\n\n\n\n\nTip\n\n\n\nThere are many terms used to account for a confounder which are used interchangeably. Among other, depending on the perspective, you could “block a backdoor path”, “condition on a confounder”, “adjust for the confounder”, or “control for the confounder”.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#slides-recap",
    "href": "content/course_weeks/week_04/week_4.html#slides-recap",
    "title": "4 - Matching",
    "section": "",
    "text": "Last week, we discussed the significant advantage of employing a randomized treatment assignment: it allowed straightforward comparisons of group averages or the application of univariate linear regressions (i.e., with only one variable on the right-hand side), thereby easing statistical inference.\nHowever, what if the treatment isn’t randomized? We’ve discovered that when developing our identification strategy, we must consider confounding variables and implement measures to control for them. How do we do this? In this chapter, you’ll delve into (1) a regression-based approach, (2) a matching-based approach, and (3) a weighting-based approach to address this challenge.\n\n\n\n\n\n\nTip\n\n\n\nThere are many terms used to account for a confounder which are used interchangeably. Among other, depending on the perspective, you could “block a backdoor path”, “condition on a confounder”, “adjust for the confounder”, or “control for the confounder”.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#regression-adjustment",
    "href": "content/course_weeks/week_04/week_4.html#regression-adjustment",
    "title": "4 - Matching",
    "section": "Regression adjustment",
    "text": "Regression adjustment\nIn this scenario, similar to the example from the first week, you’re the owner of an online marketplace enterprise aiming to assist businesses on your platform with strategic pricing guidance. However, please recall that these businesses operate anonymously and independently determine their pricing, rendering the treatment non-randomized. Once more, our focus remains on a particular category of similar products: light jackets. While in the initial week, we denoted the treatment as being on sale, this time, we adopt a more precise approach, utilizing price — a continuous variable - as the treatment.\nDownloading, reading and printing the data, we see that each row represents an observation detailing the daily sales of a particular business with additional information such as the weekday and the product rating. In total, you collected 10’000 observations.\nReading and printing the data, it’s evident that each of the 10’000 rows represents an observation detailing the daily sales of a specific business, accompanied by supplementary information like the weekday and product rating.\n\nlibrary(tidyverse)\n\nprices &lt;- readRDS(\"prices.rds\")\nprint(prices)\n\n# A tibble: 10,000 × 4\n   weekday rating price sales\n     &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1       4    4    26.5    37\n 2       3    4.2  31      39\n 3       1    4    30      63\n 4       7    3.6  28.6    56\n 5       3    4    28.4    26\n 6       4    4.8  30.2    38\n 7       5    4.6  29      45\n 8       5    4.8  29.7    41\n 9       7    4.3  29.7    43\n10       4    4    29.1    48\n# ℹ 9,990 more rows\n\n\nConditional outcome regression\nWith the regression-based approach, we employ a multivariate linear regression that incorporates confounding variables. In essence, we estimates the treatment effect conditionally on covariates.\n\\[\n\\begin{align}\nY_i &= \\beta_0 + \\beta_D D_i + \\mathbf{\\beta_{X}' X_i} + \\epsilon_i \\\\\n\\mathbb{E}[Y_i | T_i, \\mathbf{X_i}] &= \\beta_0 + \\beta_D D_i + \\mathbf{\\beta_{X}' X_i}\n\\end{align}\n\\]\nQ1. Assuming that all potential confounders are observed and in the table, estimate the treatment effect.\n\nCode# Basic OLS regression\nmod_ols &lt;- lm(sales ~ price + rating + as.factor(weekday), data = prices)\nsummary(mod_ols)\n\n\nCall:\nlm(formula = sales ~ price + rating + as.factor(weekday), data = prices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-53.40  -7.41   0.51   8.22  39.36 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          141.291      3.403   41.52   &lt;2e-16 ***\nprice                 -2.690      0.121  -22.14   &lt;2e-16 ***\nrating                 3.416      0.304   11.24   &lt;2e-16 ***\nas.factor(weekday)2  -34.128      0.469  -72.73   &lt;2e-16 ***\nas.factor(weekday)3  -34.345      0.469  -73.18   &lt;2e-16 ***\nas.factor(weekday)4  -33.835      0.466  -72.54   &lt;2e-16 ***\nas.factor(weekday)5  -33.771      0.469  -71.96   &lt;2e-16 ***\nas.factor(weekday)6  -33.454      0.471  -71.01   &lt;2e-16 ***\nas.factor(weekday)7   -0.116      0.446   -0.26     0.79    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12 on 9991 degrees of freedom\nMultiple R-squared:  0.588, Adjusted R-squared:  0.588 \nF-statistic: 1.78e+03 on 8 and 9991 DF,  p-value: &lt;2e-16\n\n\nFrisch–Waugh–Lovell Theorem\nFollowing the Frisch–Waugh–Lovell theorem, you can decompose your regression into a three-stage process and obtain identical estimates. The idea behind this is to initially eliminate all variation in \\(D\\) and \\(Y\\) that can be explained by the covariates \\(X\\). Subsequently, you account for the remaining variation in \\(Y\\) using the remaining variation in \\(D\\).\n\n\n\nDenoising: run a regression of the form \\(Y_i = \\beta_{Y0} + \\mathbf{\\beta_{Y \\sim X}' X_i} + \\epsilon_{Y_i \\sim X_i}\\) and extract the estimated residuals \\(\\hat\\epsilon_{Y_i \\sim X_i}\\).\n\nDebiasing: run a regression of the form \\(D_i = \\beta_{D0} + \\mathbf{\\beta_{D \\sim X}' X_i} + \\epsilon_{D_i \\sim X_i}\\) and extract the estimated residuals \\(\\hat\\epsilon_{D_i \\sim X_i}\\).\n\nResidual regression: run a residual-on-residual regression of the form \\(\\hat\\epsilon_{Y_i \\sim X_i} = \\beta_D \\hat\\epsilon_{D_i \\sim X_i} + \\epsilon_i\\) (no constant).\n\n\nQ2. Use the three-step procedure as described above to obtain the estimate. Check that they are identical to the estimates obtained with lm().\n\n\n\n\n\n\nTip\n\n\n\nHints:\n\nWhen you run a model with lm(), residuals are automatically computed. You can access them by model_name$residuals. Residuals are the difference between the actual value and the value predicted by the model. By construction of the ordinary least square regression, residuals are zero on average.\nTo run a model without a constant/intercept, use y ~ 0 + ....\n\n\n\n\nCode# Frisch–Waugh–Lovell Theorem: 3-step procedure\n\n# (1) Debiasing:\nmod_D &lt;- lm(price ~ as.factor(weekday) + rating, prices)\nD_hat &lt;- mod_D$residuals\n\n# (2) Denoising:\nmod_Y &lt;- lm(sales ~ as.factor(weekday) + rating, prices)\nY_hat &lt;- mod_Y$residuals\n\n# (3) Residual regression\nmod_fwl &lt;- lm(Y_hat ~ 0 + D_hat)\nsummary(mod_fwl)\n\n\nCall:\nlm(formula = Y_hat ~ 0 + D_hat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-53.40  -7.41   0.51   8.22  39.36 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nD_hat   -2.690      0.121   -22.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12 on 9999 degrees of freedom\nMultiple R-squared:  0.0468,    Adjusted R-squared:  0.0467 \nF-statistic:  491 on 1 and 9999 DF,  p-value: &lt;2e-16\n\n\nLet’s visualize what we have done in the steps before and what the FWL theorem makes so intuitive. With the regression-based approach, we account for the confounders by remove variance in both \\(D\\) and \\(Y\\) due to \\(X\\) and then, we perform a “clean” residual regression.\nWhen we would simply run a normal regression without any accounting for the confounder, we would obtain what is depicted here. There does not appear to be a relationship between price and sales.\n\nCode# Plot\nggplot(prices, aes(y = sales, x = price)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method='lm') +\n  labs(x = \"Price (X)\", y = \"Sales (Y)\")\n\n\n\n\n\n\n\nHowever, when we do account for the confounders and regress the residual \\(\\hat{Y}\\) on the residual \\(\\hat{D}\\), we obtain what is expected: a negative correlation. Higher prices lead to fewer sales.\n\nCode# Add residuals to data frame\nprices &lt;- prices |&gt; mutate(sales_hat = Y_hat, price_hat = D_hat)\n\n# Plot\nggplot(prices, aes(y = sales_hat, x = price_hat)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method='lm') +\n  labs(x = \"Price residuals (X_hat)\", y = \"Sales residuals (Y_hat)\")\n\n\n\n\n\n\n\nQ3. Compute the estimate that describes what is shown in the first plot.\n\nCode# Naive estimate\nmod_naive &lt;- lm(sales ~ price, data = prices)\nsummary(mod_naive)\n\n\nCall:\nlm(formula = sales ~ price, data = prices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.81 -13.04  -1.43  12.61  56.21 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -37.275      4.627   -8.06  8.7e-16 ***\nprice          3.038      0.157   19.37  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18 on 9998 degrees of freedom\nMultiple R-squared:  0.0362,    Adjusted R-squared:  0.0361 \nF-statistic:  375 on 1 and 9998 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#matching",
    "href": "content/course_weeks/week_04/week_4.html#matching",
    "title": "4 - Matching",
    "section": "Matching",
    "text": "Matching\nImagine another situation. You are operating an online shop, and a year ago, introduced a plus membership aimed at binding customers and driving revenue growth. The plus memberships comes at a small cost for the customers, which is why not all of the customers subscribed. Now you want to examine whether binding customers by this membership program in fact increases your sales with subscribed customers. However, it’s essential to consider potential confounding variables such as age, gender, or previous average purchase amounts.\nEach row in your dataset corresponds to a different customer, with accompanying demographic details, their average purchase sum before the introduction of the plus membership, their current average purchase, and an indication of their participation in the plus membership program.\n\n# Read data and show\nmembership &lt;- readRDS(\"membership.rds\")\nprint(membership)\n\n# A tibble: 1,000 × 5\n     age   sex pre_avg_purch  card avg_purch\n   &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  31.7     0          22.4     1    47.2  \n 2  41.5     1          32.5     0    72.7  \n 3  23.2     0          31.5     1    57.4  \n 4  67.1     1          47.5     1    87.8  \n 5  27.4     1          30.3     1    72.5  \n 6  53.7     1          30.8     1    54.0  \n 7  40.2     0          37.7     0    10.2  \n 8  26.7     0          25.5     1    42.0  \n 9  28.7     0          26.0     0     0.561\n10  35.2     1          46.5     1    70.0  \n# ℹ 990 more rows\n\n\nNow, we’ll delve into the matching-based approach, an alternative to regression for addressing backdoor bias. The concept is to find and match treated and non-treated units with similar or identical covariate values. This method aims to replicate the conditions of a randomized experiment, assuming we have observed all confounding variables. Matching encompasses various techniques aimed at equalizing or balancing the distribution of covariates between the treatment and control groups. Essentially, the objective is to compare “apples to apples” and ensure that treatment and control groups are as similar as possible, except for the treatment variable.\nIn R, there are several packages available to facilitate matching-based methods, one of which is the MatchIt package. You’ll need to install this package first and then load it.\n\ninstall.packages(\"MatchIt\")\n\nNearest neighbor matching\nBy enforcing treatment and control group to have little variation in the matching variables, we close the backdoors. When the backdoor variable does not vary or varies only very little, it cannot induce changes in treatment and outcome. So, when we suppress this variation in the backdoor variable, we can interpret the effect from treatment to outcome as causal.\nOne-vs-one matching\nThe procedure consists of two steps, matching and estimation. Let’s start with a one-vs-one matching. Because, as our estimand, we specify \\(ATT\\), the average treatment effect on the treatment group, we find for each treated unit an untreated unit with the highest resemblance regarding the matching variables. To retrieve the \\(ATC\\), the average effect on the control group (sometimes also called \\(ATU\\)), we would find a treated unit for each untreated unit. Using the summary() function, we can check how well the matching works in terms of the covariate balance across the groups.\n\nlibrary(MatchIt)\n\n# (1) Matching\n# 1 vs. 1 matching\nmatch_1v1 &lt;- matchit(\n  card ~ pre_avg_purch + age + sex,\n  data = membership,\n  estimand = \"ATT\",\n  method = \"nearest\",\n  distance = \"mahalanobis\",\n  ratio = 1,\n  replace = TRUE\n)\nsummary(match_1v1)\n\n\nCall:\nmatchit(formula = card ~ pre_avg_purch + age + sex, data = membership, \n    method = \"nearest\", distance = \"mahalanobis\", estimand = \"ATT\", \n    replace = TRUE, ratio = 1)\n\nSummary of Balance for All Data:\n              Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\npre_avg_purch         39.06          34.0           0.431        1.1     0.121    0.194\nage                   43.39          39.0           0.307        1.1     0.079    0.137\nsex                    0.52           0.5           0.035          .     0.017    0.017\n\nSummary of Balance for Matched Data:\n              Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\npre_avg_purch         39.06         38.80           0.023        1.1     0.007    0.026           0.091\nage                   43.39         43.09           0.021        1.1     0.008    0.029           0.098\nsex                    0.52          0.52           0.000          .     0.000    0.000           0.000\n\nSample Sizes:\n              Control Treated\nAll               547     453\nMatched (ESS)     210     453\nMatched           288     453\nUnmatched         259       0\nDiscarded           0       0\n\n\nThe next step, the estimation, we will perform using the known lm() command. But we’ll need the matched data which includes the matched rows and the corresponding weights.\n\n# Use matched data\ndf_1v1 &lt;- match.data(match_1v1)\nprint(df_1v1)\n\n# A tibble: 741 × 6\n     age   sex pre_avg_purch  card avg_purch weights\n   &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1  31.7     0          22.4     1      47.2   1    \n 2  41.5     1          32.5     0      72.7   1.27 \n 3  23.2     0          31.5     1      57.4   1    \n 4  67.1     1          47.5     1      87.8   1    \n 5  27.4     1          30.3     1      72.5   1    \n 6  53.7     1          30.8     1      54.0   1    \n 7  40.2     0          37.7     0      10.2   0.636\n 8  26.7     0          25.5     1      42.0   1    \n 9  35.2     1          46.5     1      70.0   1    \n10  21.8     0          35.7     1      45.5   1    \n# ℹ 731 more rows\n\n\nNow, we can simply run a regression. Please note, we only include the treatment variable as a predictor, because we closed the backdoor path in the matching step.\nQ4. Run the estimation step of one-vs-one matching.\n\nCode# (2) Estimation\nmod_1v1 &lt;- lm(avg_purch ~ card, data = df_1v1, weights = weights)\nsummary(mod_1v1)\n\n\nCall:\nlm(formula = avg_purch ~ card, data = df_1v1, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-62.35 -13.26  -0.87  11.37  75.78 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     39.9        1.1    36.4   &lt;2e-16 ***\ncard            15.2        1.4    10.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19 on 739 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.137 \nF-statistic:  118 on 1 and 739 DF,  p-value: &lt;2e-16\n\n\nQ5. Compute the \\(ATC\\) (or: \\(ATU\\)) and the \\(ATE\\).\n\nCode# ATU:\n# (1) Matching\n# 1 vs. 1 matching\nmatch_1v1_atu &lt;- matchit(\n  card ~ pre_avg_purch + age + sex,\n  data = membership,\n  estimand = \"ATC\",\n  method = \"nearest\",\n  distance = \"mahalanobis\",\n  ratio = 1,\n  replace = TRUE\n)\nsummary(match_1v1_atu)\n\n\nCall:\nmatchit(formula = card ~ pre_avg_purch + age + sex, data = membership, \n    method = \"nearest\", distance = \"mahalanobis\", estimand = \"ATC\", \n    replace = TRUE, ratio = 1)\n\nSummary of Balance for All Data:\n              Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\npre_avg_purch         39.06          34.0           0.450        1.1     0.121    0.194\nage                   43.39          39.0           0.328        1.1     0.079    0.137\nsex                    0.52           0.5           0.035          .     0.017    0.017\n\nSummary of Balance for Matched Data:\n              Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\npre_avg_purch          34.2          34.0           0.019       0.95     0.008    0.031           0.097\nage                    39.1          39.0           0.001       0.94     0.008    0.033           0.096\nsex                     0.5           0.5           0.000          .     0.000    0.000           0.000\n\nSample Sizes:\n              Control Treated\nAll               547     453\nMatched (ESS)     547     197\nMatched           547     288\nUnmatched           0     165\nDiscarded           0       0\n\nCode# Use matched data\ndf_1v1_atu &lt;- match.data(match_1v1_atu)\nprint(df_1v1_atu)\n\n# A tibble: 835 × 6\n     age   sex pre_avg_purch  card avg_purch weights\n   &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1  31.7     0          22.4     1    47.2     1.05 \n 2  41.5     1          32.5     0    72.7     1    \n 3  23.2     0          31.5     1    57.4     1.58 \n 4  67.1     1          47.5     1    87.8     0.527\n 5  27.4     1          30.3     1    72.5     1.05 \n 6  53.7     1          30.8     1    54.0     1.05 \n 7  40.2     0          37.7     0    10.2     1    \n 8  26.7     0          25.5     1    42.0     1.05 \n 9  28.7     0          26.0     0     0.561   1    \n10  21.8     0          35.7     1    45.5     1.58 \n# ℹ 825 more rows\n\nCode# (2) Estimation\nmod_1v1_atu &lt;- lm(avg_purch ~ card, data = df_1v1_atu, weights = weights)\nsummary(mod_1v1_atu)\n\n\nCall:\nlm(formula = avg_purch ~ card, data = df_1v1_atu, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-74.57 -11.76   0.87  12.16  66.08 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   34.844      0.791    44.0   &lt;2e-16 ***\ncard          16.947      1.347    12.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18 on 833 degrees of freedom\nMultiple R-squared:  0.16,  Adjusted R-squared:  0.159 \nF-statistic:  158 on 1 and 833 DF,  p-value: &lt;2e-16\n\nCode# ATE = p_T * ATT + p_C * ATC\nATE &lt;- weighted.mean(c(mod_1v1$coefficients[2], mod_1v1_atu$coefficients[2]), c(453, 547))\nprint(ATE)\n\n[1] 16\n\n\nOne vs. many matching\nSometimes, you want to match more than just one unit. For example, imagine that you have relatively few treated units but a large amount of untreated units. To leverage all the information contained in the untreated units, matching more units might increase the validity and precision of your estimate.\n\n\n\n\n\n\nNote\n\n\n\nThe R package MatchIt offers a wide range of matching methods and we only scratch the surface of what is possible. Feel free to have a look at the documentation for further information.\n\n\nQ6. Run one-vs-many matching: find 3 matches for each the treated units.\n\nCodeM &lt;- 3\n\n# (1) Matching\n# One-vs-many matching\nmatch_1vM &lt;- matchit(\n  card ~ pre_avg_purch + age + sex,\n  data = membership,\n  estimand = \"ATT\",\n  method = \"nearest\",\n  distance = \"mahalanobis\",\n  ratio = M,\n  replace = TRUE\n)\nsummary(match_1vM)\n\n\nCall:\nmatchit(formula = card ~ pre_avg_purch + age + sex, data = membership, \n    method = \"nearest\", distance = \"mahalanobis\", estimand = \"ATT\", \n    replace = TRUE, ratio = M)\n\nSummary of Balance for All Data:\n              Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\npre_avg_purch         39.06          34.0           0.431        1.1     0.121    0.194\nage                   43.39          39.0           0.307        1.1     0.079    0.137\nsex                    0.52           0.5           0.035          .     0.017    0.017\n\nSummary of Balance for Matched Data:\n              Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\npre_avg_purch         39.06         38.71           0.030        1.1     0.008    0.037            0.13\nage                   43.39         43.01           0.027        1.1     0.008    0.028            0.13\nsex                    0.52          0.52           0.000          .     0.000    0.000            0.00\n\nSample Sizes:\n              Control Treated\nAll               547     453\nMatched (ESS)     326     453\nMatched           462     453\nUnmatched          85       0\nDiscarded           0       0\n\nCode# Use matched data\ndf_1vM &lt;- match.data(match_1vM)\nprint(df_1vM)\n\n# A tibble: 915 × 6\n     age   sex pre_avg_purch  card avg_purch weights\n   &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1  31.7     0          22.4     1    47.2     1    \n 2  41.5     1          32.5     0    72.7     0.680\n 3  23.2     0          31.5     1    57.4     1    \n 4  67.1     1          47.5     1    87.8     1    \n 5  27.4     1          30.3     1    72.5     1    \n 6  53.7     1          30.8     1    54.0     1    \n 7  40.2     0          37.7     0    10.2     1.02 \n 8  26.7     0          25.5     1    42.0     1    \n 9  28.7     0          26.0     0     0.561   0.340\n10  35.2     1          46.5     1    70.0     1    \n# ℹ 905 more rows\n\nCode# (2) Estimation\nmatchit_mod_1vM &lt;- lm(avg_purch ~ card, data = df_1vM, weights = weights)\nsummary(matchit_mod_1vM)\n\n\nCall:\nlm(formula = avg_purch ~ card, data = df_1vM, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-59.56 -13.31  -1.38  10.49  74.44 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   39.870      0.859    46.4   &lt;2e-16 ***\ncard          15.300      1.222    12.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18 on 913 degrees of freedom\nMultiple R-squared:  0.147, Adjusted R-squared:  0.146 \nF-statistic:  157 on 1 and 913 DF,  p-value: &lt;2e-16\n\n\nPropensity score matching\nWhen employing approaches based on covariate-matching, you quickly run into the curse of dimensionality as your number of covariates grows. As the dimensionality grows, it becomes increasingly difficult to find suitable matches, particularly in high-dimensional spaces where finding matches can be improbable.\nConsider a scenario with two covariates, each with five distinct values. In this case, observations fall into one of 25 cells defined by the covariate value grid. Now, envision ten covariates with three different values each, creating already approximately 60,000 cells. This significantly boosts the likelihood of many cells being populated by only one or zero observations, making it impossible to find matches for numerous observations.\nOne approach to tackle this issue is the use of propensity scores. Propensity score represents the predicted probability of treatment assignment based on matching variables. In our case, we utilize age, gender, and previous average purchases to predict the likelihood of a customer participating in the membership program. Specifically, customers who spend more on average are expected to have a higher likelihood of participation. To model this relationship, we employ logistic regression, a technique that predicts outcomes between zero and one, generating the propensity score.\n\\[\n\\pi_i = PS(\\mathbf{X_i}) = P(D_i = 1 | \\mathbf{X_i})\n\\]\nQ7. Run the matching using the propensity score as the matching variable. First, for each unit, compute the propensity of being treated.\n\nCode# Estimate propensity to be treated\nmod_ps &lt;- glm(\n  card ~ pre_avg_purch + age + sex,\n  family = binomial(link = 'logit'), \n  data = membership\n  )\n\n# Extract propensity score\nmembership &lt;- membership |&gt; mutate(propensity = mod_ps$fitted) # or: mod_ps$fitted.values\nsummary(membership$propensity)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.17    0.37    0.45    0.45    0.53    0.78 \n\n\nQ8. Having obtained the propensity score, use it as the matching variable.\n\nCode# (1) Matching\nmatch_ps &lt;- matchit(\n  card ~ propensity,\n  data = membership,\n  estimand = \"ATT\",\n  method = \"nearest\",\n  distance = \"mahalanobis\",\n  ratio = 1,\n  replace = TRUE\n)\nsummary(match_ps)\n\n\nCall:\nmatchit(formula = card ~ propensity, data = membership, method = \"nearest\", \n    distance = \"mahalanobis\", estimand = \"ATT\", replace = TRUE, \n    ratio = 1)\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\npropensity          0.48          0.43            0.44        1.2      0.12      0.2\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\npropensity          0.48          0.48               0          1     0.001    0.015           0.008\n\nSample Sizes:\n              Control Treated\nAll               547     453\nMatched (ESS)     186     453\nMatched           262     453\nUnmatched         285       0\nDiscarded           0       0\n\nCode# Equivalent:\n# match_ps2 &lt;- matchit(\n#   card ~ propensity,\n#   data = membership,\n#   estimand = \"ATT\",\n#   ratio = 1,\n#   replace = TRUE\n# )\n\n\n# Use matched data\ndf_ps &lt;- match.data(match_ps)\nprint(df_ps)\n\n# A tibble: 715 × 7\n     age   sex pre_avg_purch  card avg_purch propensity weights\n   &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1  31.7     0          22.4     1      47.2      0.320   1    \n 2  41.5     1          32.5     0      72.7      0.423   0.578\n 3  23.2     0          31.5     1      57.4      0.368   1    \n 4  67.1     1          47.5     1      87.8      0.608   1    \n 5  27.4     1          30.3     1      72.5      0.373   1    \n 6  53.7     1          30.8     1      54.0      0.440   1    \n 7  40.2     0          37.7     0      10.2      0.458   0.578\n 8  26.7     0          25.5     1      42.0      0.331   1    \n 9  35.2     1          46.5     1      70.0      0.522   1    \n10  21.8     0          35.7     1      45.5      0.397   1    \n# ℹ 705 more rows\n\nCode# (2) Estimation\nmod_ps &lt;- lm(avg_purch ~ card, data = df_ps, weights = weights)\nsummary(mod_ps)\n\n\nCall:\nlm(formula = avg_purch ~ card, data = df_ps, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-59.56 -13.29  -0.06  11.43 123.33 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    39.63       1.18    33.6   &lt;2e-16 ***\ncard           15.54       1.48    10.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19 on 713 degrees of freedom\nMultiple R-squared:  0.134, Adjusted R-squared:  0.132 \nF-statistic:  110 on 1 and 713 DF,  p-value: &lt;2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nInstead of running a separate logistic regression to compute the propensity scores, you could also provide the arguments method = \"glm\" and link = \"logit\" to the matchit() function.\n\n\nIt’s important to note that while propensity score matching effectively mitigates the curse of dimensionality, a fundamental issue arises: having the same propensity score does not guarantee that observations share identical covariate values. Conversely, identical covariate values do imply the same propensity score. This inherent asymmetry raises concerns among prominent statisticians, leading to criticism of propensity score matching as a robust identification strategy.1",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#inverse-probability-weighting",
    "href": "content/course_weeks/week_04/week_4.html#inverse-probability-weighting",
    "title": "4 - Matching",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nStep-by-step approach\nInstead inverse probability weighting (IPW) has proven to be a more precise method than matching approaches, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use the propensity score of an observation unit to increase or decrease its weights and thereby make some observations more important than others. The weight obtains as\n\\[\nw_i = \\frac{D_i}{\\pi_i} + \\frac{(1-D_i)}{(1-\\pi_i)}\n\\]\nwhere only one of the terms is always active as \\(D_i\\) is either one or zero. \\(\\pi_i\\) denotes the propensity. Now we should better understand what “inverse probability weighting” actually means. It weights each observation by its inverse of its treatment probability. Let’s proceed to calculate this for our dataset.\nQ9. Given the formula, calculate the weights for each observation.\n\nCode# Calculate inverse probability weights\nmembership &lt;- membership |&gt; mutate(ipw = (card / propensity) + (1-card) / (1-propensity))\nsummary(membership$ipw)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.2     1.6     1.9     2.0     2.3     4.4 \n\n\nWe need to provide these calculated weights as an additional argument to lm() using the weights argument.\n\nCode# Regression with inverse probability weighting\nmodel_ipw &lt;- lm(avg_purch ~ card, data = membership, weights = ipw)\nsummary(model_ipw)\n\n\nCall:\nlm(formula = avg_purch ~ card, data = membership, weights = ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-107.34  -18.56   -0.18   17.59  128.59 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   37.245      0.847    44.0   &lt;2e-16 ***\ncard          15.413      1.197    12.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27 on 998 degrees of freedom\nMultiple R-squared:  0.142, Adjusted R-squared:  0.142 \nF-statistic:  166 on 1 and 998 DF,  p-value: &lt;2e-16\n\n\nIntegrated approach\nFor demonstration and learning purpose, we split the procedure in two steps but there are external packages which combine both steps, e.g. the causalweight package and the function treatweight().\n\ninstall.packages(\"causalweight\")\n\n\nlibrary(causalweight)\n\n# IPW estimation\nmodel_ipw_int &lt;- treatweight(\n  y = membership$avg_purch,\n  d = membership$card,\n  x = membership[, c(\"pre_avg_purch\", \"age\", \"sex\")]\n)\nmodel_ipw_int\n\n$effect\n[1] 15\n\n$se\n[1] 0.99\n\n$pval\n[1] 3.8e-55\n\n$y1\n[1] 53\n\n$y0\n[1] 37\n\n$ntrimmed\n[1] 0",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#assignment",
    "href": "content/course_weeks/week_04/week_4.html#assignment",
    "title": "4 - Matching",
    "section": "Assignment",
    "text": "Assignment\n\nAccept the Week 4 - Assignment and follow the same steps as last week and as described in the organization chapter.\nRegression Adjustment\n\nWhen discussing your results from the regression adjustment for the sales of light jackets with your team, one of your analysts comes up with the idea to include temperature at the customers location into your analysis. Because you know the customer’s location, you can check the temperature and add it to the data. Load the data prices_new.rds.\n\nCheck whether adjusting for temperature improves your analysis using e.g. \\(R^2\\) or hypothesis tests.\nDo you think the relationship between sales and temperature is linear? Or do you suspect a non-linear relationship? Please argue, whether including a quadratic term into the functional form by y ~ x + I(x^2) + ... makes sense?\n\n\n\nMatching\nFor the next tasks, you are going to use the data set health_program.rds, which you should already be familiar with from the assignment two weeks ago. Identify the matching variables and perform\n\none-vs-one nearest-neighbor matching for \\(ATT\\) and \\(ATU\\),\nand propensity score matching for \\(ATE\\).\nLook at the matched data frames and explain the different number of rows.\n\nInverse probability weighting\n\nRun a logistic regression to estimate the treatment propensity.\nAdd the propensity scores to your data frame, compute the inverse probability weights and sort your data frame by them by running df |&gt; arrange(var) or df |&gt; arrange(-var). Take a look at the units with the highest or lowest weights. What do you notice? Use the logistic regression summary to argue why these observations have such high/low weights.\nRun the regression based on the calculated weights and compare it to the estimates from the matching estimators.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/week_04/week_4.html#footnotes",
    "href": "content/course_weeks/week_04/week_4.html#footnotes",
    "title": "4 - Matching",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://gking.harvard.edu/publications/why-propensity-scores-should-not-be-used-formatching↩︎",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "4 - Matching"
    ]
  },
  {
    "objectID": "content/course_weeks/overview.html",
    "href": "content/course_weeks/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Important\n\n\n\nDue to an unexpected scheduling conflict, we have to cancel today’s lecture (17 June). Instead, the lecture will take place tomorrow (18 June) from 15:00 to 16:30 in room O-0.007. Following the lecture, the tutorial will be held from 16:45 to 18:15 in room A-0.19.\nSorry for the inconvenience. See you tomorrow!\n\n\n\n\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nTopic\n\n\n\n\n\n\n\n1\nApril 15 & 16\nIntroduction to Causal Inference\n\n\n\n\n\n2\nApril 22 & 21\nGraphical Causal Models\n\n\n\n\n\n3\nApril 29 & 30\nRandomized Experiments & Linear Regression\n\n\n\n\n\n4\nMay 6 & 7\nMatching\n\n\n\n\n\n5\nMay 13 & 14\nDouble Machine Learning\n\n\n\n\n\n-\nMay 20 & 21\nHoliday\n-\n-\n-\n\n\n6\nMay 27 & 28\nEffect Heterogeneity\n\n\n\n\n\n7\nJune 3 & 4\nUnobserved Confounding & Instrumental Variables\n\n\n\n\n\n8\nJune 10 & 11\nRecap\n-\n\n\n\n\n9\nJune 18\nDifference-in-Differences\n\n\n\n\n\n10\nJune 24 & 25\nSynthetic Control\n\n\n\n\n\n11\nJuly 1 & 2\nRegression Discontinuity\n\n\n\n\n\n12\nJuly 8 & 9\nCausal Mediation",
    "crumbs": [
      "Content",
      "Overview"
    ]
  },
  {
    "objectID": "content/slides/01_intro.html#learning-goals",
    "href": "content/slides/01_intro.html#learning-goals",
    "title": "(1) Introduction to Causal Inference",
    "section": "Learning Goals",
    "text": "Learning Goals\n\n\nUnderstand the difference between “correlation” and “causation”\nUnderstand the shortcomings of current correlation-based approaches\nDevelop causal knowledge relevant for specific data-driven decisions\nFormalize intuition about causal relationships using a “language” of causality\nDerive causal hypotheses that can be tested with data\nDiscuss the conceptual ideas behind state-of-the-art causal data science tools and algorithms\nCarry out causal data analyses with state-of-the-art tools"
  },
  {
    "objectID": "content/slides/01_intro.html#map-of-causality",
    "href": "content/slides/01_intro.html#map-of-causality",
    "title": "(1) Introduction to Causal Inference",
    "section": "Map of Causality",
    "text": "Map of Causality\n\n\nSource: https://towardsdatascience.com (2023)."
  },
  {
    "objectID": "content/slides/01_intro.html#preliminary-schedule",
    "href": "content/slides/01_intro.html#preliminary-schedule",
    "title": "(1) Introduction to Causal Inference",
    "section": "Preliminary Schedule",
    "text": "Preliminary Schedule\n\n\n\nSession\nDate\nTopic\n\n\n\n\n1\nApril 15 & 16\nIntroduction to Causal Inference\n\n\n2\nApril 22 & 21\nGraphical Causal Models\n\n\n3\nApril 29 & 30\nRandomized Experiments\n\n\n4\nMay 6 & 7\nObserved Confounding\n\n\n5\nMay 13 & 14\nDouble Machine Learning\n\n\n-\nMay 20 & 21\nHoliday\n\n\n6\nMay 27 & 28\nEffect Heterogeneity\n\n\n7\nJune 3 & 4\nUnobserved Confounding & Instrumental Variables\n\n\n8\nJune 10 & 11\nDifference-in-Difference\n\n\n9\nJune 17 & 18\nSynthetic Control\n\n\n10\nJune 24 & 25\nRegression Discontinuity\n\n\n11\nJuly 1 & 2\nCausal Mediation\n\n\n12\nJuly 8 & 9\nFurther Topics in Causal Machine Learning"
  },
  {
    "objectID": "content/slides/01_intro.html#course-structure",
    "href": "content/slides/01_intro.html#course-structure",
    "title": "(1) Introduction to Causal Inference",
    "section": "Course Structure",
    "text": "Course Structure\n\nLecture - Causal Data Science: Monday, 11.30 - 13.00, Building D, Room D - 1.023\nLab - Business Analytics with Causal Data Science: Tuesday, 15.00 - 16.30, Building O, Room O - 0.007\n\n\n\nExamination: 10 challenges related to each topic documented in a lab journal\n\n\n\nContact: Oliver Mork (oliver.mork@tuhh.de)"
  },
  {
    "objectID": "content/slides/01_intro.html#course-literature",
    "href": "content/slides/01_intro.html#course-literature",
    "title": "(1) Introduction to Causal Inference",
    "section": "Course Literature",
    "text": "Course Literature\n\nPrimary:Secondary:\n\n\n\nDing, Peng (2023). A First Course in Causal Inference. arXiv preprint arXiv:2305.18793.\nFacure, Matheus (2023). Causal Inference in Python - Applying Causal Inference in the Tech Industry. O’Reilly Media.\nHuber, Martin (2023). Causal analysis: Impact evaluation and Causal Machine Learning with applications in R. MIT Press, 2023.\nNeal, Brady (2020). Introduction to causal inference from a Machine Learning Perspective. Course Lecture Notes (draft).\n\n\n\n\nAngrist, J. D., & Pischke, J. S. (2014). Mastering metrics: The path from cause to effect. Princeton university press.\nCunningham, Scott (2021). Causal Inference: The Mixtape, New Haven: Yale University Press.\nGertler, Paul J., et al. (2016). Impact evaluation in practice. World Bank Publications.\nHernán Miguel A., and Robins James M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nHuntington-Klein, Nick (2021). The effect: An introduction to research design and causality. Chapman and Hall/CRC.\nImbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences. Cambridge University Press.\nMullainathan, Sendhil, and Jann Spiess. (2017). Machine Learning: An Applied Econometric Approach. Journal of Economic Perspectives, 31(2): 87–106.\nPearl, Judea, and Dana Mackenzie (2018). The Book of Why. Basic Books, New York, NY.\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell (2016). Causal Inference in Statistics: A Primer. John Wiley & Sons, Inc., New York, NY.\nPeters, Jonas, Dominik Janzing, and Bernhard Schölkopf (2017). Elements of causal inference: foundations and learning algorithms. The MIT Press."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation",
    "href": "content/slides/01_intro.html#causality-vs.-correlation",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\nCausality is central to human knowledge.\nTwo famous quotes from ancient Greeks:\n\n“I would rather discover one causal law than be King of Persia.” (Democritus)\n“We do not have knowledge of a thing until we grasped its cause.” (Aristotle) \n\nHowever:\n\nClassic statistics is about association rather than causation.\nMachine learning is about prediction rather than causation."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation-1",
    "href": "content/slides/01_intro.html#causality-vs.-correlation-1",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\n“Correlation does not imply causation.”\n“You can not prove causality with statistics.” \nBut statistics is crucial for understanding causality:\n\nFormal language for causal inference.\nMethods to estimate causal effects."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation-2",
    "href": "content/slides/01_intro.html#causality-vs.-correlation-2",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\n\nSource: https://hbr.org/2021/11/leaders-stop-confusing-correlation-with-causation."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation-3",
    "href": "content/slides/01_intro.html#causality-vs.-correlation-3",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\n\nSource: https://www.tylervigen.com/spurious- correlations."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation-4",
    "href": "content/slides/01_intro.html#causality-vs.-correlation-4",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\n\n\n\n\n\nSource: Peters, Jonas. 2015. Causality: Lecture Notes, ETH Zurich."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation-5",
    "href": "content/slides/01_intro.html#causality-vs.-correlation-5",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\nCorrelation, or better association, is not (entirely) causation, if there is confounding association due to a common cause, i.e. a confounder.\nE.g. drinking the night before is a common cause of sleeping with shoes on and of waking up with a headache:\n\n \n\nSource: Neal, Brady (2020). Introduction to causal inference from a Machine Learning Perspective. Course Lecture Notes (draft)."
  },
  {
    "objectID": "content/slides/01_intro.html#causality-vs.-correlation-6",
    "href": "content/slides/01_intro.html#causality-vs.-correlation-6",
    "title": "(1) Introduction to Causal Inference",
    "section": "Causality vs. Correlation",
    "text": "Causality vs. Correlation\n\nCorrelation, or better association, is not (entirely) causation, if there is confounding association due to a common cause, i.e. a confounder.\nE.g. Consumers’ purchase intent is a common cause of the amount spent on search engine marketing (SEM) (esp. for branded vs. non-branded ads) and sales (especially for frequent consumers):\n\n\n\nShow code\nlibrary(ggdag)\nlibrary(ggplot2)\n\ncoord_dag &lt;- list(\n  x = c(SEM = 0, Intent = 1, Sales = 2),\n  y = c(SEM = 0, Intent = 1, Sales = 0)\n)\n\ndag &lt;- ggdag::dagify(SEM ~ Intent,\n                     Sales ~ SEM,\n                     Sales ~ Intent,\n                     coords = coord_dag)\n\ndag %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(colour = \"grey\") +\n  geom_dag_edges() +\n  geom_dag_text(colour = \"black\", size = 5) +\n  theme_dag(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n(Source: Blake et al. (2015). Consumer heterogeneity and paid search effectiveness: A large‐scale field experiment. Econometrica, 83(1), 155-174.)"
  },
  {
    "objectID": "content/slides/01_intro.html#motivating-example-gender-pay-gap-1",
    "href": "content/slides/01_intro.html#motivating-example-gender-pay-gap-1",
    "title": "(1) Introduction to Causal Inference",
    "section": "Motivating Example: Gender Pay Gap (1)",
    "text": "Motivating Example: Gender Pay Gap (1)\n- Reported by The New York Times in March 2019:\n      “When Google conducted a study recently to determine whether\n      the company was underpaying women and minority groups, it found\n      that men were paid less money than women for doing similar work.”\n\n(Source: https://www.nytimes.com/2019/03/04/technology/google-gender-pay-gap.html)\n\n\nThe study led Google to increase the pay of its male employees to fight this blatant discrimination of men.\nWhat’s going on here? Wasn’t Google just recently accused of discriminating against women, not men?\n\n      “Department of Labor claims that Google systematically underpays its\n      female employees.”\n\n(Source: https://www.theverge.com/2017/4/8/15229688/department-of-labor-google-gender-pay-gap)"
  },
  {
    "objectID": "content/slides/01_intro.html#motivating-example-gender-pay-gap-2",
    "href": "content/slides/01_intro.html#motivating-example-gender-pay-gap-2",
    "title": "(1) Introduction to Causal Inference",
    "section": "Motivating Example: Gender Pay Gap (2)",
    "text": "Motivating Example: Gender Pay Gap (2)\n\nSuppose we collected data on wages payed to 100 women and 100 men in company X.\nWe observe the following average monthly salaries for women and men in management and non-management positions (case numbers in parentheses):\n\n\n\n\n\nWomen\nMen\n\n\n\n\n\nNon-management:\n$3,163.30 (87)\n$3,015.18 (59)\n\n\n\nManagement:\n$5,592.44 (13)\n$5,319.82 (41)\n\n\n\n\n\n\nOur goal is to estimate the magnitude of the gender pay gap in company X. How should we tackle this problem?"
  },
  {
    "objectID": "content/slides/01_intro.html#motivating-example-gender-pay-gap-3",
    "href": "content/slides/01_intro.html#motivating-example-gender-pay-gap-3",
    "title": "(1) Introduction to Causal Inference",
    "section": "Motivating Example: Gender Pay Gap (3)",
    "text": "Motivating Example: Gender Pay Gap (3)\n\nOn average, women earn less in this example: \\[\n   \\left(\\frac{87}{100} \\cdot \\$3163.30\\right) + \\left(\\frac{13}{100} \\cdot \\$5592.44\\right) -\n   \\left(\\frac{59}{100} \\cdot \\$3015.18\\right) + \\left(\\frac{41}{100} \\cdot \\$5319.82\\right) \\\\\n   \\approx -\\$481\n\\]\nBut in each subcategory women actually have higher salaries:\n\n  - Non- Management: \\(\\$3163.30 - \\$3015.18 = \\$148.12\\)\n  - Management: \\(\\$5592.44 - \\$5319.82 = \\$272.62\\)\n\nConditioning on job position gives the adjusted gender pay gap:\n\n\\[\n   \\left(\\frac{87 + 59}{200} \\cdot \\$148.12\\right) + \\left(\\frac{13 + 41}{200} \\cdot \\$272.62\\right) \\approx \\$181.74\n\\]\n\nWhich estimate gives us a more accurate picture of the gender pay gap?"
  },
  {
    "objectID": "content/slides/01_intro.html#motivating-example-gender-pay-gap-4",
    "href": "content/slides/01_intro.html#motivating-example-gender-pay-gap-4",
    "title": "(1) Introduction to Causal Inference",
    "section": "Motivating Example: Gender Pay Gap (4)",
    "text": "Motivating Example: Gender Pay Gap (4)\n\n\nShow code\ndata &lt;- data.frame(\n  Salary = c(5319.82, 3015.18, 5592.44, 3163.30, 3960.08, 3479.09),\n  Position = c(\"Management\", \"Non-Management\", \"Management\", \"Non-Management\", \"All Positions\", \"All Positions\"),\n  Gender = c(\"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Female\")\n)\n\nlibrary(ggplot2)\n\ndata |&gt; \n  ggplot(aes(x=Gender, y=Salary, group=Position, colour=Position)) +\n  geom_line() + geom_point() + \n  theme_bw()"
  },
  {
    "objectID": "content/slides/01_intro.html#simpsons-paradox-1",
    "href": "content/slides/01_intro.html#simpsons-paradox-1",
    "title": "(1) Introduction to Causal Inference",
    "section": "Simpson’s Paradox (1)",
    "text": "Simpson’s Paradox (1)\n\nThe phenomenon that a statistical association, which holds in a population, can be reversed in every subpopulation is named after the British statistician Edward Simpson.\nSimpson’s paradox well-known, for example, in epidemiology and labor economics.\nIn the gender pay gap example, the unadjusted gender pay (- $481) gap gives the right answer.\n\n\n\nBut what about this example?\n\n\n\n\n\nHealthy Lifestyle\nUnhealthy Lifestyle\n\n\n\n\n\nNon-management:\n$3,163.30 (87)\n$3,015.18 (59)\n\n\n\nManagement:\n$5,592.44 (13)\n$5,319.82 (41)"
  },
  {
    "objectID": "content/slides/01_intro.html#simpsons-paradox-2",
    "href": "content/slides/01_intro.html#simpsons-paradox-2",
    "title": "(1) Introduction to Causal Inference",
    "section": "Simpson’s Paradox (2)",
    "text": "Simpson’s Paradox (2)\n\n\n\n\n\n\n\n\n\n\nHere, we would correctly infer that people with a healthy lifestyle earn more on average ($181.74)."
  },
  {
    "objectID": "content/slides/01_intro.html#simpsons-paradox-3",
    "href": "content/slides/01_intro.html#simpsons-paradox-3",
    "title": "(1) Introduction to Causal Inference",
    "section": "Simpson’s Paradox (3)",
    "text": "Simpson’s Paradox (3)\n\nWhat is the difference between the two examples?\n\n\n\n\n\nShow code\nlibrary(ggdag)\n\ncoord_dag &lt;- list(\n  x = c(Gender = 0, Management = 1, Salary = 2),\n  y = c(Gender = 0, Management = 1, Salary = 0)\n)\n\ndag &lt;- ggdag::dagify(Management ~ Gender,\n                     Salary ~ Gender,\n                     Salary ~ Management,\n                     coords = coord_dag)\n\ndag %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(colour = \"grey\") +\n  geom_dag_edges() +\n  geom_dag_text(colour = \"black\", size = 5) +\n  theme_dag(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nManagement as “mediator”\n\n\n\n\nShow code\nlibrary(ggdag)\n\ncoord_dag &lt;- list(\n  x = c(Lifestyle = 0, Management = 1, Salary = 2),\n  y = c(Lifestyle = 0, Management = 1, Salary = 0)\n)\n\ndag &lt;- ggdag::dagify(Lifestyle ~ Management,\n                     Salary ~ Lifestyle,\n                     Salary ~ Management,\n                     coords = coord_dag)\n\ndag %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(colour = \"grey\") +\n  geom_dag_edges() +\n  geom_dag_text(colour = \"black\", size = 5) +\n  theme_dag(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nManagement as “confounder”"
  },
  {
    "objectID": "content/slides/01_intro.html#simpsons-paradox-4",
    "href": "content/slides/01_intro.html#simpsons-paradox-4",
    "title": "(1) Introduction to Causal Inference",
    "section": "Simpson’s Paradox (4)",
    "text": "Simpson’s Paradox (4)\n\nStatistics alone doesn’t help us to answer this question.\nNote that the joint distribution of salaries is the same in both cases.\nBoth problems are thus identical from a statistical point of view.\nInstead, we need to make causal assumptions in order to come to a conclusion here:\n\nGender affects both a person’s salary level and job position.\nWhereas lifestyle affects salaries, but is itself affected by a person’s job position.\n\nAfter the course you will know how to incorporate this kind of causal knowledge in your analysis in order to solve all sorts of practical problems of causal inference."
  },
  {
    "objectID": "content/slides/01_intro.html#simpsons-paradox-5",
    "href": "content/slides/01_intro.html#simpsons-paradox-5",
    "title": "(1) Introduction to Causal Inference",
    "section": "Simpson’s Paradox (5)",
    "text": "Simpson’s Paradox (5)\n\n\nSource: https://rpubs.com/lakenp/simpsonsparadox."
  },
  {
    "objectID": "content/slides/01_intro.html#experimentalists-view-of-causal-inference",
    "href": "content/slides/01_intro.html#experimentalists-view-of-causal-inference",
    "title": "(1) Introduction to Causal Inference",
    "section": "Experimentalists’ View of Causal Inference",
    "text": "Experimentalists’ View of Causal Inference\n\n“No causation without manipulation.” (Rubin, 1975; Holland, 1986)\n(Thought) Experiments with manipulation; also called intervention or treatment.\nTreatments can be binary, continuous, or multi-valued.\nExamples:\n\ntake a drug vs. don’t take a drug\nparticipate in a training program A vs B vs. don’t participate\namount of money spent on advertising\nchange race of job applicants? Resumes with African-American- or White-sounding names (Bertrand and Mullainathan, 2004).\nlevel of neuroticism?\n\nThe potential outcomes framework (Neyman, 1923; Rubin, 1974) is a way to formalize this idea."
  },
  {
    "objectID": "content/slides/01_intro.html#formal-notation-of-potential-outcomes",
    "href": "content/slides/01_intro.html#formal-notation-of-potential-outcomes",
    "title": "(1) Introduction to Causal Inference",
    "section": "Formal Notation of Potential Outcomes",
    "text": "Formal Notation of Potential Outcomes\n\n\\(n\\) experimental units indexed by \\(i = 1, . . . , n\\)\n\\(Y\\) is the outcome of interest.\n\\(T_i\\) is the (random) treatment variable for unit \\(i\\).\n\nAssume it can take two levels: \\(t_i = 1\\) for treatment and \\(t_i = 0\\) for control.\n\n\\(Y_i(1)\\) is the potential outcome for unit \\(i\\) if unit \\(i\\) receives treatment.\n\\(Y_i(0)\\) is the potential outcome for unit \\(i\\) if unit \\(i\\) does not receive treatment. \nThe Individual Treatment Effect (ITE) for unit \\(i\\) is defined as:\n\n\\(\\tau_i = Y_i(1) - Y_i(0) \\quad \\forall \\quad i = 1, . . . , n\\)."
  },
  {
    "objectID": "content/slides/01_intro.html#assumptions-in-the-po-framework-1",
    "href": "content/slides/01_intro.html#assumptions-in-the-po-framework-1",
    "title": "(1) Introduction to Causal Inference",
    "section": "Assumptions in the PO Framework (1)",
    "text": "Assumptions in the PO Framework (1)\nFor the potential outcomes and the ITE to be precisely defined, we need to make an initial set of assumptions:\n\n\n\nAssumption 1: “No Interference”\n\n\nUnit i’s potential outcomes do not depend on other units’ treatments.\n\\(Y_i(t_1,...,t_{i-1},t_i,t_{i+1},...t_n) = Y_i(t_i)\\)\n\n\n\n\n\n\nAssumption 2: “Consistency.”\n\n\nThere are no other versions of the treatment. Equivalently, we require that the treatment levels be well-defined, or have no ambiguity at least for the outcome of interest. If the treatment is \\(T\\), then the observed outcome \\(Y\\) is the potential outcome under treatment \\(T\\).\nFormally, \\(T = t  =&gt; Y = Y(t)\\) or equivalently \\(Y = Y(T)\\)\n\n\n\n\n\n\nAssumption 3: “Stable Unit Treatment Value Assumption (SUTVA).”\n\n\nBoth Assumptions 1 and 2 hold: \\(Y_i = Y(T_i)\\)"
  },
  {
    "objectID": "content/slides/01_intro.html#fundamental-problem-of-causal-inference",
    "href": "content/slides/01_intro.html#fundamental-problem-of-causal-inference",
    "title": "(1) Introduction to Causal Inference",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nTypically, only one of those outcomes is actually observed for unit \\(i\\):\n\n\\(Y_i = T_iY_i(1) + (1 − T_i)Y_i(0)\\).\n\nThe other one remains unobserved or counterfactual.\nThis makes calculating the ITE \\(\\tau_i\\) impossible.\n\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i\\)\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(Y_i(1) - Y_i(0)\\)\n\n\n\n\n1\n0\n0\n?\n0\n?\n\n\n2\n1\n1\n1\n?\n?\n\n\n3\n1\n0\n0\n?\n?\n\n\n4\n0\n0\n?\n0\n?\n\n\n5\n0\n1\n?\n1\n?\n\n\n6\n1\n1\n1\n?\n?"
  },
  {
    "objectID": "content/slides/01_intro.html#getting-around-the-fundamental-problem",
    "href": "content/slides/01_intro.html#getting-around-the-fundamental-problem",
    "title": "(1) Introduction to Causal Inference",
    "section": "Getting Around the Fundamental Problem",
    "text": "Getting Around the Fundamental Problem\n\nDoes the Average Treatment Effect (ATE) help?\nDefined in terms of expectations:\n\n\\(\\tau = \\mathbb{E}[\\tau_i] = \\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)]\\)\n\nDefined in terms of averages:\n\n\\(\\tau = \\frac{1}{n} \\sum_{i=1}^{n} [\\tau_i] = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i(1) - Y_i(0)] = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i(1)] - \\frac{1}{n} \\sum_{i=1}^{n} [Y_i(0)]\\)\n\n\n\n\nStill not computable, because we don’t know the counterfactuals."
  },
  {
    "objectID": "content/slides/01_intro.html#assumptions-in-the-po-framework-2",
    "href": "content/slides/01_intro.html#assumptions-in-the-po-framework-2",
    "title": "(1) Introduction to Causal Inference",
    "section": "Assumptions in the PO Framework (2)",
    "text": "Assumptions in the PO Framework (2)\n\nWe need to make further assumptions to make progress.\n\n\n\n\nAssumption 4: “Ignorability / Exchangeability”.\n\n\nIgnorability (of how people selected their treatment) is equivalent to random assignment into treatments.\nExchangeability means that observations in treatment and control group could be swapped, and one would still obtain the same outcomes. This implies that observations in groups are the same in all relevant aspects other than the treatment.\nFormally, \\((Y(1), Y(0)) \\perp\\!\\!\\!\\perp T\\)."
  },
  {
    "objectID": "content/slides/01_intro.html#ate-identification---intuition",
    "href": "content/slides/01_intro.html#ate-identification---intuition",
    "title": "(1) Introduction to Causal Inference",
    "section": "ATE Identification - Intuition",
    "text": "ATE Identification - Intuition\n\nUsing assumptions 4 and 2, we obtain the following simplification:\n\n\\(\\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mathbb{E}[Y_i(1)|T_i=1] - \\mathbb{E}[Y_i(0)|T_i=0] = \\mathbb{E}[Y_i|T_i=1] - \\mathbb{E}[Y_i|T_i=0]\\)\n\nThis implies the ATE to be obtainable as associational difference:\n\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i\\)\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\n\n\n\n1\n0\n0\n?\n0\n\n\n4\n0\n0\n?\n0\n\n\n5\n0\n1\n?\n1\n\n\n2\n1\n1\n1\n?\n\n\n3\n1\n0\n0\n?\n\n\n6\n1\n1\n1\n?\n\n\n\n\nWe can then estimate \\(\\mathbb{E}[Y_i|T_i=1] = 0.66\\) and \\(\\mathbb{E}[Y_i|T_i=0] = 0.33\\) and use these values to replace the missing counterfactuals.\nATE is now identifiable in the sense that it can be computed from a purely statistical quantity."
  },
  {
    "objectID": "content/slides/01_intro.html#assumptions-in-the-po-framework-3",
    "href": "content/slides/01_intro.html#assumptions-in-the-po-framework-3",
    "title": "(1) Introduction to Causal Inference",
    "section": "Assumptions in the PO Framework (3)",
    "text": "Assumptions in the PO Framework (3)\n\nLet’s make exchangeability more realistic, i.e. conditional on covariates, so that subgroups will be exchangable.\n\n\n\n\nAssumption 5: “Conditional Exchangeability / Unconfoundedness”.\n\n\nFormally, \\((Y(1), Y(0)) \\perp\\!\\!\\!\\perp T \\, | \\, X\\)."
  },
  {
    "objectID": "content/slides/01_intro.html#assumptions-in-the-po-framework-4",
    "href": "content/slides/01_intro.html#assumptions-in-the-po-framework-4",
    "title": "(1) Introduction to Causal Inference",
    "section": "Assumptions in the PO Framework (4)",
    "text": "Assumptions in the PO Framework (4)\n\nConditioning on many covariates can also be detrimental, because we might end up conditioning on a zero probability event for some subgroups / values of X (division by zero)\n\n\n\n\nAssumption 6: “Positivity / Overlap / Common Support”.\n\n\nFor all values of covariates \\(x\\) present in the population of interest (i.e. \\(x\\) such that \\(P(X=x) &gt; 0\\)), we have \\(0 &lt; P(T=1|X=x) &lt; 1\\).\n\n\n\n\nThere is a trade-off between positivity and unconfoundedness.\nSome models might be forced to extrapolate to regions without sufficient support by using their parametric assumptions."
  },
  {
    "objectID": "content/slides/01_intro.html#derivation-of-the-average-treatment-effect-ate",
    "href": "content/slides/01_intro.html#derivation-of-the-average-treatment-effect-ate",
    "title": "(1) Introduction to Causal Inference",
    "section": "Derivation of the Average Treatment Effect (ATE)",
    "text": "Derivation of the Average Treatment Effect (ATE)\n\nWith the assumptions of conditional unconfoundedness, positivity, consistency, and no interference, we can identify the ATE as:\n\n\n\n\nTheorem 1: “Identification of the ATE”:\n\n\n\\(\\tau = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mathbb{E_X}[\\mathbb{E}[Y_i|T_i=1, X_i] - \\mathbb{E}[Y_i|T_i=0, X_i]]\\)"
  },
  {
    "objectID": "content/slides/01_intro.html#derivation-of-the-average-treatment-effect-ate-1",
    "href": "content/slides/01_intro.html#derivation-of-the-average-treatment-effect-ate-1",
    "title": "(1) Introduction to Causal Inference",
    "section": "Derivation of the Average Treatment Effect (ATE)",
    "text": "Derivation of the Average Treatment Effect (ATE)\n\nProof:\n\n\\[\\begin{align*}\n\\tau = \\mathbb{E}[\\tau_i] &= \\mathbb{E}[Y_i(1) - Y_i(0)] \\\\\n&= \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] \\\\\n& \\text{(linearity of expectation)} \\\\\n&= \\mathbb{E}_X [\\mathbb{E}[Y_i(1) \\mid X_i]] - \\mathbb{E}_X [\\mathbb{E}[Y_i(0) \\mid X_i]] \\\\\n&\\text{(law of iterated expectations)} \\\\\n&= \\mathbb{E}_X [\\mathbb{E}[Y_i(1) \\mid T_i = 1, X_i]] - \\mathbb{E}_X [\\mathbb{E}[Y_i(0) \\mid T_i = 0, X_i]] \\\\\n&\\text{(unconfoundedness and positivity)} \\\\\n&= \\mathbb{E}_X [\\mathbb{E}[Y_i \\mid T_i = 1, X_i]] - \\mathbb{E}_X [\\mathbb{E}[Y_i \\mid T_i = 0, X_i]] \\\\\n&\\text{(consistency)}\n\\end{align*}\\]"
  },
  {
    "objectID": "content/slides/01_intro.html#other-causal-quantities",
    "href": "content/slides/01_intro.html#other-causal-quantities",
    "title": "(1) Introduction to Causal Inference",
    "section": "Other Causal Quantities",
    "text": "Other Causal Quantities\n\nThe ATE is just one of many causal quantities that can be estimated using the PO framework.\n\n\n\n\n“Average Treatment Effect on the Treated” (ATT):\n\n\n\\(ATT = \\mathbb{E}[Y_i(1)|T_i=1] - \\mathbb{E}[Y_i(0)|T_i=1]\\)\n\n\n\n\n\n\n“Conditional Average Treatment Effect” (CATE):\n\n\n\\(CATE = \\mathbb{E}[Y_i(1)|X_i=x] - \\mathbb{E}[Y_i(0)|X_i=x]\\)"
  },
  {
    "objectID": "content/optional_read/stats.html",
    "href": "content/optional_read/stats.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Now we will delve into some statistical concepts, that are the foundation for statistical modeling processes used in causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to read here.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Statistical Concepts"
    ]
  },
  {
    "objectID": "content/optional_read/stats.html#footnotes",
    "href": "content/optional_read/stats.html#footnotes",
    "title": "Statistical Concepts",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://ourworldindata.org/human-height↩︎",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Statistical Concepts"
    ]
  },
  {
    "objectID": "content/organization/submission.html",
    "href": "content/organization/submission.html",
    "title": "GitHub & Submission",
    "section": "",
    "text": "For each week, there will be assignments for you to solve writing and rendering R programming code in .qmd files using RStudio and uploading these files via GitHub1. We’ll go through it step by step, but briefly summarized, .qmd files allow writing text and code and rendering presentable .html files based on the Quarto2 infrastructure and GitHub is is a hosting platform for so called repositories, which typically consists of data and code. But don’t worry, after the instructions on this page, it will get a lot clearer.\n\n\n\n\n\n\nTip\n\n\n\nIn the following I will guide your through the necessary steps. Some details in the screenshot might not be identical and slightly deviate, but you should be able to follow the general workflow.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#instructions",
    "href": "content/organization/submission.html#instructions",
    "title": "GitHub & Submission",
    "section": "",
    "text": "For each week, there will be assignments for you to solve writing and rendering R programming code in .qmd files using RStudio and uploading these files via GitHub1. We’ll go through it step by step, but briefly summarized, .qmd files allow writing text and code and rendering presentable .html files based on the Quarto2 infrastructure and GitHub is is a hosting platform for so called repositories, which typically consists of data and code. But don’t worry, after the instructions on this page, it will get a lot clearer.\n\n\n\n\n\n\nTip\n\n\n\nIn the following I will guide your through the necessary steps. Some details in the screenshot might not be identical and slightly deviate, but you should be able to follow the general workflow.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#initializing-github-github-desktop",
    "href": "content/organization/submission.html#initializing-github-github-desktop",
    "title": "GitHub & Submission",
    "section": "Initializing GitHub & GitHub Desktop",
    "text": "Initializing GitHub & GitHub Desktop\n\nCreate a free GitHub account. If you already have a GitHub account, you can skip this step.\nDownload and install GitHub Desktop and connect it to your account (Sign into GitHub.com). GitHub Desktop is a graphical user interface, which allows you to sync your local code changes with your online repository.\nCheck if git is installed on your system. It should already be the case but you can check it by opening RStudio, going to the terminal pane and entering which git. It should output the file path to git on your system. If you don’t get the expected result, download and install git.\nAccept the assignment and follow through the steps to enter the virtual classroom. This is the assignment for the first week. For each week, you will be provided with a new assignment, which will be linked in the respective chapter. \nAfter a while (refresh your page), you will get the link to your repository, which is currently free of any content but contains the framework to publish your solutions at a later stage. Click on that link, which is highlighted in blue.\nClone your repository, i.e. you create a local version on your hard drive. Until now, your repository is online hosted on GitHub, but of course, you need a local version to open the files in RStudio and add your solutions and code to your repository. It will open GitHub Desktop (sign into your account if you haven’t done already) and lets you set the path on your local drive. \nAfter setting the path and confirming by clicking Clone, you will be asked how you are planning “to use the fork”. Please make sure to select For my own purposes. This is important, because you do not want to change the original repository, but have your own version of it that you will modify (if you look carefully, you see that the links for the two options differ). \nNow go into your previously specified path and check whether the local version of the repository exists. If not, please read the steps again and check what you missed out.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#implementing-your-solution",
    "href": "content/organization/submission.html#implementing-your-solution",
    "title": "GitHub & Submission",
    "section": "Implementing your solution",
    "text": "Implementing your solution\nYou will find a couple of files in the repository. Let’s discuss some of them:\n\nss24_cds_week_1.Rproj: a R project file. It opens RStudio and mainly sets the correct working environment, which e.g. helps you load and save files.\nplayground.R : a classical R script. Use it to prototype your code and solutions.\nsubmission.qmd : a file that contains a combination of markdown and executable code cells. Here, you will type in your final solutions and render a .html that will be graded.\n.csv files: data you will need for the lab session and assignments\n\nThese files, that are typically found in GitHub repositories, you can ignore for now:\n\nREADME.md : to describe what the purpose of the repository is.\n.gitignore : to set what files should be excluded from pushing changes.\n\nNow, let’s simulate the workflow you will be going through to submit your solutions. We will just make a simple change and update the online version of the repository. But it is the exact workflow needed for all your submissions in the coming weeks.\n\nYou should be already in your local repository. Open the file ss24_cds_week_1.Rproj. This will automatically open RStudio, and your current working environment will be inside this project. That means everything you save will be auto saved to this folder (unless you tell RStudio to save something somewhere else. Have a look at the files tab in the bottom right hand corner. Most files you click will be opened up as text files in the RStudio editor.\nNow open submission.qmd and take a look at the assignments. The first one is already solved, so let’s take a look at the second one. Obviously, the questions are only for the purpose of demonstration, but let’s assume the question would be difficult.\nOpen playground.R to try to find the solution to the question. Here, you will probably quickly come up with the right solution, which is to simply type 2+2 and let R give you the result.\nTransfer the command to submission.qmd and paste it into an executable code chunk. At the top, you find a green C button with a plus to Insert a new code chunk (please also see the shortcut). Save the file.\nThen click on Render which will create the .html file which is your actual submission. On most devices, the file will automatically be opened. If not, find the file in your folder and open it. Check whether you see the intended output. It should look like this file, but with the new solution added. If it looks very different from the example, please check whether in the header of your .qmd file embed-resources: true is enabled.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#uploading-your-solution",
    "href": "content/organization/submission.html#uploading-your-solution",
    "title": "GitHub & Submission",
    "section": "Uploading your solution",
    "text": "Uploading your solution\n\nIf you are happy with your solution and the rendered file, you still have to upload it to GitHub. Otherwise, we can’t see it and are not able to grade it. Therefore, Go back to GitHub Desktop. You should see something similar to the image below. You can see what you changed in the .qmd and the accompanied html should be different as well. There might be a lot more changes that you expect because a lot of stuff ran in the background when rendering.\n\n\n\n\nCommit\n\n\n\nNow you still need to push your changes to GitHub. First, commit your changes at the left bottom by providing a short description of what you have changed and click on Commit to main. Now you can push to origin (you might have to Fetch origin beforehand).\n\n 3. Take a look at your online repository and check if everything was successfully uploaded.\n\n\n\n\n\n\nTip\n\n\n\nFor each assignment, there will be a deadline and you can only push changes to GitHub until that deadline. As long as it is before the deadline, you can make as many changes as you want.",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#submission",
    "href": "content/organization/submission.html#submission",
    "title": "GitHub & Submission",
    "section": "Submission",
    "text": "Submission\nSubmit your credentials and GitHub user name via the following form. If you do not submit your information, we won’t be able to evaluate your assignments. Please fill it out by Monday, 22 April 2024.\nWird geladen…\n\n\n\n\n\n\nSummary: How to successfully submit\n\n\n\n\nFill out the form. (once)\nWrite your solutions down in a .qmd file and render.\nCommit and push your changes to your GitHub\n\n\n\n\n\n\nCommit",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/submission.html#footnotes",
    "href": "content/organization/submission.html#footnotes",
    "title": "GitHub & Submission",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/↩︎\nhttps://quarto.org/↩︎",
    "crumbs": [
      "Organization",
      "GitHub & Submission"
    ]
  },
  {
    "objectID": "content/organization/mattermost.html",
    "href": "content/organization/mattermost.html",
    "title": "Mattermost",
    "section": "",
    "text": "In the course of the next chapters, we will do a lot of coding and errors will occur all the time. That is nothing you should be afraid of and in fact, dealing with errors is an elementary component in programming in data science.\nIn most cases, other people from around the world have had similar problems and you will find the right solution to your problem by just googling it. Two great resources to help you are StackOverflow and RStudio Community. Please try to do that as a first step when you run into an error.\nIf you have any questions about the class content, coding problems and other challenges, please use our Mattermost channel, so that everyone can benefit from the discussions. Please help each other, try to answer emerging questions and actively engage in the channel. Questions, that are not directly related to the class content, can be sent to me.\nFollow these steps to join the channel:\n\nGo on https://communicating.tuhh.de/\nClick Click here to sign in\nClick the Button GitLab\nYou may need to login to GitLab with your Kerberos/LDAP data (e.g. cba1020 and your password) on the following page and/or authorize once for Mattermost to access GitLab. You may also need to accept the terms.\nAfter accessing Mattermost, join the team W-11 students\nJoin Causal Data Science Channel (you might need to wait a bit, as I first have to add you)\n\nThere, and in the sessions, I will try to help you as much as possible.\nIn order to keep the discussion efficient and manageable it is necessary that we all follow some basic rules:\n\nPost error message: if you run into an error it is necessary that I know what the error is. Often reading the error message very carefully can also help you to understand where the problem comes from.\nPost the code that caused the error: in order to reproduce the error I need the last command that caused the error. If we need more context we will ask you for that.\nUse the formatting guidelines of Mattermost when you post code. That makes a huge difference in terms of readability. They will also be linked in the channel description. Most important is that using ``` one line above and one line below your code will make it easy to read.\nUse thread function to reply to a discussion. This way a discussion can be easier read. You find the reply button on the right side of a message.\n\nPlaying by these rules makes it a lot easier for everyone to follow the discussion and learn from similar problems and everyone can benefit from the discussions.\nSee in this minimalistic example how little formatting makes your code and error easy to read.\n\n \n```r\nx %&gt;% sum()\n```\n\n**Error:**\nError in x %&gt;% sum() : could not find function \"%&gt;%\"\n\n\n\n\nHow to format your code and error when you ask for help.\n\n\n\n\n\nHow to format your code and error when you ask for help.",
    "crumbs": [
      "Organization",
      "Mattermost"
    ]
  },
  {
    "objectID": "content/organization/intro_R.html",
    "href": "content/organization/intro_R.html",
    "title": "Introduction to R & RStudio IDE",
    "section": "",
    "text": "Before we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -&gt; Global options -&gt; Appearance and switch theme in -&gt; Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab.",
    "crumbs": [
      "Organization",
      "Introduction to R & RStudio IDE"
    ]
  },
  {
    "objectID": "content/organization/intro_R.html#installing-r-rstudio-ide",
    "href": "content/organization/intro_R.html#installing-r-rstudio-ide",
    "title": "Introduction to R & RStudio IDE",
    "section": "",
    "text": "Before we dive deep into the methods that help us to make critical data-driven business decisions, we start with a brief introduction to R, the programming language most suited to solve problems of causality. Don’t worry, if you have never heard of it! We’ll go through some very concise courses that will familiarize you with its functions very quickly. Essentially, you have to tell R what to do for you in a specific language. But step by step, first, we have to do the installation.\nR is only fun to use in combination with RStudio, a graphical integrated development environment (IDE) that makes the use of R more convenient and interactive. Please follow the steps as outlined in the instructions (note, that you have to install both R and RStudio):\nWhen you have successfully installed R and RStudio, open RStudio and you should see a screen similar to this one. By the way, if you want to change the default withe theme to something else, you can do that by going to Tools -&gt; Global options -&gt; Appearance and switch theme in -&gt; Editor theme.\n\nRStudio is split into four panes that have the following functions:\nSource Editor: here, you open, edit and execute programs/scripts that you have written. Code is not run immediately. If you want to run the current line of code, you just press Run or Ctrl+Enter/CMD+Return. You can also run several lines of code by highlighting them. Please note that every line starting with # will not be run. The use of # is to write comments and annotations in your code that won’t be executed.\nConsole: here, you can enter commands directly and run code. Just type in your code and press Enter.\nEnvironment: here, you can see what objects (dataframes, arrays, values, functions) you have in your workspace/environment. \nMiscellaneous: here, you have for example a file manager, an overview of installed and loaded packages, a plot viewer and a help tab.",
    "crumbs": [
      "Organization",
      "Introduction to R & RStudio IDE"
    ]
  },
  {
    "objectID": "content/organization/intro_R.html#introduction-to-r",
    "href": "content/organization/intro_R.html#introduction-to-r",
    "title": "Introduction to R & RStudio IDE",
    "section": "Introduction to R",
    "text": "Introduction to R\nBefore you begin coding, it’s important to remember this final point: do not let the errors, warnings, and other messages that you, like everyone else, are bound to encounter intimidate you. There is no reason to panic just because you see red text in your console and in fact, what is returned will often times already help you to solve the problem and lead you onto the right track.\nThere are three different types of texts that communicate issues or information about the code execution:\n\n\nErrors: this is a legitimate error and most likely your code did not run due to the error. Many of the error messages are very concise and you will directly see what was wrong, what is missing etc. If you do not see what you did wrong at first glance, you can copy the error message and google it. It is very likely someone else has run into the same error before.\n\n\n# Example error\n1 + \"a\"\n\nError in 1 + \"a\": nicht-numerisches Argument für binären Operator\n\n#Error in 1 + \"a\" : non-numeric argument to binary operator\n\n\n\nWarnings: opposed to an error, your code did probably run but there could be something off. However, it is just a warning. You can check it and if you think the warning does not apply to your specific scenario, you can go on.\n\n\n# Example warning\nas.numeric(c(\"18\", \"30\", \"50+\", \"345,678\"))\n\nWarning: NAs durch Umwandlung erzeugt\n\n\n[1] 18 30 NA NA\n\n# Warning message: NAs introduced by coercion \n\n\n\nMessages: these are just friendly texts that provide you with useful information. They do not need immediate attention but can provide useful supplementary information.\n\n\n# Example message\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nInteractive Tutorials:\nBut let’s no more talk about it but instead start coding because the best way to get familiar with R and to code is to just start.\nIn the following chapters, you will learn to code along the way, but to start you will go through some very concise tutorials from the R package swirl. The package provides a whole bunch of tutorials in the console.\nFeel free to complete as many tutorials as you want, but for this class, the following tutorial is of particular use: The R Programming Environment (Chapter 2-12)\nswirl() does not come with R by default but is an optional package. R packages are extensions of the base functionality implemented by default when you download R. Written by users around the world, packages provide additional features and are crucial for data science tasks in practice as you will later see.\nYou need to follow two steps to use an R package:\n\nOnce: install the package. As already mentioned, packages are not installed by default and you have to download it and add it to your library. Once you’ve installed it, you don’t have to repeat this step.\nAlways: load the package. By default, just the base R functionality is loaded and when you want to make use of the additional features provided by a specific package, you have to load it every time you start RStudio.\n\nSo let’s do it for the package swirl:\nFirst, we install the package. This has to be done only once. You can either choose to write your code into the source editor or directly into the console\n\ninstall.packages(\"swirl\")\n\nThen, we load the library into our current our R session.\n\nlibrary(swirl)\n\nNow, the package is loaded and we can start making use of it.\n\ninstall_course(\"The R Programming Environment\")\n\nYou just have to type swirl() into your console and follow the instructions! Please make sure to always use the same name. This way, you can leave the tutorial and start at the same position again later. It’s best to write it down so that you do not forget it.\n\nswirl()\n\nSelect the course you just installed: The R Programming Environment and start with Chapter 2. You should skip Chapter 1 because its irrelevant for you. If you accidentally selected Chapter 1, just quickly go through it and choose No at the last question.\nswirl will ask you to install packages for you that are needed for the tutorial. Please confirm when asked. If you computer is struggling with installing a package named “vctrs”, please type in the following command. If you don’t get such an error, you can ignore it.\n\ninstall.packages(\"vctrs\", repos = \"https://packagemanager.rstudio.com/cran/latest\")\n\nIf, at some point, you want to take a break, you can leave the swirl course by typing bye() or the Esc key. You can return to the course by typing swirl() and hitting Enter. And remember, to use the same name you used the first time.\nYou don’t need to submit anything from this step. Just focus on getting familiar with R by completing the tutorial (I recommend to solve chapter 2-12)!\n\n\n\n\n\n\nNote\n\n\n\nWhenever you want to find out more about a command or you have difficulties understanding what it does, you can click on it and a help page will show up.",
    "crumbs": [
      "Organization",
      "Introduction to R & RStudio IDE"
    ]
  },
  {
    "objectID": "content/organization/quarto.html",
    "href": "content/organization/quarto.html",
    "title": "Quarto [Optional Read]",
    "section": "",
    "text": "Quarto is a scientific publishing tool that allows R, Python, Julia and Observable JS users to create dynamic documents, websites, books and more. In fact, the whole course website is created using Quarto. In case you downloaded RStudio for this course, you do not need to install Quarto anymore. If you have an older RStudio version, you might have to download an install it or update to a new RStudio version. If you are familiar with Markdown or even RMarkdown you will see a lot of similarities.\nYou can explore Quarto’s documentation to learn more about creating documents, websites, blogs, books, slides, etc.\nEach page of your website is created by a q-Markdown file (.qmd). All website pages are plain text file that have the extension .qmd. Notice that the file contains three types of content:\nAn (optional) YAML header surrounded by - - - at the top (there is no need in the beginning to alter it)\nIn the code chunks, you can set different options with #|:\n\n#| eval: false prevents running the code and include its results\n#| include: false prevents code and results from appearing in the finished file. Quarto still runs the code in the chunk, and the results can be used by other chunks\n#| echo: false prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\n#| message: false prevents messages that are generated by code from appearing in the finished file.\n#| warning: false prevents warnings that are generated by code from appearing in the finished.\n#| fig-cap: “…” adds a caption to graphical results.\n\nSee the Quarto Cheat Sheet or the official quarto documentation for further information regarding the markdown syntax. It is necessary, that your code is formatted correctly to be evaluated.",
    "crumbs": [
      "Organization",
      "Quarto [Optional Read]"
    ]
  },
  {
    "objectID": "content/organization/quarto.html#quarto",
    "href": "content/organization/quarto.html#quarto",
    "title": "Quarto [Optional Read]",
    "section": "",
    "text": "Quarto is a scientific publishing tool that allows R, Python, Julia and Observable JS users to create dynamic documents, websites, books and more. In fact, the whole course website is created using Quarto. In case you downloaded RStudio for this course, you do not need to install Quarto anymore. If you have an older RStudio version, you might have to download an install it or update to a new RStudio version. If you are familiar with Markdown or even RMarkdown you will see a lot of similarities.\nYou can explore Quarto’s documentation to learn more about creating documents, websites, blogs, books, slides, etc.\nEach page of your website is created by a q-Markdown file (.qmd). All website pages are plain text file that have the extension .qmd. Notice that the file contains three types of content:\nAn (optional) YAML header surrounded by - - - at the top (there is no need in the beginning to alter it)\nIn the code chunks, you can set different options with #|:\n\n#| eval: false prevents running the code and include its results\n#| include: false prevents code and results from appearing in the finished file. Quarto still runs the code in the chunk, and the results can be used by other chunks\n#| echo: false prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\n#| message: false prevents messages that are generated by code from appearing in the finished file.\n#| warning: false prevents warnings that are generated by code from appearing in the finished.\n#| fig-cap: “…” adds a caption to graphical results.\n\nSee the Quarto Cheat Sheet or the official quarto documentation for further information regarding the markdown syntax. It is necessary, that your code is formatted correctly to be evaluated.",
    "crumbs": [
      "Organization",
      "Quarto [Optional Read]"
    ]
  },
  {
    "objectID": "content/optional_read/motivation.html",
    "href": "content/optional_read/motivation.html",
    "title": "Motivation",
    "section": "",
    "text": "In this course, you will learn about causality in data science with a particular emphasis on business applications. Causal data science methods are increasingly recognized and developed to understand causes and effects. Moving beyond a prediction-based approach in data science, the purpose of causal methods is to understand underlying processes and mechanisms to guide strategic decision-making. Causal methods allow us to answer questions that otherwise could not be addressed.\nA large global survey1 conducted among data science practitioners in the industry in 2020 states the importance of causal data science. 83% of the respondents consider causal inference in data-driven decisions making increasingly important and 44% state that, in their data science project, causal inference already plays an important role. Additionally,\nWhile the primary goal of machine learning is typically the development of algorithms for a high prediction and classification accuracy, causal inference aims to understand and establish cause-and-effect relationships between variables.\nTypical applications in business therefore aim to answer questions like:\nMany successful companies have already recognized the advantages of causal data science. Click on the link to get more details how these companies are using tools from causal inference to generate value within their organizations.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Motivation"
    ]
  },
  {
    "objectID": "content/optional_read/motivation.html#footnotes",
    "href": "content/optional_read/motivation.html#footnotes",
    "title": "Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.causalscience.org/blog/causal-data-science-in-practice/↩︎",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Motivation"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html",
    "href": "content/optional_read/prob.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Feel free to review some basic concepts of probability and statistics. All methods that used in this course are based on statistical models and these require probability theory.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#basic-rules-of-probability",
    "href": "content/optional_read/prob.html#basic-rules-of-probability",
    "title": "Probability Theory",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nConsider the most simple example: flipping coins. We define the outcome of the flip of a coin as a random variable as we are uncertain about what side the coin lands on. To express this uncertainty, we make us of probability theory.\nAfter flipping the coin, we will see what side the coin has landed on and our random variable has taken on of the two possible events \\(\\{H, T\\} \\subseteq \\Omega\\). It will be either Head or Tail.\nSo we have already defined two terms: random variable and events. Now what is a probability? A probability is always linked to an event typically denoted by a capital letter, here either \\(H\\) and \\(T\\), and expresses how likely this event is to happen. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\\[\nP(H) = P(T) = 0.5\n\\]\nExtreme cases: If an event \\(A\\) is impossible, its probability is \\(P(A) = 0\\) and if it is certain to occur, it is \\(P(A)=1\\).\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 1: Probability is a real number greater or equal to 0.\n\n\nWe can also introduce the compliment \\(\\overline{A}\\), which is what happens when \\(A\\) does not happen and consequently, \\(P(A) + P(\\overline{A}) = 1\\). \\(A\\) and \\(\\overline{A}\\) are mutually exclusive, by definition. But there could also be two events \\(A\\) and \\(B\\) that are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\\[\nP(H \\cup T) = P(H) + P(T) = 1\n\\]\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 2: Total probability is equal to 1.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAxiom 3: Probability of mutually exclusive events is the sum of the probabilities. (Mutually exclusive: events can’t happen at the same time)\n\n\nTo understand what not mutually exclusive events are, consider events \\(studying\\) and \\(working\\). For a random person, we don’t know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\nThen, the probability of at least one of the events happening is calculated by\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nwith \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the addition rule.\n\n\n\n\n\n\nTip\n\n\n\n\\(\\cup\\) : Union, can be translated as “or”.\n\\(\\cap\\) : Intersection, can be translated as “and”.\n\n\n\n\n\n\nFor mutually exclusive events:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B)\n\\]\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule,\n\\[\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n\\]\nwhere \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened. It is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\nUsing the example with workers and students: without knowing exact numbers, we can assume that students are less likely to work than individuals who are not studying.\n\\[\nP(working|studying) &lt; P(working|\\overline{studying})\n\\]\nEssentially, we are looking at probabilities restricted to a subset of the sample, which in this comparison are the subsamples of studying persons and non-studying persons.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways. Let’s think of rolling a die twice (first roll \\(R_1\\) and second roll \\(R_2\\)).\n\\[\nP(R_2 \\mid R_1) = P(R_2)\n\\]\nThe second roll does not depend on the first one. With each roll the outcomes \\({1, 2, 3, 4, 5, 6}\\) have the same probability likely independent of the previous roll. If we want to compute the probability of both rolls being a \\(6\\), we would just have to multiply the probabilities for each roll.\n\\[\nP(R_1 = 6 \\cap R_2 = 6) = P(R_1 = 6) \\ P(R_2 = 6)\n\\]",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#probability-tree",
    "href": "content/optional_read/prob.html#probability-tree",
    "title": "Probability Theory",
    "section": "Probability Tree",
    "text": "Probability Tree\nLet’s go back to the case where events are dependent on each other. An intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events happens. The probability of two consecutive events is obtained by multiplying the probabilities.\nConsider the following example: you are project manager and based on your are interested in the probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Using historical data about past projects, you come up with the following tree.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#assignment-i",
    "href": "content/optional_read/prob.html#assignment-i",
    "title": "Probability Theory",
    "section": "Assignment I",
    "text": "Assignment I\n\nDefine being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\). (Hint: Check here, if you are not sure what is shown in the probability tree.)\nThen, compute the following probabilities and the sum of all four probabilities.\n\n\\(P(T \\cap S)\\)\n\\(P(T \\cap \\overline{S})\\)\n\\(P(\\overline{T} \\cap S)\\)\n\\(P(\\overline{T} \\cap \\overline{S})\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nWith some browsers and specific operating systems, the compliment probability is not shown correctly (missing the horizontal bar above the letter). In that case it often helps to zoom in or out.\n\n\n\n\n\n\n\n\nOptional assignments!\n\n\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#set-theory",
    "href": "content/optional_read/prob.html#set-theory",
    "title": "Probability Theory",
    "section": "Set Theory",
    "text": "Set Theory\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams that are based on set theory. We already used a simple one above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\nLet’s use an example to understand some other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices: smartphones, tablets and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current usage distribution.\nInstead of using actual data, we simulate the data collection process here. If you are interested how to do it in R, you can expand and check out the code by clicking on Code. But you don’t have to. And don’t worry, if it looks too complicated at this point, just move on.\n\n\n\n\n\n\nNote\n\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R.\n\ntibble() is the most convenient way to create tablets. You specify column name and content and assign your tibble to an object to store it.\n\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\n\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\n\nmutate() is one of the most important functions for data manipulation in tablets. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %&gt;% mutate(new_variable = existing_var / 100).\n\n\n\nCode# Load tidyverse package\nlibrary(tidyverse)\n\n# Number of obervations\nn &lt;- 1000\n\n# Create tibble\napp_usage &lt;- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage &lt;- app_usage %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer. Please note that \\(1\\) indicates usage and \\(0\\) indicates no usage.\n\n\n\n\n\n\nNote\n\n\n\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\n\n\n# Show first ten lines\nhead(app_usage, 10)\n\n\n  \n\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n\n\n\n\n\nNote\n\n\n\nSumming all values by column is done by colSums(table). To sum rows, you can use rowSums(table).\n\n\n\n# Show column sums\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500        589        389        226 \n\n\nThe sum of \\(user\\_id\\) does not really tell us anything. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n\n\n\n\n\nNote\n\n\n\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\n\n\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %&gt;% select(smartphone, tablet, computer) %&gt;% colSums()\n\nsmartphone     tablet   computer \n       589        389        226 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n\nGeneric Venn diagram\n\n\n\n\n\n\n\nNote\n\n\n\n\nwhich() checks a condition and returns the indices.\n\n\n\n# Set of phone, tablet and computer users\nset_phon &lt;- which(app_usage$smartphone == 1)\nset_tabl &lt;- which(app_usage$tablet == 1)\nset_comp &lt;- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\nAssignment II\n\nUsing the Venn diagram above, answer the following questions.\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\n\n\n\n\n\n\n\nOptional assignments!\n\n\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments.\n\n\n\nYou can also use the example to go through the basic probability rules defined above (that does not belong to the assignment anymore).\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - P(T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/optional_read/prob.html#bayes-theorem",
    "href": "content/optional_read/prob.html#bayes-theorem",
    "title": "Probability Theory",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMath\nA very important theorem in probability theory is Bayes theorem. In fact, it has been called the most powerful rule of probability and statistics. Let’s quickly go through the math. By reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nand using the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem expresses a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before. With the Bayes theorem you can answer questions like:\n\nHow likely is it that it will rain, when there are clouds in the morning?\nHow likely is it that an email is spam if certain keywords appear?\n\n\n\n\n\n\n\nTip\n\n\n\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]\n\n\nApplication\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) \n\\(P(B|\\overline{A}) = 0.01\\) \n\\(P(A) = 0.04\\) \nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B)\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nAssignment III\n\nCompute\n\n\n\\(P(\\overline{A}|B)\\) (1)\n\n\\(P(A|B)\\) (2)\n\nand fill the gaps in the following sentence:\nThese results show that in case the alarm is triggered, there is a possibility of about __% that the product is flawless and a probability of __% that the product is faulty.\n\n\n\n\n\n\n\nOptional assignments!\n\n\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments.\n\n\n\n\n\nGeneric Venn diagram",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "<b>Optional reads</b>",
      "Probability Theory"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html",
    "href": "content/course_weeks/week_11/week_11.html",
    "title": "11 - Causal Mediation",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html#slides-recap",
    "href": "content/course_weeks/week_11/week_11.html#slides-recap",
    "title": "11 - Causal Mediation",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html#practical-example",
    "href": "content/course_weeks/week_11/week_11.html#practical-example",
    "title": "11 - Causal Mediation",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_11/week_11.html#assignment",
    "href": "content/course_weeks/week_11/week_11.html#assignment",
    "title": "11 - Causal Mediation",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "11 - Causal Mediation"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html",
    "href": "content/course_weeks/week_10/week_10.html",
    "title": "10 - Regression Discontinuity",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html#slides-recap",
    "href": "content/course_weeks/week_10/week_10.html#slides-recap",
    "title": "10 - Regression Discontinuity",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html#practical-example",
    "href": "content/course_weeks/week_10/week_10.html#practical-example",
    "title": "10 - Regression Discontinuity",
    "section": "Practical example",
    "text": "Practical example\nComing soon.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_10/week_10.html#assignment",
    "href": "content/course_weeks/week_10/week_10.html#assignment",
    "title": "10 - Regression Discontinuity",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n…",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "10 - Regression Discontinuity"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html",
    "href": "content/course_weeks/week_03/week_3.html",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "",
    "text": "Last week, our attention was dedicated to identification with the aid of graphical models. This week, our focus shifts to establishing the connection between causal identification and estimation. This section delves into randomized experiments, widely regarded as the gold standard for causal inference, and linear regression, recognized as the workhorse of causal inference. Randomization is the best way to make treatment and control group comparable, allowing us to go from mere association to causation. Linear regression equips us with the tools to extract statistical information necessary to assess the level of confidence level we have in our estimates.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#slides-recap",
    "href": "content/course_weeks/week_03/week_3.html#slides-recap",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "",
    "text": "Last week, our attention was dedicated to identification with the aid of graphical models. This week, our focus shifts to establishing the connection between causal identification and estimation. This section delves into randomized experiments, widely regarded as the gold standard for causal inference, and linear regression, recognized as the workhorse of causal inference. Randomization is the best way to make treatment and control group comparable, allowing us to go from mere association to causation. Linear regression equips us with the tools to extract statistical information necessary to assess the level of confidence level we have in our estimates.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#effect-of-introducing-a-new-feature",
    "href": "content/course_weeks/week_03/week_3.html#effect-of-introducing-a-new-feature",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "Effect of introducing a new feature",
    "text": "Effect of introducing a new feature\nLet’s jump right into an example of a randomized experiment to grasp the advantages over observational studies. Imagine you’re part of the business analytics team at a software company. Your task is to analyze the impact of introducing a new feature on the time users spend within the app. While the exact nature of this feature isn’t specified, its objective is to boost users’ daily app engagement, thereby driving up ad revenue.\nAs usual, we need to download and load the data first.\n\ndf &lt;- readRDS(\"new_feature.rds\")\nprint(df)\n\n# A tibble: 1,000 × 3\n   ip              new_feature time_spent\n   &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;\n 1 153.168.98.222            0      71.4 \n 2 119.218.12.199            1      76.1 \n 3 208.197.60.31             0      68.4 \n 4 235.143.38.46             1      73.1 \n 5 86.56.28.141              0      70.4 \n 6 51.111.112.107            0      45.1 \n 7 165.165.131.102           0     106.  \n 8 98.178.26.235             0      14.0 \n 9 63.223.57.208             0       9.22\n10 149.249.67.110            1      79.5 \n# ℹ 990 more rows\n\n\nThe data contains three columns, ip, new_feature and time_spent. Values in the ip column identify the user, new_feature indicates who has the new feature in the app and time_spent is the average daily minutes a user spent on the app following the start of the randomized experiment.\nIn the lecture, you learned that when treatment is randomly assigned, potential outcomes are independent of the treatment, and any systematic difference observed in the outcomes can be attributed solely to the treatment. Consequently, we can simply compare the means of the respective groups to estimate the ATE.\n\\[\n\\hat{\\tau}_{\\text{ATE}} = \\frac{1}{\\sum_i D_i} \\sum_i D_i Y_i - \\frac{1}{\\sum_i (1 - D_i)} \\sum_i (1 - D_i) Y_i\n\\] First, we will go through the manual calculation of all we need to estimate the average treatment effect and asses the statistical significance. Afterwards, we will use linear regression, which saves us a lot of work and will be incredibly important in th weeks to come.\nManual calculation\nUsing the tidyverse allows us a versatile and flexible approach that we can extend later. We group by the treatment column and compute the group-specific means.\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Grouping by the treatment column and calculating the mean of the outcome\n# column by group\ngroup_means &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(mean_time_spent = mean(time_spent)) |&gt; \n  ungroup()\n\n# Show group-specific means\nprint(group_means)\n\n# A tibble: 2 × 2\n  new_feature mean_time_spent\n        &lt;dbl&gt;           &lt;dbl&gt;\n1           0            61.0\n2           1            63.5\n\n\nTo get the difference, we need to add one more step.\n\n# Compute the difference in means\ndiff(group_means$mean_time_spent)\n\n[1] 2.5\n\n\nUp to this point, this should sound familiar. However, although we have an estimate for the ATE, we lack certainty regarding the confidence in this estimate. We also cannot ascertain whether the difference between the mean outcomes of the treatment and control groups is solely due to chance or holds statistical significance.\nStandard deviation and standard error of the mean\nIn addition to the average, another crucial statistical measure, upon which all subsequent methods of statistical inference rely, is the variance. This measure indicates the extent of dispersion/spread in a random variable. Variance of a random variable \\(X\\) calculated by:\n\\[\n\\sigma_X^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left(X_i - \\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2\n\\] The standard deviation obtains as the square root of the variance. Unlike the variance it expressed in the same unit as the data.\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\] Let’s add the standard deviation to our summary table. We can use the built-in function sd(). If we would like to compute the variance, we could use var().\n\n# Grouping by the treatment column and calculating the mean and the standard\n# deviation of the outcome\n# column by group\ngroup_stats &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(\n    mean_time_spent = mean(time_spent),\n    # add standard deviation\n    sd_time_spent = sd(time_spent)\n    ) |&gt; \n  ungroup()\n\n# Show group-specific statistics\nprint(group_stats)\n\n# A tibble: 2 × 3\n  new_feature mean_time_spent sd_time_spent\n        &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1           0            61.0          30.5\n2           1            63.5          28.0\n\n\nKnowing the group-specific means and standard deviations brings us one step closer to answer the question whether the treatment effect is large enough to be considered statistically significant. For each group, we also need to know the standard error of the mean. It can be easily confused with the standard deviation, but there is one large difference: the formula for the standard error takes the sample size into account. A larger sample gives us more confidence in the estimate.\n\n\n\n\n\n\nTip\n\n\n\nStandard deviation measures the dispersion of individual data points from the mean, while standard error of the mean quantifies the precision of the sample mean estimate, reflecting how much the sample mean is likely to vary from the population mean.\n\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt n}\n\\] Because there is no built-in function in R, we can easily define a function (we could use external packages, but for the sake of demonstration, we define it ourselves):\n\n\n\n\n\n\nNote\n\n\n\nIn many cases, you will find functions needed for your analysis in base R or by loading external packages. But sometimes, you want to do something so specific, that no one has done it before. Then, you define your custom function by using function(arg1, arg2, ...).\n\n\n\n# Custom function for the standard error\nse &lt;- function(x) {\n  # Standard deviation divided by the square root of the sample size\n  sd(x) / sqrt(length(x))\n}\n\nWe need to update our table with the statistics by adding the number of observations \\(n\\) per group (using the function n()) and calculating the standard error of the mean.\n\n# Grouping by the treatment column and calculating the mean, standard deviation,\n# sample size, and standard error of the mean\n# column by group\ngroup_stats &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(\n    mean_time_spent = mean(time_spent),\n    # add standard deviation\n    sd_time_spent = sd(time_spent),\n    # add sample size\n    n_obs = n()\n    ) |&gt; \n  ungroup() |&gt; \n  # Add a new column with the standard error of the mean\n  mutate(se_time_spent = sd_time_spent / sqrt(n_obs))\n\n# Show group-specific statistics\nprint(group_stats)\n\n# A tibble: 2 × 5\n  new_feature mean_time_spent sd_time_spent n_obs se_time_spent\n        &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;\n1           0            61.0          30.5   590          1.26\n2           1            63.5          28.0   410          1.38\n\n\nConfidence interval\nA profoundly useful mathematical theorem is the Central Limit Theorem, which states that the distribution of sample means tends towards normality. Understanding the mean and standard error of the mean equips us with all the necessary information to comprehend and illustrate the distribution.\nHere, we plot the standard normal distribution, characterized by a mean of zero and a standard deviation of one. By delineating the mass of the distribution limited by specific values, we can construct confidence intervals around our estimates. These intervals provide a range within which the true mean likely lies, with a chosen probability. While 95% confidence intervals are most commonly reported, values such as 90% or 99% are also frequently used For instance, approximately 95% of the area under the normal distribution falls within approximately two standard deviations below and above the mean.\n\nCodeggplot(data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(-3, 3)) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = ggthemr::swatch()[2],\n                alpha = .3,\n                xlim = c(qnorm(.005), qnorm(.995))) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = ggthemr::swatch()[2],\n                alpha = .5,\n                xlim = c(qnorm(.025), qnorm(.975))) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = ggthemr::swatch()[2],\n                alpha = .9,\n                xlim = c(qnorm(.05), qnorm(.95))) +\n  scale_x_continuous(limits = c(-3, 3), breaks = seq(-3, 3, .5)) +\n  geom_vline(xintercept = qnorm(.005), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.995), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.025), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.975), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.05), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  geom_vline(xintercept = qnorm(.95), linetype = \"dashed\", color = ggthemr::swatch()[1]) +\n  theme(panel.grid = element_blank()) +\n  ggtitle(\"Standard Normal Distribution with 90%, 95% and \\n99% Confidence Interval\") + \n  labs(x = \"z-score\", y = \"Density\")\n\n\n\n\n\n\n\nWe can use this directly to compute the confidence intervals around our estimated group means. For the 95% confidence interval, we have \\(alpha = 1 - 0.95 = 0.05\\) and can get z-value using using qnorm(.975)\n\\[\n\\hat{\\mu} \\pm z_{\\alpha/2} * SE\n\\]\n\n\n\n\n\n\nTip\n\n\n\nConfidence level and significance level are sometimes confused.\n\nconfidence level = 1- significance level = 95%\nsignificance level (or alpha) = 1 - confidence level = 5%\n\n\n\nAdding the confidence intervals to our table:\n\n# Grouping by the treatment column and calculating the mean, standard deviation,\n# sample size, and standard error of the mean\n# column by group\ngroup_stats &lt;- df |&gt;\n  group_by(new_feature) |&gt; \n  summarise(\n    mean_time_spent = mean(time_spent),\n    # add standard deviation\n    sd_time_spent = sd(time_spent),\n    # add sample size\n    n_obs = n()\n    ) |&gt; \n  ungroup() |&gt; \n  # Add a new column with the standard error of the mean\n  mutate(\n    se_time_spent = sd_time_spent / sqrt(n_obs),\n    # adding confidence intervals (lower and upper bound)\n    ci_lb = mean_time_spent - qnorm(.975) * se_time_spent,\n    ci_ub = mean_time_spent + qnorm(.975) * se_time_spent\n    )\n\n# Show group-specific statistics\nprint(group_stats)\n\n# A tibble: 2 × 7\n  new_feature mean_time_spent sd_time_spent n_obs se_time_spent ci_lb ci_ub\n        &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1           0            61.0          30.5   590          1.26  58.6  63.5\n2           1            63.5          28.0   410          1.38  60.8  66.2\n\n\nComparing the confidence intervals across the groups, we see some overlap. However, that does not yet mean that they are not statistically significant, although it is usually a good indication. And, in fact, if they did not overlap at all, you would be able to conclude that they are statistcally different.\n\nCodeu0 &lt;- group_stats[group_stats$new_feature == 0, ]$mean_time_spent\nu1 &lt;- group_stats[group_stats$new_feature == 1, ]$mean_time_spent\n\no0 &lt;- group_stats[group_stats$new_feature == 0, ]$se_time_spent\no1 &lt;- group_stats[group_stats$new_feature == 1, ]$se_time_spent\n\nci_lb0 &lt;- group_stats[group_stats$new_feature == 0, ]$ci_lb\nci_lb1 &lt;- group_stats[group_stats$new_feature == 1, ]$ci_lb\n\nci_ub0 &lt;- group_stats[group_stats$new_feature == 0, ]$ci_ub\nci_ub1 &lt;- group_stats[group_stats$new_feature == 1, ]$ci_ub\n\n\nggplot(data.frame(x = c(55, 70)), aes(x)) +\n  stat_function(fun = dnorm,\n                color = ggthemr::swatch()[3],\n                alpha = .5,\n                args = list(mean = u0, sd = o0),\n                geom = \"line\") +\n    stat_function(fun = dnorm,\n                color = ggthemr::swatch()[2],\n                alpha = .5,\n                args = list(mean = u1, sd = o1),\n                geom = \"line\") +\n  stat_function(fun = dnorm,\n                fill = ggthemr::swatch()[3],\n                alpha = .5,\n                args = list(mean = u0, sd = o0),\n                geom = \"area\",\n                xlim = c(ci_lb0, ci_ub0)) +\n  stat_function(fun = dnorm,\n                fill = ggthemr::swatch()[2],\n                alpha = .5,\n                args = list(mean = u1, sd = o1),\n                geom = \"area\",\n                xlim = c(ci_lb1, ci_ub1)) +\n  ggtitle(\"Distribution of outcomes by group\") + \n  labs(x = \"Minutes\", y = \"Density\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\nHypothesis testing\nTo comprehensively address the question, “Is the difference statistically significant from zero (or any other value)?” we rely on hypothesis testing. Leveraging the mathematical principle that the sum or difference of two independent normal distributions also follows a normal distribution, we can check whether the confidence interval of the ATE contains zero. This occurs because the ATE is derived as the difference between the distributions of the two groups.\nLet’s explicitly formulate what we test:\n\\(H_0: \\mu_1 - \\mu_0 = 0\\)\nExpressed in formulas, the mean of the resulting normal distribution is\n\\[\n\\mu_{diff} = \\mu_1 - \\mu_0\n\\]\nand the standard deviation is (please note, that it is the sum and not the difference)\n\\[\nSE_{diff} = \\sqrt{SE_1^2 + SE_0^2}.\n\\]\n\n# Get means\nmu_0 &lt;- group_stats[group_stats$new_feature==0, ]$mean_time_spent\nmu_1 &lt;- group_stats[group_stats$new_feature==1, ]$mean_time_spent\n\n# Get standard errors\nse_0 &lt;- group_stats[group_stats$new_feature==0, ]$se_time_spent\nse_1 &lt;- group_stats[group_stats$new_feature==1, ]$se_time_spent\n\n\n# Mean\ndiff_mu &lt;- mu_1 - mu_0\ndiff_mu\n\n[1] 2.5\n\n\n\n# Standard error\ndiff_se &lt;- sqrt(se_1^2 + se_0^2)\ndiff_se\n\n[1] 1.9\n\n\nLet’s make another plot: the distribution of the difference. We can already do the visual hypothesis test.\n\nCodeggplot(data.frame(x = c(-10, 10)), aes(x)) +\n  stat_function(fun = dnorm,\n                alpha = .5,\n                args = list(mean = diff_mu, sd = diff_se),\n                geom = \"line\") +\n  stat_function(fun = dnorm,\n                alpha = .5,\n                args = list(mean = diff_mu, sd = diff_se),\n                geom = \"area\",\n                xlim = c(diff_mu - qnorm(.975) * diff_se, diff_mu + qnorm(.975) * diff_se)) +\n  geom_vline(xintercept = 0, color = \"white\") +\n  ggtitle(\"Visual hypothesis testing with 95% CI\") + \n  labs(x = \"Minutes\", y = \"Density\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\nWhat we can infer from the plot already, can be confirmed by computing the confidence intervals with the known formula.\n\ndiff_ci_lb = diff_mu - qnorm(.975) * diff_se\ndiff_ci_ub = diff_mu + qnorm(.975) * diff_se\n\nprint(c(\"Lower CI\" = diff_ci_lb, \"Upper CI\" = diff_ci_ub))\n\nLower CI Upper CI \n    -1.2      6.2 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNull hypothesis: the null hypothesis \\(H_0\\) is the claim the the effect being studied does not exists. The effect is a “null” effect.\n\n\nQuestion: Do we reject the null hypothesis?\n\nAnswer\nNo. The ATE is not different from zero.\n[Optional read] p-value\nFrom a slightly different perspective, we can also compute the p-value to tell us the confidence in our estimates. The question we ask is then: What is the lowest significance level that results in rejecting the null hypothesis?\nFirst, we need to compute the t-statistic:\n\\[\n\\begin{align}\nt_{ATE} = \\frac{\\mu_1 - \\mu_0}{\\sqrt{SE_1^2 + SE_0^2}} &= \\frac{\\mu_{diff}}{SE_{diff}}\n\\end{align}\n\\]\n\nt_stat = (diff_mu - 0) / diff_se\n\nKnowing the t-statistic, we provide it in to the following command.\n\n(1 - pnorm(t_stat)) * 2\n\n[1] 0.18\n\n\npnorm(q) returns the integral from \\(-\\infty\\) to \\(q\\), i.e. the mass up to the value \\(q\\). Because we are using the two-sided test, we need to multiply by 2.\nThe p-value expresses the probability with which the null hypothesis is correct. For a rigorous statistical analysis, you usually expect a value of lower than 5% (sometimes 10% or 1%). Please note that this is just a different perspective and the common levels for p-values directly derive from the common values for confidence intervals.\nLinear regression\nAll the steps required to assess the hypothesis regarding the statistical significance of the average treatment effect can be condensed into a single step: running a linear regression. Regression functions as the workhorse for causal inference, serving as the cornerstone for more advanced techniques. Through the simple command lm(), regression seamlessly provides all the necessary statistical output for our analysis. Furthermore, it allows the implementation of more complex analyses beyond single-value treatments with ease.\nThe generic functional form of a singe-valued regression without any covariate is as follows. \\(Y\\) is the outcome variable, \\(D\\) is treatment variable, and \\(e\\) is the error term. We are interested in estimating the coefficients denoted by \\(\\beta\\), particularly \\(\\beta_1\\), which is the treatment effect.\n\\[\nY_i = \\beta_0 + \\beta_1*D+e_i\n\\]\nWe’ve already encountered the syntax before, but if you do not recall: y ~ x, where \\(y\\) is the outcome and \\(x\\) the explaining variable. We specify the respective variables in our data: we want to explain the effect of new_feature on time_spent, fit the model using lm() and assign it to a variable. Then, for a good summary of the estimates and statistical measures, we apply the summary() function to the fitted object.\n\nlr &lt;- lm(time_spent ~ new_feature, data = df)\nsummary(lr)\n\n\nCall:\nlm(formula = time_spent ~ new_feature, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-63.53 -19.83   1.11  19.08  96.01 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    61.04       1.21   50.30   &lt;2e-16 ***\nnew_feature     2.49       1.90    1.32     0.19    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30 on 998 degrees of freedom\nMultiple R-squared:  0.00173,   Adjusted R-squared:  0.000732 \nF-statistic: 1.73 on 1 and 998 DF,  p-value: 0.188\n\n\nTake a look at the output and try to find the statistical measures we have manually calculated before.\n\nGroup means\nTreatment effect\nStandard error of the estimates\nConfidence interval\nHypothesis testing\n\nThe output contains a p-value but does not contain confidence intervals because it is agnostic to the confidence level. However, if you are interested in confidence intervals for a specific confidence level, you can use the confint(model, level) function to calculate them. Or just simply calculate them by hand as you have learned.\n\nconfint(lr, level = .95)\n\n            2.5 % 97.5 %\n(Intercept)  58.7   63.4\nnew_feature  -1.2    6.2\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlthough the average treatment effect is equal, a minor difference in the statistics may arise. This difference stems from approximating the t-distribution with the normal distribution, a common practice when dealing with more than 100 observations. The benefit of utilizing the normal distribution is not needing to specify the degrees of freedom (sample size minus estimated parameters).",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#ab-test-at-streaming-service",
    "href": "content/course_weeks/week_03/week_3.html#ab-test-at-streaming-service",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "A/B test at streaming service",
    "text": "A/B test at streaming service\nConsider the following scenario: you are a data scientist at a music streaming company. Naturally, you’re keen to engage users for as long as possible and encourage them to become avid users of your streaming app. Your colleagues from the department responsible for recommendation algorithms have developed a new fancy algorithm which aims to enhance the daily number of minutes of music listening by offering particularly well tailored recommendations. Following common practice of A/B testing, you randomly assign your users into a treatment group (new algorithm) and a control group (no new algorithm). Download the data here.\n\nAs a pre-check, test whether your random assignment worked well by comparing the daily number of minutes users listened to music per group.\nCompute the average treatment effect using a linear regression.\n\nTake a look at the summary statistics of the regression and answer the following questions:\n\nWhat is the estimated treatment effect?\nWhat is the null hypothesis and do you reject or do you confirm it? At what significance level?\nWhat doest \\(R^2\\) describe?\nWhat is the intercept and what does it describe?\n\n\nCalculate 90%, 95% and 99% confidence intervals for the treatment effect and compare them. Which intervals are larger? And what question do they answer?\n\nAfter one month, you are asked to take a different outcome into focus - the rate of subscription cancellations. For each user you find the information whether he/she cancelled his/her subscription in the column cancel: 1 if cancelled, and 0 if not. Because other than minutes, cancel is a binary variable (it can only be 0 or 1), you need to apply a different form of regression: the logistic regression. There is lot of theory behind it, which we will not cover now. But feel free to read here about it.\n\nHowever, what you need to understand when using logistic regression is that the interpretation of the coefficients changes because you are acutally fitting\n\\[\nlogit(P(Y=1)) = logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*D + \\epsilon,\n\\]\nwhich is different from the linear regression. We know relate a one-unit increase in \\(D\\) (treated or not-treated) with an increase in \\(logit(p)\\). Therefore, you can read the coefficient as follows:\nIncreasing \\(D\\) by 1 will result in a \\(\\beta_1\\) increase in \\(logit(p)\\), which is approximately an \\((exp(\\beta_1)-1)\\) % increase in the odds \\(\\frac{p}{1-p}\\).\nInstead of lm() use the following command: glm(formula, family=binomial(link='logit'), data), where you replace formula with the known syntax.\n\nFit the model and return the summary statistics.\nInterpret the coefficients.\n\nFind here the solutions.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_03/week_3.html#assignment",
    "href": "content/course_weeks/week_03/week_3.html#assignment",
    "title": "3 - Randomized Experiments & Linear Regression",
    "section": "Assignment",
    "text": "Assignment\n\nAccept the Week 3 - Assignment and follow the same steps as last week and as described in the organization chapter.\n\nImagine that there are cost associated with providing each user with this new algorithm. Your business colleague tells you, that from a business perspective, the introduction is only beneficial if users listen at least 5% more than before. Develop a hypothesis based on this information and test it.\nOne advantage of A/B testing in software companies is the ability to monitor the experiment simultaneously. For the first three days after having started the experiment you check the numbers and compute the ATE. Simulate this by subsetting the data to include observations available after the first day, after the second day and after the third day. Do you notice any differences? If yes, what could be a possible explanation? Run the analysis for both outcomes, minutes and cancel. (Hint: you need to use the variable log_in_day which tells you at what day the users first logged in after the start of the experiment. Before that, we could not observe their behavior.)\nCompute conditional ATEs based on the day the users first logged into the application. (Please note, this is different to 2.) There are two different ways to compute the CATEs: you could subset the data and run the analysis for each condition or you could add a so-called interaction effect to your regression syntax by y ~ x + x:interaction, where interaction is the variable you to affect the treatment effect. Run the analysis for both outcomes, minutes and cancel.\nYour colleagues responsible for the development of the recommendation algorithm tell you that they ran an experiment prior to your experiment with a small number of Beta users. These users signed up to test a Beta version of your app where new features are tested before being released. Because your colleague received different results than you, he prefers to stick to his/her own estimates. Convince your colleague that your experiment returns more valid estimates. Use a DAG to do so.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "3 - Randomized Experiments & Linear Regression"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html",
    "href": "content/course_weeks/week_05/week_5.html",
    "title": "5 - Double Machine Learning",
    "section": "",
    "text": "Last week, we delved into strategies for managing observed confounders, exploring techniques such as estimating conditional outcomes and re-weighting data based on propensity scores. Building on that foundation, we’ll now explore doubly robust methods. These approaches combine modeling of both the conditional outcome and the likelihood of treatment, offering the advantage of consistency even if only one model is accurately specified. Additionally, we’ll introduce machine learning (ML) models into our toolkit. ML enables us to perform data-driven model selection and tackle complex non-linear relationships while still allowing for statistical inference. Primarily, ML methods help us model nuisance parameters — factors that aren’t our main focus but assist in estimating our target parameter: the treatment’s effect on an outcome.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#slides-recap",
    "href": "content/course_weeks/week_05/week_5.html#slides-recap",
    "title": "5 - Double Machine Learning",
    "section": "",
    "text": "Last week, we delved into strategies for managing observed confounders, exploring techniques such as estimating conditional outcomes and re-weighting data based on propensity scores. Building on that foundation, we’ll now explore doubly robust methods. These approaches combine modeling of both the conditional outcome and the likelihood of treatment, offering the advantage of consistency even if only one model is accurately specified. Additionally, we’ll introduce machine learning (ML) models into our toolkit. ML enables us to perform data-driven model selection and tackle complex non-linear relationships while still allowing for statistical inference. Primarily, ML methods help us model nuisance parameters — factors that aren’t our main focus but assist in estimating our target parameter: the treatment’s effect on an outcome.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#practical-example",
    "href": "content/course_weeks/week_05/week_5.html#practical-example",
    "title": "5 - Double Machine Learning",
    "section": "Practical example",
    "text": "Practical example\nAs a member of the marketing team at an online retailer, your objective is to identify customers who are receptive to marketing emails. While recognizing the potential for increased spending associated with these emails, you’re also aware that some customers prefer not to receive them. To address this challenge, your aim is to estimate the average treatment effect (ATE) of sending such emails on customers’ future purchase volume. This estimation will enable your team to make informed decisions about email recipients.\n\n\n\n\n\n\nTip\n\n\n\nToday, you will download a R script and the data to answer the questions throughout the tutorial.\n\n\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Read data\nemail &lt;- read_csv(\"email_obs.csv\")\n\n# Data overview\nglimpse(email)\n\nRows: 10,000\nColumns: 27\n$ mkt_email          &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0…\n$ next_mnth_pv       &lt;dbl&gt; 1764.62, 162.21, 42.33, 12.41, 1398.49, 1088.87, 1485.70, 812.83, 1091.01, 2081.43, 984.25, 1755.68, 1.75, 1525.07, 739.7…\n$ age                &lt;dbl&gt; 34, 65, 26, 31, 36, 68, 27, 23, 46, 27, 35, 37, 30, 31, 24, 29, 23, 49, 46, 28, 49, 47, 34, 41, 42, 24, 28, 34, 55, 27, 3…\n$ tenure             &lt;dbl&gt; 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0…\n$ ammount_spent      &lt;dbl&gt; 230.31, 142.41, 103.19, 10.25, 30.34, 78.16, 178.64, 37.14, 182.87, 136.78, 4.75, 28.81, 0.68, 92.45, 40.65, 192.17, 186.…\n$ vehicle            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ food               &lt;dbl&gt; 2, 2, 0, 0, 1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 0, 2, 1, 1, 3, 1, 4, 1, 0, 1, 2, 0, 0, 1, 0, 2, 1, 1, 1, 0, 2, 1, 1…\n$ beverage           &lt;dbl&gt; 2, 3, 0, 1, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 3, 0, 0, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 2, 2, 2, 1, 0, 2, 1, 0, 3, 1, 0, 0, 0…\n$ art                &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 2, 0, 2, 3, 0, 0, 3, 0, 1, 3, 0, 0, 4, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 1, 1, 2, 0…\n$ baby               &lt;dbl&gt; 0, 3, 2, 1, 1, 1, 1, 2, 1, 0, 1, 0, 3, 0, 0, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 4, 1, 3, 1, 3, 2, 1, 3, 1, 0, 0, 2, 2, 3, 1, 2…\n$ personal_care      &lt;dbl&gt; 2, 0, 0, 1, 0, 0, 2, 0, 2, 1, 3, 1, 2, 2, 1, 2, 3, 2, 1, 0, 0, 1, 1, 2, 0, 0, 1, 2, 0, 0, 0, 4, 1, 0, 1, 2, 2, 1, 0, 0, 2…\n$ toys               &lt;dbl&gt; 0, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 2, 3, 0, 1, 2, 0, 3, 1, 1, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 1…\n$ clothing           &lt;dbl&gt; 1, 3, 4, 1, 1, 4, 2, 1, 1, 4, 2, 2, 2, 3, 1, 0, 1, 0, 2, 2, 1, 0, 0, 1, 1, 3, 0, 3, 3, 1, 0, 6, 3, 0, 3, 8, 7, 3, 2, 1, 1…\n$ decor              &lt;dbl&gt; 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 2, 5, 1, 2, 0, 1, 0, 2, 0, 1, 3, 0, 2, 0, 0, 0, 0, 1, 0, 3, 0, 1, 1, 0, 3, 0, 0, 0, 0, 0, 2…\n$ cell_phones        &lt;dbl&gt; 4, 1, 2, 3, 3, 1, 5, 1, 3, 4, 0, 3, 1, 3, 2, 0, 1, 1, 4, 3, 1, 2, 1, 4, 2, 6, 3, 5, 3, 1, 2, 1, 5, 4, 6, 3, 5, 2, 1, 3, 4…\n$ construction       &lt;dbl&gt; 3, 1, 0, 1, 2, 1, 0, 1, 1, 0, 2, 2, 1, 1, 2, 2, 2, 0, 2, 1, 2, 0, 1, 0, 2, 2, 0, 2, 1, 0, 1, 1, 1, 2, 1, 0, 3, 0, 2, 0, 0…\n$ home_appliances    &lt;dbl&gt; 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 3, 1, 0, 1, 0, 0, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 1, 1, 2, 2, 3, 2, 0, 1, 1, 2, 0, 1…\n$ electronics        &lt;dbl&gt; 2, 4, 2, 1, 4, 0, 0, 3, 1, 5, 0, 5, 0, 4, 2, 2, 2, 3, 1, 3, 1, 1, 3, 1, 1, 2, 4, 2, 3, 3, 2, 2, 1, 1, 0, 1, 0, 1, 1, 3, 3…\n$ sports             &lt;dbl&gt; 0, 3, 1, 0, 0, 1, 0, 2, 2, 1, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 3, 0, 2, 0, 2, 0, 1, 1, 2…\n$ tools              &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 3, 0, 1, 0, 3, 0, 4, 1, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0…\n$ games              &lt;dbl&gt; 2, 3, 2, 0, 1, 2, 2, 0, 2, 1, 2, 1, 2, 3, 2, 1, 2, 1, 5, 2, 1, 2, 1, 0, 3, 4, 3, 0, 2, 1, 1, 0, 1, 3, 1, 0, 1, 2, 1, 0, 4…\n$ industry           &lt;dbl&gt; 0, 1, 2, 0, 3, 1, 4, 1, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 1, 1, 3, 2, 1, 2, 2…\n$ pc                 &lt;dbl&gt; 2, 3, 5, 2, 0, 3, 3, 2, 2, 3, 3, 4, 3, 2, 0, 1, 0, 2, 0, 4, 3, 2, 0, 3, 2, 2, 3, 3, 1, 0, 1, 1, 2, 0, 2, 2, 3, 3, 1, 0, 3…\n$ jewel              &lt;dbl&gt; 0, 0, 1, 3, 2, 1, 0, 0, 0, 2, 0, 3, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 0, 0, 2, 1, 3, 1, 3, 3, 1, 1, 1, 0, 0, 1, 0, 2, 1…\n$ books              &lt;dbl&gt; 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 0, 3, 1, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2, 1, 1, 1, 1, 3, 0, 0, 2, 2…\n$ music_books_movies &lt;dbl&gt; 3, 2, 1, 2, 0, 3, 1, 0, 1, 0, 1, 0, 3, 2, 1, 1, 0, 1, 3, 0, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2…\n$ health             &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 1, 0, 2, 4, 0, 2, 5, 2, 0, 2, 1, 2, 2, 4, 3, 3, 1, 2, 2, 2, 2, 3, 1, 1, 1, 1, 2, 2, 1, 3, 3, 1, 0, 0, 3…\n\n\nThe treatment variable of interest is denoted as mkt_email, indicating whether a customer received a marketing email. The outcome of importance is the purchase volume one month after receiving the email, represented as next_mnth_pv. Alongside these key variables, the dataset encompasses various covariates such as customer age, tenure (time elapsed since the first purchase on the website), and purchase history across different product categories.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#frischwaughlovell-theorem",
    "href": "content/course_weeks/week_05/week_5.html#frischwaughlovell-theorem",
    "title": "5 - Double Machine Learning",
    "section": "Frisch–Waugh–Lovell Theorem",
    "text": "Frisch–Waugh–Lovell Theorem\nPlease recall from last week that we are able to split the estimation into three steps: estimating (1) a model for exposure, (2) a model for the outcome, and a (3) residual-on-residual regression. We obtain an estimate that is numerically equivalent to what we would have obtained in a linear regression modeling the conditional outcome.\nQuestion: What is a residual?\n\nAnswer\nDifference between the observed value and the value predicted by the fitted model.\nQ1. Estimate the treatment effect using the FWL theorem.\n\nCode# Frisch–Waugh–Lovell Theorem: 3-step procedure\n\n# (1) Debiasing:\nmod_D &lt;- lm(mkt_email ~ ., data = select(email, -next_mnth_pv))\nD_hat &lt;- mod_D$residuals\n\n# (2) Denoising:\nmod_Y &lt;- lm(next_mnth_pv ~ ., select(email, -mkt_email))\nY_hat &lt;- mod_Y$residuals\n\n# (3) Residual regression\nmod_fwl &lt;- lm(Y_hat ~ 0 + D_hat)\nsummary(mod_fwl)\n\n\nCall:\nlm(formula = Y_hat ~ 0 + D_hat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-21000   -128     -4    121 115926 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nD_hat   1313.6       37.8    34.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1880 on 9999 degrees of freedom\nMultiple R-squared:  0.108, Adjusted R-squared:  0.108 \nF-statistic: 1.21e+03 on 1 and 9999 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#double-machine-learning-with-partially-linear-regression",
    "href": "content/course_weeks/week_05/week_5.html#double-machine-learning-with-partially-linear-regression",
    "title": "5 - Double Machine Learning",
    "section": "Double machine learning with partially linear regression",
    "text": "Double machine learning with partially linear regression\nTo capture e.g. interactions and non-linearities better, the choice of flexible ML estimators instead of linear regression can be beneficial. In R, the package mlr3, alongside with the extension package mlr3learners provides a wide range of common ML estimators and a flexible framework.\n\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\n\nJust as the FWL theorem, Chernozhukov et al. (2018) proposes a three step procedure to estimate the \\(ATE\\):\n\nForm prediction model for the treatment: \\(\\hat{e}(\\mathbf{X_i})\\)\nForm prediction model for the outcome: \\(\\hat{\\mu}(\\mathbf{X_i})\\)\nRun feasible residual-on-residual regression: \\(\\hat{\\tau}_{\\text{ATE}} = \\arg \\min_{\\tilde{\\tau}} \\frac{1}{N}\\sum_{i=1}^n ( Y_i - \\hat{\\mu}(\\mathbf{X_i}) - \\tilde{\\tau} ( T_i - \\hat{e}(\\mathbf{X_i})))^2 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\mu}(\\mathbf{X_i})) (T_i - \\hat{e}(\\mathbf{X_i}))}{\\sum_{i=1}^n (T_i - \\hat{e}(\\mathbf{X_i}))^2}\\)\n\nPlease note, no particular estimation method is specified in the procedure for step (1) and (2). Only for step (3), an OLS regression will be run on the residuals. Therefore, we load the packages and define what estimators, we want to use.\n\n\n\n\n\n\nNote\n\n\n\nWith as_task_classif() or as_task_reg(), we define what we want to estimate which includes the data object and the estimation target, also known as response or outcome. When the outcome is binary, we use a classification task, when it is continuous, we use a regression task. With lrn(), we specify how we want to estimate it.\n\n\n\n# Load packages\nlibrary(mlr3)\nlibrary(mlr3learners)\n\n# Prediction model for the treatment/exposure\ntask_e &lt;- as_task_classif(email |&gt; select(-next_mnth_pv), target = \"mkt_email\")\nlrnr_e &lt;- lrn(\"classif.log_reg\", predict_type = \"prob\")\n\n# Prediction model for the outcome\ntask_m &lt;- as_task_regr(email |&gt; select(-mkt_email), target = \"next_mnth_pv\")\nlrnr_m &lt;- lrn(\"regr.lm\")\n\nUsing these estimators, we will populate the vectors e_hat and m_hat, which are estimates of the treatment propensity and the conditional outcome for each unit, respectively (step 1 and step 2).\n\n# Initialize nuisance vectors\nn &lt;- nrow(email)\nm_hat &lt;- rep(NA, n)\ne_hat &lt;- rep(NA, n)\n\nhead(m_hat)\n\n[1] NA NA NA NA NA NA\n\n\nHowever, because we generally do not rely on structural assumptions for the estimation of nuisance parameters and ML methods are prone to overfitting, we need to perform cross-fitting, i.e. we split the sample into \\(K\\) folds and for each fold \\(k\\), we train (or “fit”) a model on the remaining \\(K-1\\) folds. We predict the nuisance parameters for units in the fold \\(k\\) only with the model trained on the remaining \\(K-1\\) folds.\nQuestion: What is overfitting?\n\nAnswer\nAn inadequate modeling of the data which results in a too close fit with the training but a bad fit with the test data.\n\n\n\n\n\n\nTip\n\n\n\nLiterature and methods from causal/statistical inference and machine learning often describe the same process. E.g. in causal/statistical inference the term fitting is mostly used, while in machine learning the term training is more prevalent. Other examples are covariates vs. features or outcome vs. target.\n\n\n\n\nK-fold cross fitting with K = 5\n\nFor the sake of simplicity and demonstration, we select \\(K=2\\). Common values for smaller data sets are \\(K=5\\) or \\(K=10\\), while for larger data sets, you will also see \\(K=20\\) quite often. Generally, the choice of \\(K\\) is associated with bias-variance trade-off and empirically, the values have shown to perform well.\n\n# Split sample\nids_s1 &lt;- sample(1:n, n/2) # indices of sample 1\nids_s2 &lt;- which(!1:n %in% ids_s1) # indices of sample 2\n\nhead(ids_s1)\n\n[1] 1786   34  696  921 4240 5669\n\n\nNow, we just have to use the right sample for training and prediction as described above. Remember: a unit must never be predicted with a model that was trained on the same observation.\n\n\n\n\n\n\nNote\n\n\n\nIn the mlr3 framework, we fit/train using the dollar operator and the method train() which takes the task and the row indexes as arguments. To predict, we use the method predict(), which takes the same arguments. You might feel that the usage of the dollar operator and methods is a bit different to what we have seen before - and you’re right: it’s from the R S4 system, which is a system for object oriented programming.\n\n\n\n# Iteration 1:\n# Train: S1 - Predict: S2\n# Y ~ X\nlrnr_m$train(task_m, row_ids = ids_s1)\nm_hat[ids_s2] &lt;- lrnr_m$predict(task_m, row_ids = ids_s2)$response\n# D ~ X\nlrnr_e$train(task_e, row_ids = ids_s1)\ne_hat[ids_s2] &lt;- lrnr_e$predict(task_e, row_ids = ids_s2)$prob[, 2] # col 2 for value D = 1\n\nQ2. Run the second iteration.\n\nCode# Iteration 2:\n# Train: S2 - Predict: S1\n# Y ~ X\nlrnr_m$train(task_m, row_ids = ids_s2)\nm_hat[ids_s1] &lt;- lrnr_m$predict(task_m, row_ids = ids_s1)$response\n# D ~ X\nlrnr_e$train(task_e, row_ids = ids_s2)\ne_hat[ids_s1] &lt;- lrnr_e$predict(task_e, row_ids = ids_s1)$prob[, 2]\n\n\nHaving obtained our nuisance parameters, we can move to step 3 and estimate the residuals-on-residuals regression.\nQ3a. Obtain the residuals and store them as Y_hat and D_hat.\n\nCode# Obtain outcome and treatment residuals\nY &lt;- email$next_mnth_pv\nD &lt;- email$mkt_email\nY_hat &lt;- Y - m_hat\nD_hat &lt;- D - e_hat\n\n\nQ3b. Run the residual-on-residual regression.\n\nCode# Residual-on-residual regression\nmod_plr &lt;- lm(Y_hat ~ 0 + D_hat)\nsummary(mod_plr)\n\n\nCall:\nlm(formula = Y_hat ~ 0 + D_hat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-22982   -148     -9    141 120927 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nD_hat   1317.6       39.2    33.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1960 on 9999 degrees of freedom\nMultiple R-squared:  0.101, Adjusted R-squared:  0.101 \nF-statistic: 1.13e+03 on 1 and 9999 DF,  p-value: &lt;2e-16\n\n\nWe obtain our target parameter - the estimated effect of receiving an email on the purchase volume.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#double-machine-learning-with-augmented-inverse-probability-weighting",
    "href": "content/course_weeks/week_05/week_5.html#double-machine-learning-with-augmented-inverse-probability-weighting",
    "title": "5 - Double Machine Learning",
    "section": "Double machine learning with augmented inverse probability weighting",
    "text": "Double machine learning with augmented inverse probability weighting\nNow we want to make use of augmented inverse probability weighting, i.e. we want to combine outcome modeling and weighting with treatment probabilities. Again, Chernozhukov et al. (2018) propose a three step procedure.\n\nFor prediction model and obtain the fitted values of the propensity scores:\n\n\\(\\hat{e}_t(\\mathbf{X_i})\\)\n\n\nFor outcome models and obtain the fitted values of the outcome regressions:\n\n\n\\(\\hat{\\mu}(t, \\mathbf{X_i})\\).\n\n\nConstruct the doubly robust estimator:\n\n\\(\\hat{\\tau}_{\\text{ATE}} = \\hat{\\mu}_{1} - \\hat{\\mu}_{0}\\) with\n\\(\\hat{\\mu}_{1} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\hat{\\mu}(t= 1, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = 1)(Y_i - \\hat{\\mu}(t = 1, \\mathbf{ X_i})}{\\hat{e}_1(\\mathbf{X_i})}\\right]\\)\n\\(\\hat{\\mu}_{0} = \\frac{1}{n} \\sum_{i=1}^n \\left[\\hat{\\mu}(t= 0, \\mathbf{X_i}) + \\frac{\\mathbb{1}(T_i = 0)(Y_i - \\hat{\\mu}(t = 0, \\mathbf{ X_i})}{\\hat{e}_0(\\mathbf{X_i})}\\right]\\) and\n\\(\\hat{e}_0(\\mathbf{X_i}) = 1 - \\hat{e}_1(\\mathbf{X_i})\\)\n\n\n\nAgain, we initialize our nuisance vectors, but this time, we need to differentiate between a model for the untreated and one for the treated outcomes because we estimate the \\(APO\\) for all units and all outcomes, separately.\n\n# Initialize nuisance vectors\ne_hat &lt;- rep(NA,n)\nm0_hat &lt;- rep(NA,n) # untreated\nm1_hat &lt;- rep(NA,n) # treated\n\nTherefore, we also filter on whether some received the treatment or not.\n\n# Treatment indices\nids_d0 &lt;- which(email$mkt_email==0)\nids_d1 &lt;- which(email$mkt_email==1)\n\nThen, again using cross-fitting, because we do not rely on structural assumptions, we train and predict.\n\n# Iteration 1:\n# Train in S1, predict in S2\n# D ~ X\nlrnr_e$train(task_e, row_ids = ids_s1)\ne_hat[ids_s2] &lt;- lrnr_e$predict(task_e, row_ids = ids_s2)$prob[, 2]\n# Y0 ~ X\nlrnr_m$train(task_m, row_ids = intersect(ids_s1, ids_d0))\nm0_hat[ids_s2] &lt;- lrnr_m$predict(task_m, row_ids = ids_s2)$response\n# Y1 ~ X\nlrnr_m$train(task_m, row_ids = intersect(ids_s1, ids_d1))\nm1_hat[ids_s2] &lt;- lrnr_m$predict(task_m, row_ids = ids_s2)$response\n\nQ4. Run the second iteration.\n\nCode# Iteration 2:\n# Train in S2, predict in S1\n# D ~ X\nlrnr_e$train(task_e, row_ids = ids_s2)\ne_hat[ids_s1] &lt;- lrnr_e$predict(task_e, row_ids = ids_s1)$prob[, 2]\n# Y0 ~ X\nlrnr_m$train(task_m, row_ids = intersect(ids_s2, ids_d0))\nm0_hat[ids_s1] &lt;- lrnr_m$predict(task_m, row_ids = ids_s1)$response\n# Y1 ~ X\nlrnr_m$train(task_m, row_ids = intersect(ids_s2, ids_d1))\nm1_hat[ids_s1] &lt;- lrnr_m$predict(task_m, row_ids = ids_s1)$response\n\n\nAnd finally, using the formula from above, we obtain the potential outcomes for each unit. The ATE obtains as the difference between those two.\nQ5. Calculate the potential outcomes and the ATE.\n\nCode# Potential outcomes with AIPW\nY_0_PO &lt;- m0_hat + (1-D) * (Y-m0_hat) / (1-e_hat)\nY_1_PO &lt;- m1_hat + D * (Y-m1_hat) / e_hat\n\n# ATE\nY_ate &lt;- Y_1_PO - Y_0_PO\n\n\nA trick to obtain the same statistical information as we are used to, is by simply regressing the variable Y_ate on the constant 1.\n\n# Obtain statistical inference (same as t-test)\nmod_aipw &lt;- lm(Y_ate ~ 1)\nsummary(mod_aipw)\n\n\nCall:\nlm(formula = Y_ate ~ 1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-19694   -309    -32    264 117421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1336.1       19.2    69.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1920 on 9999 degrees of freedom",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#doubleml",
    "href": "content/course_weeks/week_05/week_5.html#doubleml",
    "title": "5 - Double Machine Learning",
    "section": "DoubleML",
    "text": "DoubleML\nIn the previous examples, we manually coded the estimation procedures, which involved splitting the sample and executing all steps ourselves. However, employing a function to automate these tasks offers numerous advantages. For instance, it enables easy adjustment of the number of folds \\(K\\), streamlines the process of changing the models for estimation, and significantly reduces the risk of errors by preventing confusion regarding which sample to use for training and prediction.\nThe R package DoubleML provides an implementation of the double / debiased machine learning framework and is built on top of the mlr3 framework we have already used. It also has a Python twin.\n\ninstall.packages(\"DoubleML\")\n\nA little different than before, we need to specify the data object first. The object contains the data frame, the outcome column y_col and the treatment column d_cols.\n\n# Load package\nlibrary(DoubleML)\n\n# Specify data object\nemail_dml_data &lt;- DoubleMLData$new(\n  data = as.data.frame(email),\n  y_col = \"next_mnth_pv\", \n  d_cols = \"mkt_email\"\n  )\n\nPartially linear regression\nWe choose the class DoubleMLPLR, which is short for “partially linear regression” and provide the data object and specify what estimators to use. We can just reuse the estimators from before. The methods fit() and summary() are self-explanatory.\n\n# Specify task\ndml_plr_obj &lt;- DoubleMLPLR$new(\n  data =  email_dml_data, # data object\n  ml_l = lrnr_m, # outcome model\n  ml_m = lrnr_e, # exposure model\n  apply_cross_fitting = TRUE,\n  n_folds = 10\n  )\n\n# Fit and return summary\ndml_plr_obj$fit()\ndml_plr_obj$summary()\n\nEstimates and significance testing of the effect of target variables\n          Estimate. Std. Error t value Pr(&gt;|t|)    \nmkt_email   1315.72       3.86     341   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAugmented inverse probablity weighting\nThe steps are the same for our AIPW estimator, only that we use the class DoubleMLIRM and additionally need to specify the score argument which we set to \\(ATE\\).\n\n# Specify task\ndml_aipw_obj = DoubleMLIRM$new(\n  data = email_dml_data,\n  ml_g = lrnr_m,\n  ml_m = lrnr_e,\n  score = \"ATE\",\n  trimming_threshold = 0.01, # to prevent too extreme weights\n  apply_cross_fitting = TRUE,\n  n_folds = 10)\n\n# Fit and return summary\ndml_aipw_obj$fit()\ndml_aipw_obj$summary()\n\nEstimates and significance testing of the effect of target variables\n          Estimate. Std. Error t value Pr(&gt;|t|)    \nmkt_email    1334.7       19.4    68.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCode# Table of all estimates and standard errors\ntbl_coef &lt;- tibble(\n  estimator = c(\n    \"PLR (by hand)\",\n    \"AIPW (by hand)\",\n    \"PLR (DoubleML)\",\n    \"AIPW (DoubleML)\"\n  ),\n  coef = c(\n    broom::tidy(mod_plr)$estimate,\n    broom::tidy(mod_aipw)$estimate,\n    dml_plr_obj$coef,\n    dml_aipw_obj$coef\n  ),\n  se   = c(\n    broom::tidy(mod_plr)$std.error,\n    broom::tidy(mod_aipw)$std.error,\n    dml_plr_obj$se,\n    dml_aipw_obj$se\n  )\n)\n\n# Plot comparison\nggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +\n  geom_errorbar(width = .5) +\n  ylim(c(1100, 1400))",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_05/week_5.html#assignment",
    "href": "content/course_weeks/week_05/week_5.html#assignment",
    "title": "5 - Double Machine Learning",
    "section": "Assignment",
    "text": "Assignment\n\nAccept the Week 5 - Assignment and follow the same steps as last week and as described in the organization chapter.\nFor the purpose of introduction, we have just used logistic and linear regression. But as we mentioned, the usage of more flexible ML methods can have benefits.\n\n\nUse the commands from the DoubleML package but swap out the estimators with ML methods for\n\npartially linear regression and\naugmented inverse probability weighting.\n\n\n\nYou can find an overview of more ML-like estimators here. Please note, that for some of the learners, you have to install the package mlr3extralearners which is just available from GitHub via …\n\n# You can ignore this for now.\ninstall.packages(\"remotes\")\nremotes::install_github(\"mlr-org/mlr3extralearners\")\n\nload it via …\n\nlibrary(mlr3extralearners)\n\nand, in some cases, install learners via …\n\ninstall_learners(\"..\")\n\n\nA few weeks later you are able to run an AB-test with perfectly randomized groups that either receive a marketing email or do not. Load email_rnd.csv, recall, how to retrieve the \\(ATE\\) in randomized experiments and report it.\nCompare the treatment effects from task 1 and 2 visually using ggplot and shortly discuss what estimate you have the most trust in.\nExplain why in double machine learning cross-fitting plays such a crucial role. What would happen if you left out the step of cross-fitting?\n\n\n\n\n\nK-fold cross fitting with K = 5",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "5 - Double Machine Learning"
    ]
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html",
    "href": "content/course_weeks/week_09/week_9_oli.html",
    "title": "9 - Difference-in-Differences",
    "section": "",
    "text": "So far, we have primarily discussed cross-sectional data, where each unit is observed only once. However, sometimes you can observe units over multiple time periods, resulting in panel data. This allows us to see how a unit changes before and after a treatment. When randomization isn’t feasible, panel data is the best alternative for identifying causal effects. The most well-known method for estimating this effect is called difference-in-differences (DiD)."
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#slides-recap",
    "href": "content/course_weeks/week_09/week_9_oli.html#slides-recap",
    "title": "9 - Difference-in-Differences",
    "section": "",
    "text": "So far, we have primarily discussed cross-sectional data, where each unit is observed only once. However, sometimes you can observe units over multiple time periods, resulting in panel data. This allows us to see how a unit changes before and after a treatment. When randomization isn’t feasible, panel data is the best alternative for identifying causal effects. The most well-known method for estimating this effect is called difference-in-differences (DiD)."
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#panel-data",
    "href": "content/course_weeks/week_09/week_9_oli.html#panel-data",
    "title": "9 - Difference-in-Differences",
    "section": "Panel Data",
    "text": "Panel Data\nIn marketing, randomization is often impractical because you cannot always control who receives the treatment. For instance, let’s say you decide to place billboards in a city to advertise your app, aiming to measure the resulting number of downloads attributed to these billboards. You will advertise in some cities but not in others, creating a geo experiment. Additionally, you will track several time-invariant covariates:\n\n\n\\(age_i\\): age average in city \\(i\\)\n\n\n\\(population\\_size_i\\): integer indicating population size of city \\(i\\)\n\n\n\\(user\\_share_i\\): initial share of app users prior to observation period in city \\(i\\)\n\n\n\\(competitor\\_price_i\\): average of competitor app price in that given area\n\nUnlike your competitor, your app is free to download, and you plan to generate revenue within the app. Our observed units are cities \\(i\\) over multiple time periods \\(t\\). Let’s have a look at the data.\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Read data\ndf &lt;- readRDS(\"mkt_panel.rds\")\n\n# Print first lines\nprint(df |&gt; head(10))\n\n   city month post downloads ever_treated age user_share population_size competitor_price\n1     1     1    0       418            1  44       0.23              15              2.2\n2     1     2    0       437            1  44       0.23              15              2.2\n3     1     3    0       472            1  44       0.23              15              2.2\n4     1     4    0       460            1  44       0.23              15              2.2\n5     1     5    0       509            1  44       0.23              15              2.2\n6     1     6    0       524            1  44       0.23              15              2.2\n7     1     7    0       568            1  44       0.23              15              2.2\n8     1     8    0       605            1  44       0.23              15              2.2\n9     1     9    1       640            1  44       0.23              15              2.2\n10    2     1    0       461            1  45       0.23               8              3.1\n\n\nWe see that for city \\(i=1\\), we have eight months with \\(post = 0\\) and 1 month with \\(post = 1\\). The outcome changes over time but the covariates are time invariant. Please also note that the treatment indicator indicates whether a unit was “ever” treated.\nWhen counting the number of control and treatment units per period, we’ll see that we have a complete panel without any missing data.\n\n# Panel data overview\ndf_piv &lt;- df |&gt;\n  count(month, ever_treated, post) |&gt; \n  pivot_wider(\n    names_from = \"ever_treated\",\n    values_from = \"n\",\n    names_prefix = \"ever_treated_\"\n    )\n\n# Print\nprint(df_piv)\n\n# A tibble: 9 × 4\n  month  post ever_treated_0 ever_treated_1\n  &lt;int&gt; &lt;dbl&gt;          &lt;int&gt;          &lt;int&gt;\n1     1     0             91            109\n2     2     0             91            109\n3     3     0             91            109\n4     4     0             91            109\n5     5     0             91            109\n6     6     0             91            109\n7     7     0             91            109\n8     8     0             91            109\n9     9     1             91            109"
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#basic-differences-in-differences",
    "href": "content/course_weeks/week_09/week_9_oli.html#basic-differences-in-differences",
    "title": "9 - Difference-in-Differences",
    "section": "Basic differences-in-differences",
    "text": "Basic differences-in-differences\nNot always you will have several pre-treatment periods and DiD already works when you only observe one period before and one period after the treatment. So let’s image that scenario and later re-introduce the other pre-treatment periods.\n\n# Last pre-treatment and post-treatment period\ndf_2p &lt;- df |&gt; filter(month %in% 8:9)\n\n# Print\nprint(df_2p |&gt; count(month, ever_treated, post))\n\n  month ever_treated post   n\n1     8            0    0  91\n2     8            1    0 109\n3     9            0    1  91\n4     9            1    1 109\n\n\nBecause we observe both control and treated units before and after the treatment, we can compute the treatment effect \\(\\tau_{\\text{DiD}}\\) using the difference-in-differences method. This involves first calculating the difference in outcomes between the treated and control groups after the treatment, then subtracting the difference in outcomes between these groups before the treatment. This double differencing helps isolate the treatment effect.\n\\[\n\\begin{aligned}\n\\tau_{\\text{DiD}} = \\underbrace{\\mathbb{E}[Y_{i,1} | D_i=1] - \\mathbb{E}[Y_{i,1} | D_i=0]}_{\\text{difference in post-treatment}} \\\\ - \\underbrace{(\\mathbb{E}[Y_{i,0} | D_i=1] - \\mathbb{E}[Y_{i,0} | D_i=0])}_{\\text{difference in pre-treatment}}\n\\end{aligned}\n\\]\nIn a slightly different and cleaned up notation, we see that the difference we obtain is only attributed to the treatment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nTime\nOutcome\n1st Difference\nDiD\n\n\n\nTreatment (D=1)\n0\n\\(Y= Y_{T=0, D=1}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0,D=1} + T + D\\)\n\\(T +D\\)\n\n\n\n\n\n\n\n\\(D\\)\n\n\nControl (D=0)\n0\n\\(Y = Y_{T=0, D=0}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0, D=0} + T\\)\n\\(T\\)\n\n\n\n\n\n\n\n\n\nTask 1: With the help of the formula/table, compute \\(\\tau_{\\text{DiD}}\\). What is the interpretation with regard to our example?\n\nCode# Step 1: group by treatment group and period\ndid_p2 &lt;- df_2p |&gt; \n  group_by(ever_treated, post) |&gt; \n  summarise(\n    y = mean(downloads)\n  ) |&gt; \n  ungroup()\n\nprint(did_p2)\n\n# A tibble: 4 × 3\n  ever_treated  post     y\n         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1            0     0  517.\n2            0     1  546.\n3            1     0  566.\n4            1     1  609.\n\n\n\n# Step 2: Extract values and doubly differencing\nwith(did_p2, {\n  y_00 &lt;- y[ever_treated == 0 & post == 0]\n  y_01 &lt;- y[ever_treated == 0 & post == 1]\n  y_10 &lt;- y[ever_treated == 1 & post == 0]\n  y_11 &lt;- y[ever_treated == 1 & post == 1]\n  \n  (y_11 - y_10) - (y_01 - y_00)\n})\n\n[1] 15\n\n\nAlternatively, we can use regression-based approaches to estimate the treatment effect. Two different approaches yielding equivalent results are:\n\nTwo-way fixed effects (TWFE).\n\n\\[\nY_{i,t} = \\alpha_i + \\gamma_t + \\tau_{\\text{DiD}} (D_i \\times t) + \\epsilon_{i,t}\n\\]\n\nRegression with treatment and time dummy, as well as interaction of treatment and time.\n\n\\[\nY_{i,t} = \\alpha + \\beta D_i + \\gamma t + \\tau_{\\text{DiD}} (D_i \\times t) + \\epsilon_{i,t}\n\\] Task 2: Perform both approaches and report the estimated coefficient.\n\nCode# (1)\nmod_1 &lt;- lm(downloads ~ ever_treated:post + as.factor(city) + as.factor(month), data = df_2p)\n\n# Coefficient\ncoef_mod_1 &lt;- mod_1$coefficients[\"ever_treated:post\"]\nsprintf(\"%.2f\", coef_mod_1)\n\n[1] \"14.86\"\n\n\n\nCode# (2)\nmod_2 &lt;- lm(downloads ~ ever_treated*post, data = df_2p)\nsummary(mod_2)\n\n\nCall:\nlm(formula = downloads ~ ever_treated * post, data = df_2p)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-489.1  -54.8   -3.8   55.5  564.1 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          516.7       11.3   45.93   &lt;2e-16 ***\never_treated          48.8       15.2    3.20   0.0015 ** \npost                  29.0       15.9    1.82   0.0691 .  \never_treated:post     14.9       21.6    0.69   0.4908    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 107 on 396 degrees of freedom\nMultiple R-squared:  0.0911,    Adjusted R-squared:  0.0842 \nF-statistic: 13.2 on 3 and 396 DF,  p-value: 3.01e-08\n\n\nFor two-way fixed effects estimation, we can generally also use the lm() command which we have used for so many applications already. However, it is recommended to use the fixest package due to its increased speed and focus on fixed effects. As it is the workhorse of many other packages, let’s get it installed.\n\ninstall.packages(\"fixest\")\n\nThe formula notation is slightly different as we separate the fixed effects using the | operator. This approach allows the algorithm to converge faster, and the fixed effects will not appear in the regression summary. Since fixed effects are typically not of primary interest and we often have a large number of them, this simplification is quite convenient.\n\nCode# Faster way for (1)\nlibrary(fixest)\nsummary(feols(downloads ~ ever_treated:post | city + month, data = df_2p))\n\nOLS estimation, Dep. Var.: downloads\nObservations: 400\nFixed-effects: city: 200,  month: 2\nStandard-errors: Clustered (city) \n                  Estimate Std. Error t value  Pr(&gt;|t|)    \never_treated:post       15          3       5 1.399e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 10.4     Adj. R2: 0.98258 \n             Within R2: 0.112107"
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#parallel-trends",
    "href": "content/course_weeks/week_09/week_9_oli.html#parallel-trends",
    "title": "9 - Difference-in-Differences",
    "section": "Parallel trends",
    "text": "Parallel trends\nHere, where we only have two time period, we need to rely on one assumption to hold: the parallel trends assumption. Opposed to methods where we just know one outcome - the “after” outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential outcomes are equal \\((Y_1(0)-Y_0(0)) \\perp\\!\\!\\!\\perp T\\). That is a big difference, because do not have to assume that observation units are similar in all their characteristics. Instead DiD hinges on a different assumption, the parallel trends assumption. It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but only the change of outcomes from before to after. It implies that there is no factor that has only an impact on just one of the groups.\n\\[\n\\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | D_i=1] = \\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | D_i=0]\n\\]\nWhile the parallel trends assumption is generally untestable, in the case with only two periods, we are particularly in the dark.\nGraphically, what we assume is that the treated line would have evolved similarly to the control line - as depicted by the dotted line. What can we do to increase the plausibility of this assumption?\n\nCodey10 &lt;- did_p2[did_p2$ever_treated==1 & did_p2$post==0, ]$y\ny01 &lt;- did_p2[did_p2$ever_treated==0 & did_p2$post==1, ]$y\ny00 &lt;- did_p2[did_p2$ever_treated==0 & did_p2$post==0, ]$y\n\ndata_cf &lt;- tibble(\n  x = c(0, 1),\n  y = c(y10, y10 + (y01-y00))\n)\n\nggplot(did_p2, aes(x = post, y = y, group = ever_treated, color = as.factor(ever_treated))) +\n  geom_point() +\n  geom_line() +\n  geom_line(data = data_cf, aes(x=x, y=y, group = NA), color=\"#5AFFC5\", linetype = \"dotted\") +\n  geom_vline(xintercept = .5, linetype = \"dashed\") +\n  scale_x_continuous(breaks=c(0,1)) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#testing-for-parallel-trends",
    "href": "content/course_weeks/week_09/week_9_oli.html#testing-for-parallel-trends",
    "title": "9 - Difference-in-Differences",
    "section": "Testing for parallel trends",
    "text": "Testing for parallel trends\nThere is some remedy when we are able to observe multiple periods prior to the treatment. By comparing trends before treatment between the treatment and control groups, researchers aim to demonstrate that there was no significant difference prior to the treatment. The logic is that if there was no difference before treatment, any difference observed after the treatment is likely due to the treatment itself.\nHowever, even that cannot provide full certainty about the parallel trends assumption. There may still be unobserved factors that could affect the treatment. Nevertheless, event studies are a useful tool for arguing that the treatment and control groups are comparable.\nLet’s now switch back to the dataset with multiple pre-treatment periods and begin by plotting the observed outcomes over time. This will help us determine if there are converging or diverging trends between the treatment and control groups. By visualizing these trends, we can already assess whether the parallel trends assumption holds before the treatment.\n\n# Outcomes across time by group\ndf |&gt; \n  ggplot(aes(\n    x = month,\n    y = downloads,\n    fill = as.factor(ever_treated),\n    color = as.factor(ever_treated)\n  )) + \n  stat_summary(fun.data = mean_se, geom = \"point\", alpha = .8) +\n  stat_summary(fun.data = mean_se, geom = \"line\", alpha = .8) +\n  stat_summary(fun.data = mean_se, geom = \"ribbon\", alpha = .2, color = NA) +\n  geom_vline(xintercept = 8.5, linetype = \"dashed\") +\n  scale_x_continuous(breaks=1:9) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAt the first glance, prior to the treatment, the outcomes seem to evolve in parallel. However, a better way is to actually perform statistical tests and check the hypothesis that the evolution of both groups is parallel. For each difference between two pre-treatment periods, the should be no difference in the evolution of the outcomes across the treatment groups.\nTask 3a: Subset the data so you can perform test for pre-trends between the pre-treatment months 7 and 8 using the TWFE approach from above.\n\nCodedf_placebo &lt;- df |&gt; \n  filter(month %in% c(7, 8)) |&gt; \n  mutate(post = if_else(month == 8, 1, 0))\n\nsummary(feols(downloads ~ ever_treated:post | city + month, data = df_placebo))\n\nOLS estimation, Dep. Var.: downloads\nObservations: 400\nFixed-effects: city: 200,  month: 2\nStandard-errors: Clustered (city) \n                  Estimate Std. Error t value Pr(&gt;|t|) \never_treated:post       -2        2.9   -0.68  0.49595 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 10.4     Adj. R2: 0.980199\n             Within R2: 0.002316\n\n\nTask 3b: Subset the data so you can perform test for pre-trends between the pre-treatment months 3 and 8 using the TWFE approach from above.\n\nCodedf_placebo &lt;- df |&gt; \n  filter(month %in% c(8, 3)) |&gt; \n  mutate(post = if_else(month == 8, 1, 0))\n\nfeols(downloads ~ ever_treated:post | city + month, data = df_placebo)\n\nOLS estimation, Dep. Var.: downloads\nObservations: 400\nFixed-effects: city: 200,  month: 2\nStandard-errors: Clustered (city) \n                  Estimate Std. Error t value   Pr(&gt;|t|)    \never_treated:post      -17        4.8    -3.6 0.00033698 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 16.7     Adj. R2: 0.95883 \n             Within R2: 0.063532\n\n\nWith this approach, you would have to iterate through quite a lot of combinations. Another approach would be to add “fake” post columns to the data frame and interact them with the treatment indicator.\nTask 4: Subset the data so you can perform test for pre-trends between ALL two arbitrary pre-treatment periods using the TWFE approach from above. You’ll need to create new columns with e.g. post_t = if_else(month == t, 1, 0), where t is a treatment period and introduce new interactions into the formula.\n\nCodedf_placebo_all &lt;- df |&gt; \n  mutate(\n    post_1 = if_else(month == 1, 1, 0),\n    post_2 = if_else(month == 2, 1, 0),\n    post_3 = if_else(month == 3, 1, 0),\n    post_4 = if_else(month == 4, 1, 0),\n    post_5 = if_else(month == 5, 1, 0),\n    post_6 = if_else(month == 6, 1, 0),\n    post_7 = if_else(month == 7, 1, 0),\n    post_8 = if_else(month == 8, 1, 0)\n    )\n\nfeols(downloads ~ \n        ever_treated:post_1 +\n        ever_treated:post_2 +\n        ever_treated:post_3 +\n        ever_treated:post_4 +\n        ever_treated:post_5 +\n        ever_treated:post_6 +\n        ever_treated:post_7 +\n        #ever_treated:post_8 + reference value\n        ever_treated:post | city + month, data = df_placebo_all)\n\nOLS estimation, Dep. Var.: downloads\nObservations: 1,800\nFixed-effects: city: 200,  month: 9\nStandard-errors: Clustered (city) \n                    Estimate Std. Error t value   Pr(&gt;|t|)    \never_treated:post_1     19.1        6.3    3.03 2.7416e-03 ** \never_treated:post_2     16.8        5.5    3.03 2.7654e-03 ** \never_treated:post_3     17.4        4.8    3.64 3.4584e-04 ***\never_treated:post_4     10.0        4.1    2.42 1.6456e-02 *  \never_treated:post_5      5.7        3.8    1.51 1.3288e-01    \never_treated:post_6      3.0        3.2    0.93 3.5104e-01    \never_treated:post_7      2.0        2.9    0.68 4.9679e-01    \never_treated:post       14.9        3.0    4.97 1.4628e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 19.6     Adj. R2: 0.967926\n             Within R2: 0.030489\n\n\nAn even more programmatic way to test for pre-trends is to run an event study to perform the tests all in one command. We only have to slightly change the input. Instead of the single treatment indicator ever_treated:post, which is only active for the treated group after treatment, we include several interactions which cover all periods for the treated group.\nWe can do so with the help of i() from the fixest package and a newly created variable that measures the time distance to the treatment period.\n\nCode# Create time distance column\ntreat_month &lt;- 9\ndf &lt;- df |&gt; mutate(time_to_treat = if_else(ever_treated == 1, month - treat_month, 0))\n\nevt_stdy &lt;- feols(downloads ~ i(time_to_treat, ever_treated, ref = -1) | city + month, data = df)\nsummary(evt_stdy)\n\nOLS estimation, Dep. Var.: downloads\nObservations: 1,800\nFixed-effects: city: 200,  month: 9\nStandard-errors: Clustered (city) \n                               Estimate Std. Error t value   Pr(&gt;|t|)    \ntime_to_treat::-8:ever_treated     19.1        6.3    3.03 2.7416e-03 ** \ntime_to_treat::-7:ever_treated     16.8        5.5    3.03 2.7654e-03 ** \ntime_to_treat::-6:ever_treated     17.4        4.8    3.64 3.4584e-04 ***\ntime_to_treat::-5:ever_treated     10.0        4.1    2.42 1.6456e-02 *  \ntime_to_treat::-4:ever_treated      5.7        3.8    1.51 1.3288e-01    \ntime_to_treat::-3:ever_treated      3.0        3.2    0.93 3.5104e-01    \ntime_to_treat::-2:ever_treated      2.0        2.9    0.68 4.9679e-01    \ntime_to_treat::0:ever_treated      14.9        3.0    4.97 1.4628e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 19.6     Adj. R2: 0.967926\n             Within R2: 0.030489\n\n\nUsing iplot() we can visualize the multiple treatment effects. Any treatment effect prior to the treatment is a big cause of concern with regards to the parallel trends assumption.\n\n# Plot event study\niplot(evt_stdy)"
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#conditional-parallel-trends",
    "href": "content/course_weeks/week_09/week_9_oli.html#conditional-parallel-trends",
    "title": "9 - Difference-in-Differences",
    "section": "Conditional parallel trends",
    "text": "Conditional parallel trends\nNow, we have seen that we cannot assume parallel trends - there are significant differences in the evolution of outcomes. But what can we do to increase the plausibility of assuming parallel trends? One approach that is widely been used in practice is the inclusion of covariates. But as you have learned in the lecture, you have to be extremely careful.\nWhat you obtain then is the conditional parallel trends assumption, which is the assumption of parallel trends conditional on covariates.\n\\[\n\\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | T_i=1, \\mathbf{X_i}] = \\mathbb{E}[Y_{i,t=1}(0) - Y_{i,t=0}(0) | T_i=0, \\mathbf{X_i}] \\quad\n\\]\nTask 5: Include the covariates into the event study and consider that they have time-varying effects.\n\nCode# Time-invariant, therefore multicollinearity.\nevt_stdy_cond_1 &lt;- feols(\n  downloads ~ i(time_to_treat, ever_treated, ref = -1) +\n    age + user_share + population_size + competitor_price\n  | city + month, data = df)\n\nThe variables 'age', 'user_share', 'population_size' and 'competitor_price' have been removed because of collinearity (see $collin.var).\n\nCodeevt_stdy_cond_1\n\nOLS estimation, Dep. Var.: downloads\nObservations: 1,800\nFixed-effects: city: 200,  month: 9\nStandard-errors: Clustered (city) \n                               Estimate Std. Error t value   Pr(&gt;|t|)    \ntime_to_treat::-8:ever_treated     19.1        6.3    3.03 2.7416e-03 ** \ntime_to_treat::-7:ever_treated     16.8        5.5    3.03 2.7654e-03 ** \ntime_to_treat::-6:ever_treated     17.4        4.8    3.64 3.4584e-04 ***\ntime_to_treat::-5:ever_treated     10.0        4.1    2.42 1.6456e-02 *  \ntime_to_treat::-4:ever_treated      5.7        3.8    1.51 1.3288e-01    \ntime_to_treat::-3:ever_treated      3.0        3.2    0.93 3.5104e-01    \ntime_to_treat::-2:ever_treated      2.0        2.9    0.68 4.9679e-01    \ntime_to_treat::0:ever_treated      14.9        3.0    4.97 1.4628e-06 ***\n... 4 variables were removed because of collinearity (age, user_share and 2 others [full set in $collin.var])\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 19.6     Adj. R2: 0.967926\n             Within R2: 0.030489\n\n\n\nCode# Time-invariant, but time-varying effects.\nevt_stdy_cond_2 &lt;- feols(\n  downloads ~ i(time_to_treat, ever_treated, ref = -1) +\n    age:month + user_share:month + population_size:month + competitor_price: month\n  | city + month, data = df)\nevt_stdy_cond_2\n\nOLS estimation, Dep. Var.: downloads\nObservations: 1,800\nFixed-effects: city: 200,  month: 9\nStandard-errors: Clustered (city) \n                               Estimate Std. Error t value   Pr(&gt;|t|)    \ntime_to_treat::-8:ever_treated    -3.11      3.339   -0.93 3.5244e-01    \ntime_to_treat::-7:ever_treated    -2.24      3.364   -0.67 5.0676e-01    \ntime_to_treat::-6:ever_treated     1.58      3.271    0.48 6.3003e-01    \ntime_to_treat::-5:ever_treated    -2.66      3.071   -0.87 3.8768e-01    \ntime_to_treat::-4:ever_treated    -3.79      3.115   -1.22 2.2503e-01    \ntime_to_treat::-3:ever_treated    -3.36      2.851   -1.18 2.3982e-01    \ntime_to_treat::-2:ever_treated    -1.17      2.827   -0.41 6.8065e-01    \ntime_to_treat::0:ever_treated     18.03      2.870    6.28 2.0508e-09 ***\nage:month                          3.47      0.300   11.57  &lt; 2.2e-16 ***\nmonth:user_share                  49.36      5.315    9.29  &lt; 2.2e-16 ***\nmonth:population_size              0.32      0.048    6.60 3.6438e-10 ***\nmonth:competitor_price             2.22      0.430    5.16 5.9256e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 14.1     Adj. R2: 0.983262\n             Within R2: 0.495338\n\n\n\n# Plot event study\niplot(evt_stdy_cond_2)"
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#att-conditional-on-covariates",
    "href": "content/course_weeks/week_09/week_9_oli.html#att-conditional-on-covariates",
    "title": "9 - Difference-in-Differences",
    "section": "ATT conditional on covariates",
    "text": "ATT conditional on covariates\nTWFE\nTask 6: Use the TWFE approach to estimate the ATT by under the assumption of parallel trend conditional on the covariates.\n\nCodefeols(downloads ~ post:ever_treated +\n                    age:month + user_share:month + population_size:month + competitor_price: month\n                  | city + month, data = df)\n\nOLS estimation, Dep. Var.: downloads\nObservations: 1,800\nFixed-effects: city: 200,  month: 9\nStandard-errors: Clustered (city) \n                       Estimate Std. Error t value   Pr(&gt;|t|)    \npost:ever_treated         19.80      2.525     7.8 2.6261e-13 ***\nage:month                  3.46      0.296    11.7  &lt; 2.2e-16 ***\nmonth:user_share          49.49      5.244     9.4  &lt; 2.2e-16 ***\nmonth:population_size      0.32      0.048     6.6 4.5016e-10 ***\nmonth:competitor_price     2.21      0.428     5.1 6.2757e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 14.2     Adj. R2: 0.983281\n             Within R2: 0.49367 \n\n\nDoubly robust estimator\nDuring the last weeks we have already learned the advantages of doubly robust estimation and in fact, there is a doubly robust DiD estimator proposed by Sant’Anna and Zhao, 2020 and implemented in the DRDID package.\n\ninstall.packages(\"DRDID\")\n\n\nlibrary(DRDID)\n\ndrdid(\n  yname = \"downloads\",\n  tname = \"post\",\n  idname = \"city\",\n  dname = \"ever_treated\",\n  xformla = ~age+user_share+population_size+competitor_price,\n  data = df_2p\n)\n\n Call:\ndrdid(yname = \"downloads\", tname = \"post\", idname = \"city\", dname = \"ever_treated\", \n    xformla = ~age + user_share + population_size + competitor_price, \n    data = df_2p)\n------------------------------------------------------------------\n Further improved locally efficient DR DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n 18.6812     3.2087     5.822        0       12.3922    24.9703  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: weighted least squares.\n Propensity score est. method: inverse prob. tilting.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.\n\n\nTask 7: Following the same syntax, use the outcome regression adjustment with ordid() and the inverse probability weighting approach ipwdid() to estimate the treatment effect.\nOutcome regression\n\nCodeordid(\n  yname = \"downloads\",\n  tname = \"post\",\n  idname = \"city\",\n  dname = \"ever_treated\",\n  xformla = ~age+user_share+population_size+competitor_price,\n  data = df_2p\n)\n\n Call:\nordid(yname = \"downloads\", tname = \"post\", idname = \"city\", dname = \"ever_treated\", \n    xformla = ~age + user_share + population_size + competitor_price, \n    data = df_2p)\n------------------------------------------------------------------\n Outcome-Regression DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n 19.2478     3.2017     6.0117       0       12.9724    25.5232  \n------------------------------------------------------------------\n Estimator based on panel data.\n Outcome regression est. method: OLS.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details.\n\n\nInverse probability weighting regression\n\nCodeipwdid(\n  yname = \"downloads\",\n  tname = \"post\",\n  idname = \"city\",\n  dname = \"ever_treated\",\n  xformla = ~age+user_share+population_size+competitor_price,\n  data = df_2p\n)\n\n Call:\nipwdid(yname = \"downloads\", tname = \"post\", idname = \"city\", \n    dname = \"ever_treated\", xformla = ~age + user_share + population_size + \n        competitor_price, data = df_2p)\n------------------------------------------------------------------\n IPW DID estimator for the ATT:\n \n   ATT     Std. Error  t value    Pr(&gt;|t|)  [95% Conf. Interval] \n 18.7472     3.3073     5.6684       0       12.2648    25.2296  \n------------------------------------------------------------------\n Estimator based on panel data.\n Hajek-type IPW estimator (weights sum up to 1).\n Propensity score est. method: maximum likelihood.\n Analytical standard error.\n------------------------------------------------------------------\n See Sant'Anna and Zhao (2020) for details."
  },
  {
    "objectID": "content/course_weeks/week_09/week_9_oli.html#assignment",
    "href": "content/course_weeks/week_09/week_9_oli.html#assignment",
    "title": "9 - Difference-in-Differences",
    "section": "Assignment",
    "text": "Assignment\nComing soon.\n\n\nonly controlling for covariate change\nsoaked up by FE\ncompetitor price can be affected by treatment?\nwhat advantage of DR?"
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html",
    "href": "content/course_weeks/week_08/week_8.html",
    "title": "8 - Recap from week 5, 6, 7",
    "section": "",
    "text": "Important\n\n\n\nDue to an unexpected scheduling conflict, we have to cancel today’s lecture (17 June). Instead, the lecture will take place tomorrow (18 June) from 15:00 to 16:30 in room O-0.007. Following the lecture, the tutorial will be held from 16:45 to 18:15 in room A-0.19.\nSorry for the inconvenience. See you tomorrow!\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBefore start, please fill out course evaluation.\n\n\nThis week, we will do a recap of topics from week 5, week 6 and week 7. More precisely, the focus of this week’s tutorial will be on:\n\nDouble Machine Learning: General Recipe\nHeterogeneous Treatment Effect: Optimal Policy Learning\nUnobserved Confounding: Instrument Variable Estimation with Double Machine Learning",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Recap from week 5, 6, 7"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#slides-recap",
    "href": "content/course_weeks/week_08/week_8.html#slides-recap",
    "title": "8 - Recap from week 5, 6, 7",
    "section": "",
    "text": "Important\n\n\n\nDue to an unexpected scheduling conflict, we have to cancel today’s lecture (17 June). Instead, the lecture will take place tomorrow (18 June) from 15:00 to 16:30 in room O-0.007. Following the lecture, the tutorial will be held from 16:45 to 18:15 in room A-0.19.\nSorry for the inconvenience. See you tomorrow!\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBefore start, please fill out course evaluation.\n\n\nThis week, we will do a recap of topics from week 5, week 6 and week 7. More precisely, the focus of this week’s tutorial will be on:\n\nDouble Machine Learning: General Recipe\nHeterogeneous Treatment Effect: Optimal Policy Learning\nUnobserved Confounding: Instrument Variable Estimation with Double Machine Learning",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Recap from week 5, 6, 7"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#data",
    "href": "content/course_weeks/week_08/week_8.html#data",
    "title": "8 - Recap from week 5, 6, 7",
    "section": "Data",
    "text": "Data\nLet’s directly start using the following data set. Data very similar to what we have used last week but extended by a few covariates. For now, ignore the instrument popup, which you will need for your assignment.\n\nCode#df &lt;- readRDS(\"~/my_drive/02_TUHH/03_github/causal_ds_ss24/content/course_weeks/week_08/rand_enc_w8.rds\")\n\n\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Read data\ndf &lt;- readRDS(\"rand_enc_w8.rds\")",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Recap from week 5, 6, 7"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#double-machine-learning-general-recipe",
    "href": "content/course_weeks/week_08/week_8.html#double-machine-learning-general-recipe",
    "title": "8 - Recap from week 5, 6, 7",
    "section": "Double Machine Learning: General Recipe",
    "text": "Double Machine Learning: General Recipe\nWe’ll follow the general recipe for double machine learning from the lecture to estimate the average treatment effect using augmented inverse probability weighting (AIPW). It splits the process into four steps:\n\nFind Neyman-orthogonal score for your target parameter.\nPredict nuisance parameters.\nSolve empirical moment condition to estimate target parameter.\nCalculate standard error.\n\nStep 1\nIn the first step, we have to find the Neyman orthogonal score, which is\n\\[\n\\mathbb{E} \\left[ \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1,X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} - \\tau_{ATE} \\right] = 0.\n\\] We can already see that there are three nuisance parameters we need to fit and predict, given by the set \\(\\eta\\).\n\\[\n\\begin{align*}\n\\eta &= (\\mu(1, X), \\mu(0, X), e(X)), \\, \\text{where} \\\\\n\\mu(t,X) :&= \\mathbb{E}[Y \\mid T = t, X] \\\\\ne(X) :&= \\mathbb{E}[T=1 \\mid X]\n\\end{align*}\n\\]\nStep 2\nLike before, we are going to use the mlr3 environment and code all steps by hand. Let’s load the packages and define outcome and treatment.\n\nlibrary(mlr3)\nlibrary(mlr3learners)\n\n# Outcome\nY &lt;- df$time_spent\n# Treatment\nD &lt;- df$used_ftr\n\nOne key element of double machine learning is cross-fitting. So let’s create folds and assign each unit to one fold. Units from one particular fold will be predicted based on a model fitted on all other units.\n\nn &lt;- length(Y)\nnfolds &lt;- 5\nfold &lt;- sample(1:nfolds, n, replace=TRUE)\n\nFold by fold, we will go through the prediction process. Therefore, we first initialize empty nuisance vectors to be populated.\n\n# Initialize empty nuisance vectors\nehat &lt;- rep(NA, n)\nmuhat0 &lt;- rep(NA, n)\nmuhat1 &lt;- rep(NA, n)\n\nNow we can specify the models we want to use. The advantage of double machine learning is that we can choose from a wide range of flexible estimators to address non-linearities etc. E.g. the estimators classif.ranger and regr.ranger are random forests which have proven to show very good prediction performance.\nTask 1: Complete the code.\n\n# Define prediction model for the treatment/exposure\ntask_e &lt;- ...\nlrnr_e &lt;- ...\n\n# Define prediction model for the outcome\ntask_mu &lt;- ...\nlrnr_mu &lt;- ...\n\n\nCode# Define prediction model for the treatment/exposure\ntask_e &lt;- as_task_classif(df |&gt; select(-time_spent, -popup), target = \"used_ftr\")\nlrnr_e &lt;- lrn(\"classif.ranger\", predict_type = \"prob\")\n\n# Define prediction model for the outcome\ntask_mu &lt;- as_task_regr(df |&gt; select(-used_ftr, -popup), target = \"time_spent\")\nlrnr_mu &lt;- lrn(\"regr.ranger\", predict_type = \"response\")\n\n\nNow, we will loop through the estimation procedure to obtain estimated nuisance parameters for all units.\n\n\n\n\n\n\nNote\n\n\n\nfor loop\nA for loop in R is a control flow statement that allows code to be executed repeatedly based on a sequence of values, iterating over each element in a vector or list. It is commonly used for performing repetitive tasks efficiently.\n\n\nTask 2: Complete the code.\n\n# cross fitting of nuisance parameters\nfor (i in 1:nfolds){\n  print(paste(\"Fold\", i, \"...\"))\n\n  # outcome mu(0,x)\n  lrnr_mu$train(task_mu, row_ids = intersect(which(fold != i), which(df$used_ftr == 0)))\n  # muhat0[...] &lt;- \n  \n  # outcome mu(1,x)\n  # ...\n\n  \n  # propensity score e(x)\n  # ...\n}\n\n\nCode# cross fitting of nuisance parameters\nfor (i in 1:nfolds){\n  print(paste(\"Fold\", i, \"...\"))\n  # propensity score e(x)\n  lrnr_e$train(task_e, row_ids = which(fold != i))\n  ehat[which(fold == i)] &lt;- lrnr_e$predict(task_e, row_ids = which(fold == i))$prob[, 2]\n  \n  # outcome mu(0,x)\n  lrnr_mu$train(task_mu, row_ids = intersect(which(fold != i), which(df$used_ftr == 0)))\n  muhat0[which(fold == i)] &lt;- lrnr_mu$predict(task_mu, row_ids = which(fold == i))$response\n  \n  # outcome mu(1,x)\n  lrnr_mu$train(task_mu, row_ids = intersect(which(fold != i), which(df$used_ftr == 1)))\n  muhat1[which(fold == i)] &lt;- lrnr_mu$predict(task_mu, row_ids = which(fold == i))$response\n}\n\n[1] \"Fold 1 ...\"\n[1] \"Fold 2 ...\"\n[1] \"Fold 3 ...\"\n[1] \"Fold 4 ...\"\n[1] \"Fold 5 ...\"\n\n\nStep 3\nFor the third step, solving for our target parameter, we obtain:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} - \\tau_{\\text{ATE}} \\right] &= 0 \\\\\n\\tau_{\\text{ATE}} \\underbrace{(-1)}_{\\psi_a} + \\mathbb{E} \\bigg[ \\underbrace{\\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)}}_{\\psi_b} \\bigg] &= 0 \\\\\n\\Rightarrow \\tau_{\\text{ATE}} = -\\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]} = \\mathbb{E} \\left[  \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} \\right]\n\\end{align*}\n\\]\nFor this particular method, obtaining \\(\\psi_a\\) is quite simple.\n\npsi_a &lt;- rep(-1,length(Y))\n\nBut what about \\(\\psi_b\\)? That’s quite a long mathematical expression. Let’s break it down.\n\\[\n\\tau_{\\text{ATE}} = -\\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]} = \\mathbb{E} \\left[  {\\color{#FF7E15}\\mu(1, X) - \\mu(0, X)} + \\frac{T(Y - \\mu(1, X))}{e(X)} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} \\right]\n\\]\nTask 3a: Complete the code.\n\npsi_b_mu &lt;- ...\n\n\nCodepsi_b_mu &lt;- muhat1 - muhat0\n\n\n\\[\n\\tau_{\\text{ATE}} = -\\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]} = \\mathbb{E} \\left[  \\mu(1, X) - \\mu(0, X) + {\\color{#FF7E15}\\frac{T(Y - \\mu(1, X))}{e(X)}} - \\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)} \\right]\n\\]\nTask 3b: Complete the code.\n\npsi_b_res1 &lt;- ...\n\n\nCodepsi_b_res1 &lt;- D * (Y - muhat1) / ehat\n\n\n\\[\n\\tau_{\\text{ATE}} = -\\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]} = \\mathbb{E} \\left[  \\mu(1, X) - \\mu(0, X) + \\frac{T(Y - \\mu(1, X))}{e(X)} - {\\color{#FF7E15}\\frac{(1 - T)(Y - \\mu(0, X))}{1 - e(X)}} \\right]\n\\]\nTask 3c: Complete the code.\n\npsi_b_res0 &lt;- ...\n\n\nCodepsi_b_res0 &lt;- (1 - D) * (Y - muhat0) / (1 - ehat)\n\n\nBringing it back together, \\(\\psi_a\\) obtains as\nTask 3d: Complete the code.\n\npsi_b &lt;- ...\n\n\nCodepsi_b &lt;- psi_b_mu + psi_b_res1 - psi_b_res0\n\n\nand \\(\\tau_{\\text{ATE}} = -\\frac{\\mathbb{E}[\\psi_b(W; \\eta)]}{\\mathbb{E}[\\psi_a(W; \\eta)]}\\) as\nTask 3e: Complete the code.\n\ntau &lt;- ...\n\n\nCodetau &lt;- -sum(psi_b) / sum(psi_a)\ntau\n\n[1] 9\n\n\nStep 4\nIn the fourth step, compute statistical information such as standard errors. It is a bit cumbersome which is why I summarized it into a function. But feel free to check the details in the lecture slides.\n\nsummary_dml &lt;- function(psi_a, psi_b) {\n  # compute ATE\n  tau &lt;- -sum(psi_b) / sum(psi_a)\n  \n  # compute variance\n  psi &lt;- tau * psi_a + psi_b\n  Psi &lt;- - psi / mean(psi_a)\n  sigma2 &lt;- var(Psi)\n\n  # standard error\n  se &lt;- sqrt(sigma2 / n)\n\n  # t-statistic and p-value\n  se &lt;- sqrt(sigma2 / n)\n  t &lt;- tau / se\n  p &lt;- 2 * pt(abs(t), n ,lower = FALSE)\n  \n  # bind together\n  return(data.frame(tau = round(tau, 3), t = round(t, 3), se = round(se, 3), p = round(p, 5)))\n}\n\nApply function to our estimated vectors.\n\n# Inference\ntau_stats &lt;- summary_dml(psi_a, psi_b)\nprint(tau_stats)\n\n  tau  t   se p\n1   9 20 0.45 0\n\n\nDoubleML\nOf course, there is a faster and more flexible approach than the manual coding. In just a couple of steps, using Double_ML package, you will get the same result and there is less risk of including errors due to e.g. confusing indices etc.\nTask 4: Complete the code.\n\n# Load required packages\nlibrary(DoubleML)\n\n# suppress messages during fitting\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n# Specify data object\n# ...\n\n# Specify task\n# ...\n\n# Fit\n# ...\n\n# Return summary\n# ...\n\n\nCode# Load required packages\nlibrary(DoubleML)\n\n# suppress messages during fitting\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n# Specify data object\ndml_data &lt;- DoubleMLData$new(\n  data = as.data.frame(df),\n  y_col = \"time_spent\", \n  d_cols = \"used_ftr\",\n  x_cols = c(\"age\", \"gender\", \"premium\", \"daily_logins\")\n  )\n\n# Specify task\ndml_aipw_obj = DoubleMLIRM$new(\n  data = dml_data,\n  ml_g = lrnr_mu,\n  ml_m = lrnr_e,\n  score = \"ATE\",\n  trimming_threshold = 0.01, # to prevent too extreme weights\n  apply_cross_fitting = TRUE,\n  n_folds = 5)\n\n# Fit and return summary\ndml_aipw_obj$fit()\ndml_aipw_obj$summary()\n\nEstimates and significance testing of the effect of target variables\n         Estimate. Std. Error t value Pr(&gt;|t|)    \nused_ftr     9.170      0.448    20.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Recap from week 5, 6, 7"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#heterogeneous-treatment-effect-optimal-policy-learning",
    "href": "content/course_weeks/week_08/week_8.html#heterogeneous-treatment-effect-optimal-policy-learning",
    "title": "8 - Recap from week 5, 6, 7",
    "section": "Heterogeneous Treatment Effect: Optimal Policy Learning",
    "text": "Heterogeneous Treatment Effect: Optimal Policy Learning\nWhile we have estimated the average treatment effect, now we would like to estimate who would benefit from the treatment. To be more precise, who should we make sure to use the new feature? That is what policy learning is about. It yields a personalized treatment recommendation.\nMathematically, we want to find optimal policy \\(\\pi^*\\) that maximizes \\(Q(\\pi) := \\mathbb{E}[Y_i(\\pi(\\mathbf{X_i}))]\\). Based on the covariates, we decide whether a unit is assigned to the treatment or to the control group and in such a way that the each unit receives the best treatment or consequently that total effects are as large as possible.\nWith some re-arrangements and by comparing the policy against random policy, we get the optimization function with the desired properties. It shows that we need to “get right for those with biggest CATEs”. Because for each unit wrongly assigned, we lose or earn absolute value of its CATE.\n\\[\n\\hat{\\pi} = \\underset{\\pi \\in \\Pi}{\\arg \\max} \\left\\{ \\frac{1}{N} \\sum_{i=1}^N \\overbrace{|\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}|}^{\\text{weight}} \\underbrace{\\operatorname{sign}(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}})}_{\\text{to be classified}} \\overbrace{(2\\pi(X_i) - 1)}^{\\text{function to be learned}} \\right\\}\n\\]\nHowever, due to the fundamental problem of causal inference, we do not know the CATEs beforehand. Therefore, we use \\(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}\\) as an approximation. We add the needed terms to our data.\n\n\n\n\n\n\nNote\n\n\n\nabs() returns the absolute value while sign() returns the sign (either 1 or -1).\n\n\n\n# Approximation of individual treatment effects\nite_pseudo &lt;- dml_aipw_obj$psi_b[, 1, 1]\n\n# Add weight and class (1 or -1) to data\ndf_pl &lt;- df |&gt;\n  select(-popup, -used_ftr, -time_spent) |&gt;\n  add_column(sign = sign(ite_pseudo)) |&gt;\n  add_column(weight = abs(ite_pseudo))\n\nThen, we can simplify the expression from above and see we are left with a binary weighted classification problem and can use methods like decision trees/forests, logistic lasso, SVM etc. See here for a full overview of learners. The outcome in the classification problem is \\(\\operatorname{sign}(\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}})\\), the covariates are \\(X_i\\) and the weights are \\(|\\tilde{\\tau_i}_{\\text{ATE}}^{\\text{AIPW}}|\\).\nTask 3e: Complete the code.\n\n## Specify task\n# Task\n# ...\n\n# Assign weight column\n# ...\n\n# Specify learner\n# ...\n\n# Train\n# ...\n\n# Predict classes\n# ...\n\n\nCode## Specify task and learner\n# Task\ntask_pl &lt;- as_task_classif(df_pl, target = \"sign\")\n# Assign weight column\ntask_pl$set_col_roles(col = \"weight\", roles = \"weight\")\n\n# Specify learner\nlrnr_pl &lt;- lrn(\"classif.xgboost\")\n\n# Train\nlrnr_pl$train(task_pl)\n\n# Predict classes\npl_class &lt;- lrnr_pl$predict(task_pl)$response\nsummary(pl_class)\n\n  -1    1 \n  26 9974 \n\n\nLet’s add the recommended class to our data and check how the units assigned to different classes differ in regard to their characteristics.\n\n# Add to data frame\ndf_pl$rec_class &lt;- pl_class\n\n# Average values by recommended class (like CLAN)\ndf_pl_CLAN &lt;- df_pl |&gt;\n  group_by(rec_class) |&gt;\n  summarise(\n    age = mean(age),\n    gender = mean(gender),\n    premium = mean(premium),\n    logins = mean(daily_logins)\n  ) |&gt;\n  ungroup()\n\n# Print\nprint(df_pl_CLAN)\n\n# A tibble: 2 × 5\n  rec_class   age gender premium logins\n  &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1         33.4  0.385   0.115   2.85\n2 1          29.2  0.501   0.207   2.66\n\n\n\nCode# Plot CLAN\ndf_pl_CLAN |&gt;\n  pivot_longer(cols = -rec_class) |&gt; \n  ggplot(aes(x = rec_class, y = value)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~name, scales = \"free\")",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Recap from week 5, 6, 7"
    ]
  },
  {
    "objectID": "content/course_weeks/week_08/week_8.html#assignment",
    "href": "content/course_weeks/week_08/week_8.html#assignment",
    "title": "8 - Recap from week 5, 6, 7",
    "section": "Assignment",
    "text": "Assignment\nFor this week’s assignment, you’ll be asked to perform an instrumental variable estimation. You’ll need the following nuisance parameters\n\\[\n\\quad \\mu(Z_i, \\mathbf{X_i}) = \\mathbb{E}[Y_i \\mid Z_i, \\mathbf{X_i}] \\quad \\quad \\quad e(\\mathbf{Z_i, X_i}) = P[T_i \\mid Z_i, \\mathbf{X_i}] \\quad \\quad \\quad h(\\mathbf{X_i}) = P[Z_i \\mid \\mathbf{X_i}]\n\\]\nand the moment condition of Neyman-orthogonal score\n\\[\n\\begin{align}\n\\mathbb{E}[\\psi_b] + \\mathbb{E}[\\psi_a] \\cdot \\tau_{\\text{LATE}} = &\\mathbb{E}\\bigg[\\mu(1, \\mathbf{X_i}) - \\mu(0, \\mathbf{X_i}) + \\frac{Z_i(Y_i - \\mu(1, \\mathbf{X_i}))}{h(\\mathbf{X_i})} - \\frac{(1-Z_i)(Y_i - \\mu(0, \\mathbf{X_i})}{(1-h(\\mathbf{X_i}))} \\bigg] \\\\\n&+ \\mathbb{E}\\bigg[(-1)\\left[e(1, \\mathbf{X_i}) - e(0, \\mathbf{X_i}) + \\frac{Z_i(T_i - e(1, \\mathbf{X_i}))}{h(\\mathbf{X_i})} - \\frac{(1-Z_i)(T_i - e(0, \\mathbf{X_i})}{(1-h(\\mathbf{X_i}))}\\right] \\bigg] \\cdot \\tau_{\\text{LATE}} = 0.\n\\end{align}\n\\]\n\nAccept the Week 8 - Assignment and follow the same steps as last week and as described in the organization chapter.\nPlease also remember to fill out the course evaluation if you have not done so already.\n\n\nUsing the data from the tutorial include the instrument popup and compute the local average treatment effect (LATE) using interactive (AIPW) instrumental variable estimation. Follow the general recipe and answer the following questions. (You can choose any learner available in the mlr3 environment.)\n\nWhat nuisance parameters do you need to estimate? Answer in the following style: “Regress … on … conditional on …”.\nPlug your estimates in to the moment condition of the Neyman-orthogonal score to retrieve the estimate.\nAdditionally, perform the estimation using the DoubleML package.\nCompare the estimates to the estimate obtained in the tutorial.\n\n\n\nHaving obtained a pseudo outcome in the previous steps, use it to do offline policy learning with a decision tree lrn(\"classif.rpart\").\n\nConsider the following restriction: for the treatment effect to be beneficial for an individual, he/she needs to spend at least two more times on the app. Put differently, the cost of the treatment can be translated to two minutes. Integrate this restriction into your problem and train the decision tree.\nUse the function rpart.plot() from the rpart.plot package to interpret the result. Describe what you can infer from the plot.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "8 - Recap from week 5, 6, 7"
    ]
  },
  {
    "objectID": "content/course_weeks/week_01/week_1.html",
    "href": "content/course_weeks/week_01/week_1.html",
    "title": "1 - Introduction to Causal Inference",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure to read and follow the instructions in Organization before reading this section.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "1 - Introduction to Causal Inference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_01/week_1.html#christmas-example",
    "href": "content/course_weeks/week_01/week_1.html#christmas-example",
    "title": "1 - Introduction to Causal Inference",
    "section": "Christmas example",
    "text": "Christmas example\nConsider this scenario: you’re the owner of an online marketplace company that small and medium-sized businesses use to sell advertise and sell their products. The businesses act autonomously regarding prices, advertising etc. Because your revenue depends on the prosperity of these businesses, you want to support them by offering guidance when to implement sales campaigns featuring temporary price drops. From a business perspective, a price drop is beneficial when the increase in number of sold units compensates for the lower price. Therefore,it is important to know the number of additional units sold after a price reduction. For simplicity, we only focus on one particular category, socks, and we examine the weeks leading up to and including Christmas week.\nQuestion: It helps to make oneself as clear as possible what the research question is. So, try to formulate a question that entails very clearly what effect we are interested in.\nAnswer\nWhat is the impact of a price reduction on the number of units sold?\nNow, let’s take a look at some data that you collected which could help you to answer your research question.\n\n\n\n\n\n\nNote\n\n\n\nlibrary() loads external packages/libraries containing functions that are not built in base R. Here, we load the library tidyverse, which is, in fact, a collection of many useful libraries specifically developed for data science tasks.\nIf you only need one particular function from a library, you can pick it by the following syntax: library::function().\nAll libraries/packages you want to use need to be installed first by running install.packages(\"library_to_install\").\n\n\n\n# Load packages from the tidyverse\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# Change the path if needed\nsales &lt;- read_csv(\"xmas_sales.csv\")\n# Currently coded as 0/1, we convert to FALSE/TRUE\nsales$is_on_sale &lt;- as.logical(sales$is_on_sale)\n\nThe data consists of the following columns:\n\n\nstore: unique identifier of store\n\nweeks_to_xmas: weekly data for each store leading up to Christmas\n\navg_week_sales: historical average of sales indicating business size\n\nis_on_sale: sale/price reduction indicator\n\nweekly_amount_sold: average weekly sales during that week\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many other ways to get a first look at your data instead of simply entering the variable name or use print(). Often times you will see head(data, n) to see the first \\(n\\) lines. Just the same you can use tail(data, n) to see the \\(n\\) last lines. If it is mainly numeric data, summary() provides a good overview. If you have many columns, glimpse() from the dplyr package contained in the tidyverse helps you.\n\n\n\n# Simply enter the table name to show its content\nprint(sales)\n\n# A tibble: 2,000 × 5\n   store weeks_to_xmas avg_week_sales is_on_sale weekly_amount_sold\n   &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt; &lt;lgl&gt;                   &lt;dbl&gt;\n 1     1             3           13.0 TRUE                    220. \n 2     1             2           13.0 TRUE                    185. \n 3     1             1           13.0 TRUE                    146. \n 4     1             0           13.0 FALSE                   102. \n 5     2             3           19.9 FALSE                   103. \n 6     2             2           19.9 FALSE                    53.7\n 7     2             1           19.9 FALSE                    13.8\n 8     2             0           19.9 FALSE                     0  \n 9     3             3           18.5 FALSE                    97.0\n10     3             2           18.5 FALSE                    54.7\n# ℹ 1,990 more rows\n\n\nLet’ connect the data from the table with the formula notation you have learned in the lecture. First of all, it is important to state that our units of analysis \\(i\\) are stores.\n\\(D_i\\) denotes the treatment for unit \\(i\\) and for our example it can take either one of the two values:\n\\[\nD_i=\\begin{cases}1 \\ \\text{if unit i received treatment}\\\\0 \\ \\text{otherwise}\\\\\\end{cases}\n\\]\nDon’t be confused by the term treatment, it is not a medical treatment (but can be) and other terms used are intervention or manipulation. Here, the treatment \\(D_i\\) is whether a store dropped its prices in a particular week.\n\n\n\n\n\n\nTip\n\n\n\nSometimes, you will encounter \\(T_i\\) instead of \\(D_i\\). But because in R, T is used to abbreviate the boolean value TRUE and in many other applications, \\(T\\) is reserved for time, we will use \\(D\\) instead.\n\n\nQuestion: What is the data equivalent for the outcome \\(Y_i\\)?\nAnswer\nIt is weekly_amount_sold.\nLet’s revisit the initial research question. We are interested in the effect a price reduction has on sales. We can express it as either\n\nthe effect of \\(T\\) on \\(Y\\)\n\nthe is_on_sale on weekly_amount_sold.",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "1 - Introduction to Causal Inference"
    ]
  },
  {
    "objectID": "content/course_weeks/week_01/week_1.html#fundamental-problem-of-causal-inference",
    "href": "content/course_weeks/week_01/week_1.html#fundamental-problem-of-causal-inference",
    "title": "1 - Introduction to Causal Inference",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nTo compute the effect, we would ideally know for each observation the counterfactual outcome, i.e. if it was on sale, how would have been sales if it was not on sales or if it was not on sale, how would have been sales if it was on sale? However, due to the fundamental problem of causal inference, it is impossible to observe both states.\nTherefore, at first, you are often tempted to compare the observations that were on sale (“treated”) with the observations that were not on sale (“control”). Because that’s easy to calculate, let’s plot the result. A good way to compare and plot observations of two groups is a box plot.\n\n\n\n\n\n\nNote\n\n\n\nFor plots, we make use of the package ggplot2 which is included in the tidyverse you already loaded. Both plots show the same data and convey the same message, but because ggplot2 is a package dedicated to data visualization it has some advantages regarding aesthetics and the efficiency in creating plots.\nIf you are interested in an introduction to ggplot2, I recommend this resource written by Megan Hall.\nBy the way, don’t be confused when your plot appears different in terms of colors, fonts etc. compared to the one shown here. It is adjusted to match the website theme.\n\n\n\n# Box plot in base R\n# boxplot(weekly_amount_sold ~ is_on_sale, data = sales)\n\n# Box plot in ggplot2\nggplot(\n  data = sales, # first, provide the data\n  aes( # then, provide the aesthetics (at least X and Y)\n    x = is_on_sale, \n    y = weekly_amount_sold\n    )) +\n  geom_boxplot() # with a \"+\" add what type of plot\n\n\n\n\n\n\n\nWhat do we see? Stores that dropped their prices sell more. It confirms our intuition that people buy more on sale. However, the difference seems very high. Let’s calculate it.\n\n\n\n\n\n\nNote\n\n\n\nTo access a data frame, there are several ways in R. In the following chapters, we will use other ways to extract data but here we use the following syntax: dataframe[condition, ]$column. First you take the data frame that contains the data you want to extract or subset. Then you provide a condition on how to subset. If you just want to have a specific column, you can extract it using the $ operator. Please note that if your condition refers to a column in the same data frame, you need to call the data frame another time like dataframe$filter_column.\nTo compute the average of a column (or more precise: a vector), we use the function mean().\nFor printing several variables, in notebooks, you can just collect them in a vector by c(var1, var2, ...). If you want to name the elements provide a name in quotation marks: c(\"name1\" = var1, ...).\n\n\n\n# Outcome for all observations on sale\nY1 &lt;- sales[sales$is_on_sale == TRUE, ]$weekly_amount_sold\nY1_mean &lt;- mean(Y1)\n# Outcome for all observations not on sale\nY0 &lt;- sales[sales$is_on_sale == FALSE, ]$weekly_amount_sold\nY0_mean &lt;- mean(Y0)\n\n# Show both outcomes and their difference\nc(\n  \"Avg. outcome on sale\" = Y1_mean,\n  \"Avg. outcome not on sale\" = Y0_mean,\n  \"Difference\" = Y1_mean  - Y0_mean\n)\n\n    Avg. outcome on sale Avg. outcome not on sale               Difference \n                     141                       63                       78 \n\n\nThe difference is even higher than the outcome for stores that did not implement a sales campaign. At this point, the alarm bells should ring - a simple comparison is very unlikely to yield a valid result.\nQuestion: What explanations can you think of that distort the relationship between the treatment variable and the outcome? Think back to what has been discussed in the lecture.\nAnswer\nThere might be one or several common causes (confounders). Two causes that affect both is_on_sale and weekly_amount_sold are the (1) business size (or: avg_week_sales) and the (2) time distance to Christmas (weeks_to_xmas). Because (1) larger businesses are more likely to implement sales campaigns and naturally sell more and because (2) sales are often implemented close to Christmas when customers buy anyway.\nSummarizing, there is no way to know the true causal effect of price cuts on units sold as we do not observe both worlds for all units: the world with price cuts and the world without price cuts. That’s what the fundamental problem of causal inference states. Throughout the whole course, we will come up with ways and methods to deal with this problem and get as close to the causal effect as possible.\nUnrealistic scenario\nFor now, let’s imagine the impossible and assume we can actually see both worlds and know both states for each observation. In potential outcomes (PO) notation, that means we can see both \\(Y_{i1}\\) and \\(Y_{i0}\\), where \\(0\\) and \\(1\\) refer to the treatment states for unit \\(i\\).\n\\[\nY_i=\\begin{cases}Y_{i1} \\ \\text{if unit i received treatment}\\\\Y_{i0} \\ \\text{otherwise}\\\\\\end{cases}\n\\]\nWhen you take a look at the table, you see the observed outcome y and both potential outcomes y0 and y1, one of which is the observed and the other one the counterfactual outcome. You also see a store identifier, the treatment status t, a covariate x and the individual treatment effect (\\(ITE\\)) te.\n\n# Read data  \"unrealistic\" scenario\nsales_unreal &lt;- read_csv(\"sales_unreal.csv\")\n\n# Print table\nprint(sales_unreal)\n\n# A tibble: 6 × 7\n      i    y0    y1     t     x     y    te\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   200   220     0     0   200    20\n2     2   120   140     0     0   120    20\n3     3   300   400     0     1   300   100\n4     4   450   500     1     0   500    50\n5     5   600   600     1     0   600     0\n6     6   600   800     1     1   800   200\n\n\nThe ITEs are computed as:\n\\[\n\\tau_i = \\text{ITE}_i = Y_{i1} - Y_{i0}\n\\]\nIn this unrealistic scenario, knowing all states, it is easy to compute the average treatment effect (\\(ATE\\)). For each unit, we subtract the untreated outcome from the treated outcome and take the average. Or even easier, because there are already computed ITEs in the data, we take the average of those.\n\nATE &lt;- mean(sales_unreal$y1 - sales_unreal$y0) # equivalent to: mean(sales_unreal$te)\nATE\n\n[1] 65\n\n\nThe true average causal treatment effect is 65. In formula notation, we calculated the sample equivalent of\n\\[\n\\text{ATE} = E[\\tau_i] = E[Y_{i1} - Y_{i0}] \\,.\n\\]\nWithout any problems, you could also calculate the conditional average treatment effect (\\(CATE\\)), i.e. the average treatment effect for units where the \\(X\\) takes on the specified value \\(x\\).\n\\[\nCATE = E[Y_{i1} - Y_{i0} | X = x]\n\\]\nRealistic scenario\nBut now let’s get back to the actual scenario: we just observe one outcome and we do not what would have happened in a different world. Consequently, the data looks like this:\n\n# Read \"realistic\" scenario\nsales_real &lt;- read_csv(\"sales_real.csv\")\n\n# Print table\nprint(sales_real)\n\n# A tibble: 6 × 7\n      i    y0    y1     t     x     y te   \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n1     1   200    NA     0     0   200 NA   \n2     2   120    NA     0     0   120 NA   \n3     3   300    NA     0     1   300 NA   \n4     4    NA   500     1     0   500 NA   \n5     5    NA   600     1     0   600 NA   \n6     6    NA   800     1     1   800 NA   \n\n\nRemember, the true causal effect is 65. Let’s check, how close we get when we try to estimate the ATE by comparing the treated observations to the untreated observations.\n\n# Average outcome for treated observations\ny1 &lt;- mean(sales_real[sales_real$t == 1, ]$y)\n\n# Average outcome for not treated observations\ny0 &lt;- mean(sales_real[sales_real$t == 0, ]$y)\n\n# Show both outcomes and their difference\nc(\n  \"Avg. treated outcomes\" = y1,\n  \"Avg. not treated outcomes\" = y0,\n  \"Difference\" = y1 - y0\n)\n\n    Avg. treated outcomes Avg. not treated outcomes                Difference \n                      633                       207                       427 \n\n\nIt’s \\(426.667\\) and thus very far off. Again, it proves the danger of taking naive averages and the inequality of association and causation. The reason here is that businesses engaged in sales are different from those that did not and would have sold more regardless of price cut.\nBias\nThe difference is also called bias and with full knowledge of all states can be calculated by:\n\\[\n\\begin{align}\nE[Y_1 - Y_0] &= E[Y|D=1] - E[Y|D=0] \\\\ &= E[Y_1|D=1] - E[Y_0|D=0] + E[Y_0|D=1] - E[Y_0|D=1] \\\\\n&= \\underbrace{E[Y_1 - Y_0|D=1]}_{ATT} + \\underbrace{\\{ E[Y_0|D=1] - E[Y_0|D=0] \\}}_{BIAS}\n\\end{align}\n\\]\nYou see that there is a bias when \\(E[Y_0|D=1]\\) is not equal to \\(E[Y_0|D=1]\\). That means, for no bias to occur, treated and untreated units only differ in their treatment status and is called the ignorability or exchangeability assumption.\nGoing back to the dataset with many observations, what can you infer from this plot? Does the plot indicate any violations of the assumptions?\n\n# Plot business size vs outcome\nggplot(\n  data = sales,\n  aes(x = avg_week_sales, \n      y = weekly_amount_sold, \n      color = is_on_sale) # different color depending on value of 'is_on_sale'\n  ) + geom_point(alpha = .5) # with alpha, we control point transparency",
    "crumbs": [
      "Content",
      "<b>Weekly content</b>",
      "1 - Introduction to Causal Inference"
    ]
  }
]