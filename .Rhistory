# Fit and return summary
dml_plr_obj$fit()
dml_plr_obj$summary()
# Specify task
dml_plr_obj <- DoubleMLPLR$new(
data =  email_dml_data, # data object
ml_l = lrnr_yx, # outcome model
ml_m = lrnr_dx, apply_cross_fitting = T # exposure model
)
# Fit and return summary
dml_plr_obj$fit()
dml_plr_obj$summary()
# Specify task
dml_plr_obj <- DoubleMLPLR$new(
data =  email_dml_data, # data object
ml_l = lrnr_yx, # outcome model
ml_m = lrnr_dx, # exposure model
apply_cross_fitting = TRUE,
n_folds = 10
)
# Fit and return summary
dml_plr_obj$fit()
dml_plr_obj$summary()
dml_aipw_obj = DoubleMLIRM$new(
data = email_dml_data,
ml_g = lrnr_yx,
ml_m = lrnr_dx,
score = "ATE",
#trimming_threshold = 0.01,
apply_cross_fitting = TRUE,
n_folds = 10)
dml_aipw_obj$fit()
dml_aipw_obj$summary()
dml_aipw_obj = DoubleMLIRM$new(
data = email_dml_data,
ml_g = lrnr_yx,
ml_m = lrnr_dx,
score = "ATE",
trimming_threshold = 0.15,
apply_cross_fitting = TRUE,
n_folds = 10)
dml_aipw_obj$fit()
dml_aipw_obj$summary()
# Specify task
dml_plr_obj <- DoubleMLPLR$new(
data =  email_dml_data, # data object
ml_l = lrnr_yx, # outcome model
ml_m = lrnr_dx, # exposure model
score = "ATE",
apply_cross_fitting = TRUE,
n_folds = 10
)
dml_aipw_obj = DoubleMLIRM$new(
data = email_dml_data,
ml_g = lrnr_yx,
ml_m = lrnr_dx,
score = "ATT",
#trimming_threshold = 0.01,
apply_cross_fitting = TRUE,
n_folds = 10)
dml_aipw_obj = DoubleMLIRM$new(
data = email_dml_data,
ml_g = lrnr_yx,
ml_m = lrnr_dx,
score = "ATTE",
#trimming_threshold = 0.01,
apply_cross_fitting = TRUE,
n_folds = 10)
dml_aipw_obj$fit()
dml_aipw_obj$summary()
dml_aipw_obj = DoubleMLIRM$new(
data = email_dml_data,
ml_g = lrnr_yx,
ml_m = lrnr_dx,
score = "ATE",
#trimming_threshold = 0.01,
apply_cross_fitting = TRUE,
n_folds = 10)
dml_aipw_obj$fit()
dml_aipw_obj$summary()
#| message: false
library(tidyverse)
#email <- read_csv("content/course_weeks/week_05/email_obs_data.csv") |> sample_n(1e+4)
email <- read_csv("email_obs_data.csv") |> sample_n(1e+4)
email <- read_csv("content/course_weeks/week_05/email_obs_data.csv") |> sample_n(1e+4)
# Frisch–Waugh–Lovell Theorem: 3-step procedure
# (1)
mod_D <- lm(mkt_email ~ ., data = select(email, -next_mnth_pv))
D_hat <- mod_D$residuals
# (2) Denoising:
mod_Y <- lm(next_mnth_pv ~ ., select(email, -mkt_email))
Y_hat <- mod_Y$residuals
# (3) Residual regression
mod_fwl <- lm(Y_hat ~ 0 + D_hat)
summary(mod_fwl)
#| message: false
#| warning: false
# Load packages
library(mlr3)
library(mlr3learners)
# Prediction model for the treatment/exposure
task_e <- as_task_classif(email |> select(-next_mnth_pv), target = "mkt_email")
lrnr_e <- lrn("classif.log_reg", predict_type = "prob")
# Prediction model for the outcome
task_m <- as_task_regr(email |> select(-mkt_email), target = "next_mnth_pv")
lrnr_m <- lrn("regr.lm")
# Initialize nuisance vectors
n <- nrow(email)
mhat <- rep(NA, n)
ehat <- rep(NA, n)
head(mhat)
# Split sample
ids_s1 <- sample(1:n, n/2) # indices of sample 1
ids_s2 <- which(!1:n %in% ids_s1) # indices of sample 2
# Train: S1 - Predict: S2
# Y ~ X
lrnr_m$train(task_m, row_ids = ids_s1)
mhat[ids_s2] <- lrnr_m$predict(task_m, row_ids = ids_s2)$response
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s1)
ehat[ids_s2] <- lrnr_e$predict(task_e, row_ids = ids_s2)$prob[,2]
# Train: S2 - Predict: S1
# Y ~ X
lrnr_m$train(task_m, row_ids = ids_s2)
mhat[ids_s1] <- lrnr_m$predict(task_m, row_ids = ids_s1)$response
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s2)
ehat[ids_s1] <- lrnr_e$predict(task_e, row_ids = ids_s1)$prob[,2]
lrnr_e$predict(task_e, row_ids = ids_s2)
lrnr_e$predict(task_e, row_ids = ids_s1)$prob[, 2]
# Train: S1 - Predict: S2
# Y ~ X
lrnr_m$train(task_m, row_ids = ids_s1)
mhat[ids_s2] <- lrnr_m$predict(task_m, row_ids = ids_s2)$response
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s1)
ehat[ids_s2] <- lrnr_e$predict(task_e, row_ids = ids_s2)$prob[, 2]
# Train: S2 - Predict: S1
# Y ~ X
lrnr_m$train(task_m, row_ids = ids_s2)
mhat[ids_s1] <- lrnr_m$predict(task_m, row_ids = ids_s1)$response
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s2)
ehat[ids_s1] <- lrnr_e$predict(task_e, row_ids = ids_s1)$prob[, 2]
ehat[ids_s1]
ehat[ids_s1] |> summary()
ehat[ids_s1] == 1
ehat[ids_s1][ehat[ids_s1] == 1]
ehat[ehat[ids_s1] == 1]
sum([ehat[ids_s1] == 1])
sum(ehat[ids_s1] == 1])
sum(ehat[ids_s1] == 1)
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s2)
ehat[ids_s2][ehat[ids_s2] == 1]
ehat |> max()
100*ehat |> max()
ehat==1
ehat[ehat==1]
ehat[ehat>.91]
lrnr_e$predict(task_e, row_ids = ids_s1)$
lrnr_e$predict(task_e, row_ids = ids_s1)
lrnr_e$predict(task_e, row_ids = ids_s1)$prob
lrnr_e$predict(task_e, row_ids = ids_s1)$prob |> head()
lrnr_e$predict(task_e, row_ids = ids_s1)$prob["1"]
lrnr_e$predict(task_e, row_ids = ids_s1)$prob[,"1"]
lrnr_e$predict(task_e, row_ids = ids_s1)$prob[,1]
lrn()
mlr3learners::lrn()
install.packages("mlr3extralearners")
remotes::install_github("mlr-org/mlr3extralearners")
library(mlr3learners)
#| message: false
library(tidyverse)
#email <- read_csv("content/course_weeks/week_05/email_obs_data.csv") |> sample_n(1e+4)
email <- read_csv("email_obs_data.csv") |> sample_n(1e+4)
email <- read_csv("content/course_weeks/week_05/email_obs_data.csv") |> sample_n(1e+4)
# Frisch–Waugh–Lovell Theorem: 3-step procedure
# (1)
mod_D <- lm(mkt_email ~ ., data = select(email, -next_mnth_pv))
D_hat <- mod_D$residuals
# (2) Denoising:
mod_Y <- lm(next_mnth_pv ~ ., select(email, -mkt_email))
Y_hat <- mod_Y$residuals
# (3) Residual regression
mod_fwl <- lm(Y_hat ~ 0 + D_hat)
summary(mod_fwl)
#| eval: false
install.packages("mlr3")
#| message: false
#| warning: false
# Load packages
library(mlr3)
library(mlr3learners)
# Prediction model for the treatment/exposure
task_e <- as_task_classif(email |> select(-next_mnth_pv), target = "mkt_email")
#lrnr_e <- lrn("classif.log_reg", predict_type = "prob")
lrnr_e <- lrn("classif.lightgbm", predict_type = "prob")
lrn()
library(mlr3extralearners)
lrn()
#| message: false
#| warning: false
# Load packages
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
# Prediction model for the treatment/exposure
task_e <- as_task_classif(email |> select(-next_mnth_pv), target = "mkt_email")
#lrnr_e <- lrn("classif.log_reg", predict_type = "prob")
lrnr_e <- lrn("classif.lightgbm", predict_type = "prob")
# Prediction model for the outcome
task_m <- as_task_regr(email |> select(-mkt_email), target = "next_mnth_pv")
#lrnr_m <- lrn("regr.lm")
lrnr_m <- lrn("regr.randomForest")
install_learners("classif.lightgbm")
#| message: false
#| warning: false
# Load packages
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
# Prediction model for the treatment/exposure
task_e <- as_task_classif(email |> select(-next_mnth_pv), target = "mkt_email")
#lrnr_e <- lrn("classif.log_reg", predict_type = "prob")
lrnr_e <- lrn("classif.lightgbm", predict_type = "prob")
# Prediction model for the outcome
task_m <- as_task_regr(email |> select(-mkt_email), target = "next_mnth_pv")
#lrnr_m <- lrn("regr.lm")
lrnr_m <- lrn("regr.randomForest")
install_learners("regr.randomForest")
mod_AB <- lm(next_mnth_pv ~ ., data = email_AB)
#| message: false
email_AB <- read_csv("content/course_weeks/week_05/email_rnd_data.csv")
mod_AB <- lm(next_mnth_pv ~ ., data = email_AB)
mod_AB <- lm(next_mnth_pv ~ mkt_email, data = email_AB)
summary(mod_AB)
#| message: false
#| warning: false
# Load package
library(DoubleML)
# Specify data object
email_dml_data <- DoubleMLData$new(
data = as.data.frame(email),
y_col = "next_mnth_pv",
d_cols = "mkt_email"
)
email
write_csv(email, "content/course_weeks/week_05/email_obs.csv")
email$mkt_email |> mean()
dml_aipw_obj
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = "center", fig.retina = 3, out.width = "75%")
set.seed(11)
options("digits" = 2, "width" = 150)
options(dplyr.summarise.inform = FALSE)
# custom ggplot theme
# colors from TUHH brand identitiy
tuhh_colors <- c("#D0D0CE", "#00C1D4", "#FF4F4F", "#5AFFC5",
"#FFDE36", "#143BFF", "#FF7E15", "#FFAEA2")
# initialise theme
cds_theme <- ggthemr::define_palette(
swatch = tuhh_colors,
gradient = c(lower = "#FFAEA2", upper = "#00C1D4"),
background = "#0F2231",
line = c("#FFFFFF", "#FFFFFF"),
text = c("#FFFFFF", "#FFFFFF"),
gridline = c(ggplot2::alpha("#D0D0CE", 0.2),
ggplot2::alpha("#D0D0CE", 0.4))
)
# set theme
ggthemr::ggthemr(cds_theme, type = "outer")
# source custom DAG theme
source(paste0(here::here(), "/code/dag_theme.R"))
here::here()
email <- read_csv("content/course_weeks/week_05/email_obs.csv")
library(tidyverse)
email <- read_csv("content/course_weeks/week_05/email_obs.csv")
# Frisch–Waugh–Lovell Theorem: 3-step procedure
# (1)
mod_D <- lm(mkt_email ~ ., data = select(email, -next_mnth_pv))
D_hat <- mod_D$residuals
# (2) Denoising:
mod_Y <- lm(next_mnth_pv ~ ., select(email, -mkt_email))
Y_hat <- mod_Y$residuals
# (3) Residual regression
mod_fwl <- lm(Y_hat ~ 0 + D_hat)
summary(mod_fwl)
#| eval: false
install.packages("mlr3")
#| message: false
#| warning: false
# Load packages
library(mlr3)
library(mlr3learners)
# Prediction model for the treatment/exposure
task_e <- as_task_classif(email |> select(-next_mnth_pv), target = "mkt_email")
lrnr_e <- lrn("classif.log_reg", predict_type = "prob")
# Prediction model for the outcome
task_m <- as_task_regr(email |> select(-mkt_email), target = "next_mnth_pv")
lrnr_m <- lrn("regr.lm")
# Initialize nuisance vectors
n <- nrow(email)
mhat <- rep(NA, n)
ehat <- rep(NA, n)
head(mhat)
# Split sample
ids_s1 <- sample(1:n, n/2) # indices of sample 1
ids_s2 <- which(!1:n %in% ids_s1) # indices of sample 2
head(ids_s1)
# Train: S1 - Predict: S2
# Y ~ X
lrnr_m$train(task_m, row_ids = ids_s1)
mhat[ids_s2] <- lrnr_m$predict(task_m, row_ids = ids_s2)$response
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s1)
ehat[ids_s2] <- lrnr_e$predict(task_e, row_ids = ids_s2)$prob[, 2] # col 2 for value D = 1
# Train: S2 - Predict: S1
# Y ~ X
lrnr_m$train(task_m, row_ids = ids_s2)
mhat[ids_s1] <- lrnr_m$predict(task_m, row_ids = ids_s1)$response
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s2)
ehat[ids_s1] <- lrnr_e$predict(task_e, row_ids = ids_s1)$prob[, 2]
# Residual-on-residual regression
# Obtain outcome and treatment residuals
Y <- email$next_mnth_pv
D <- email$mkt_email
res_Y <- Y - mhat
res_D <- D - ehat
# Run regression
mod_ds <- lm(res_Y ~ 0 + res_D)
summary(mod_ds)
ehat <- rep(NA,n)
m0hat <- rep(NA,n) # untreated
m1hat <- rep(NA,n) # treated
ids_d0 <- which(email$mkt_email==0)
ids_d1 <- which(email$mkt_email==1)
# Train in S1, predict in S2
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s1)
ehat[ids_s2] <- lrnr_e$predict(task_e, row_ids = ids_s2)$prob[, 2]
# Y0 ~ X
lrnr_m$train(task_m, row_ids = intersect(ids_s1, ids_d0))
m0hat[ids_s2] <- lrnr_m$predict(task_m, row_ids = ids_s2)$response
# Y1 ~ X
lrnr_m$train(task_m, row_ids = intersect(ids_s1, ids_d1))
m1hat[ids_s2] <- lrnr_m$predict(task_m, row_ids = ids_s2)$response
# Train in S2, predict in S1
# D ~ X
lrnr_e$train(task_e, row_ids = ids_s2)
ehat[ids_s1] <- lrnr_e$predict(task_e, row_ids = ids_s1)$prob[, 2]
# Y0 ~ X
lrnr_m$train(task_m, row_ids = intersect(ids_s2, ids_d0))
m0hat[ids_s1] <- lrnr_m$predict(task_m, row_ids = ids_s1)$response
# Y1 ~ X
lrnr_m$train(task_m, row_ids = intersect(ids_s2, ids_d1))
m1hat[ids_s1] <- lrnr_m$predict(task_m, row_ids = ids_s1)$response
# Potential outcomes with AIPW
Y_t_0 <- m0hat + (1-D) * (Y-m0hat) / (1-ehat)
Y_t_1 <- m1hat + D * (Y-m1hat) / ehat
# ATE
Y_ate <- Y_t_1 - Y_t_0
# Obtain statistical inference (same as t-test)
summary(lm(Y_ate ~ 1))
#| include: false
lgr::get_logger("mlr3")$set_threshold("warn")
#| message: false
#| warning: false
# Load package
library(DoubleML)
# Specify data object
email_dml_data <- DoubleMLData$new(
data = as.data.frame(email),
y_col = "next_mnth_pv",
d_cols = "mkt_email"
)
# Specify task
dml_plr_obj <- DoubleMLPLR$new(
data =  email_dml_data, # data object
ml_l = lrnr_m, # outcome model
ml_m = lrnr_e, # exposure model
apply_cross_fitting = TRUE,
n_folds = 10
)
# Fit and return summary
dml_plr_obj$fit()
dml_plr_obj$summary()
# Specify task
dml_aipw_obj = DoubleMLIRM$new(
data = email_dml_data,
ml_g = lrnr_m,
ml_m = lrnr_e,
score = "ATE",
trimming_threshold = 0.01, # to prevent too extreme weights
apply_cross_fitting = TRUE,
n_folds = 10)
# Fit and return summary
dml_aipw_obj$fit()
dml_aipw_obj$summary()
dml_aipw_obj$se
dml_aipw_obj$coef
dml_aipw_obj$draw_sample_splitting
dml_aipw_obj$psi_b
dml_aipw_obj$all_se
dml_aipw_obj$confint()
tibble(
c(dml_plr_obj$coef, dml_aipw_obj$coef)
)
tibble(
x = c(dml_plr_obj$coef, dml_aipw_obj$coef)
)
tibble(
x = c(dml_plr_obj$coef),
y = dml_aipw_obj$coef
)
tibble(
coef = c(dml_plr_obj$coef, dml_aipw_obj$coef),
se   = c(dml_plr_obj$se, dml_aipw_obj$se),
)
tibble(
estimator = c("PLR", "AIPW"),
coef = c(dml_plr_obj$coef, dml_aipw_obj$coef),
se   = c(dml_plr_obj$se, dml_aipw_obj$se),
)
mod_ds$coefficients
broom::tidy(mod_ds)
# Obtain statistical inference (same as t-test)
mod_aipw <- lm(Y_ate ~ 1)
summary(mod_aipw)
tibble(
estimator = c("PLR (by hand)", "AIPW (by hand)", "PLR", "AIPW"),
coef = c(broom::tidy(mod_plr)$estimate, broom::tidy(mod_aipw)$estimate, dml_plr_obj$coef, dml_aipw_obj$coef),
se   = c(broom::tidy(mod_plr)$std.error, broom::tidy(mod_aipw)$std.error, dml_plr_obj$se, dml_aipw_obj$se),
)
# Run regression
mod_plr <- lm(res_Y ~ 0 + res_D)
summary(mod_plr)
tibble(
estimator = c("PLR (by hand)", "AIPW (by hand)", "PLR", "AIPW"),
coef = c(broom::tidy(mod_plr)$estimate, broom::tidy(mod_aipw)$estimate, dml_plr_obj$coef, dml_aipw_obj$coef),
se   = c(broom::tidy(mod_plr)$std.error, broom::tidy(mod_aipw)$std.error, dml_plr_obj$se, dml_aipw_obj$se),
)
tibble(
estimator = c("PLR (by hand)", "AIPW (by hand)", "PLR", "AIPW"),
coef = c(broom::tidy(mod_plr)$estimate, broom::tidy(mod_aipw)$estimate, dml_plr_obj$coef, dml_aipw_obj$coef),
se   = c(broom::tidy(mod_plr)$std.error, broom::tidy(mod_aipw)$std.error, dml_plr_obj$se, dml_aipw_obj$se)
)
tibble(
estimator = c("PLR (by hand)", "AIPW (by hand)", "PLR (DoubleML)", "AIPW (DoubleML)"),
coef = c(broom::tidy(mod_plr)$estimate, broom::tidy(mod_aipw)$estimate, dml_plr_obj$coef, dml_aipw_obj$coef),
se   = c(broom::tidy(mod_plr)$std.error, broom::tidy(mod_aipw)$std.error, dml_plr_obj$se, dml_aipw_obj$se)
)
# Specify task
dml_plr_obj <- DoubleMLPLR$new(
data =  email_dml_data, # data object
ml_l = lrnr_m, # outcome model
ml_m = lrnr_e, # exposure model
apply_cross_fitting = TRUE,
n_folds = 2
)
# Fit and return summary
dml_plr_obj$fit()
dml_plr_obj$summary()
# Table of all estimates and standard errors
tbl_coef <- tibble(
estimator = c(
"PLR (by hand)",
"AIPW (by hand)",
"PLR (DoubleML)",
"AIPW (DoubleML)"
),
coef = c(
broom::tidy(mod_plr)$estimate,
broom::tidy(mod_aipw)$estimate,
dml_plr_obj$coef,
dml_aipw_obj$coef
),
se   = c(
broom::tidy(mod_plr)$std.error,
broom::tidy(mod_aipw)$std.error,
dml_plr_obj$se,
dml_aipw_obj$se
)
)
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_pointrange()
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_pointrange() +
geom_errorbar()
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_errorbar() +
ylim(c(0,1500))
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_errorbar() +
ylim(c(1200,1400))
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_errorbar() +
ylim(c(1100,1400))
?geom_errorbar
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_errorbar(width = .2) +
ylim(c(1100, 1400))
# Plot
ggplot(tbl_coef, aes(x = estimator, y = coef, ymin = coef - 1.96*se, ymax = coef + 1.96*se)) +
geom_errorbar(width = .5) +
ylim(c(1100, 1400))
