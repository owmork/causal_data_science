---
title: "12 - Synthetic Control (2/2)"
linktitle: "12 - Synthetic Control (2/2)"
output:
  blogdown::html_page:
    toc: true
menu:
  example:
    parent: Course content
    weight: 1
type: docs
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = "center", fig.retina = 3, out.width = "75%")
set.seed(11)
options("digits" = 2, "width" = 150)
options(dplyr.summarise.inform = FALSE)

# custom ggplot theme
# colors from TUHH brand identitiy
tuhh_colors <- c("#D0D0CE", "#00C1D4", "#FF4F4F", "#5AFFC5",
                 "#FFDE36", "#143BFF", "#FF7E15", "#FFAEA2")

# initialise theme
cds_theme <- ggthemr::define_palette(
  swatch = tuhh_colors,
  gradient = c(lower = "#FFAEA2", upper = "#00C1D4"),
  background = "#0F2231",
  line = c("#FFFFFF", "#FFFFFF"),
  text = c("#FFFFFF", "#FFFFFF"),
  gridline = c(ggplot2::alpha("#D0D0CE", 0.2), 
               ggplot2::alpha("#D0D0CE", 0.4))
)

# set theme
ggthemr::ggthemr(cds_theme, type = "outer")

# source custom DAG theme
source(paste0(here::here(), "/code/dag_theme.R"))
```

# Slides

<iframe style="width: 100%; height: 45vw; max-height: 50vh;" frameborder="0" allowfullscreen src="https://tuhhstartupengineers-classroom.github.io/ss24-causal-data-science/slides/09_synth_c.html">

# Practical example

## Augmented synthetic control

We revisit the [data](https://cloud.tuhh.de/index.php/s/HSnCpJt9C98Cqs7) example from last week (slightly changed and trimmed) and apply a more advanced method for synthetic control estimation, known as the augmented synthetic control, proposed by [Ben-Michael, Feller, and Rothstein, 2021](https://arxiv.org/pdf/1811.04170). This doubly robust estimator combines elements from both the synthetic control estimator and an outcome model estimator. While it does not strictly prohibit negative weights, regularization is used to minimize their occurrence. The general idea is to assess the pre-treatment imbalance using an outcome model and then reweight the synthetic control with the new weights.

$$
\begin{align*}
\hat{Y}_{1T}^{\text{aug}}(0) &= \overbrace{\sum_{i=2}^{N+1} \hat{w}_i^{\text{sc}} Y_{iT}}^{\text{SC estimate}} + \overbrace{\left( \hat{m}_{1T} - \sum_{i=2}^{N+1} \hat{w}_i^{\text{sc}} \hat{m}_{iT} \right)}^{\text{imbalance correction}} \\
&= \underbrace{\hat{m}_{1T}}_{\text{outcome model}} + \underbrace{\sum_{i=2}^{N+1} \hat{w}_i^{\text{sc}} (Y_{iT} - \hat{m}_{iT})}_{\text{"IPW-like" re-weights to balance residuals}}
\end{align*}
$$
Let's use the `augsynth()` method from the `augsynth` package for estimation. You have to install it from GitHub, so please run the following command.

```{r}
#| eval: false

remotes::install_github("ebenmichael/augsynth")
```

```{r}
#| message: false
#| warning: false

library(tidyverse)

ride <- readRDS("ride_12_single.rds")

print(head(ride))
```

You can specify whether you want to run an outcome model and choose its form. If you opt not to use any model, the basic synthetic control method estimates will be provided. First, we will run the analysis without an outcome model, and then we will use ridge regression. However, other methods are also available. By typing `?augsynth` into your console, you can see the additional options. Apart from this, the arguments are similar to those you have seen in previous weeks.

[**Task 1:**]{style="color:#FF7E15"} Fill out the code below and report the results. Do it once without and once without outcome model. What is the difference? Please note, that the inference takes some time.

```{r}
#| eval: false

library(augsynth)

att_sc <- augsynth(
  form = ...,
  # outcome ~ treatment | auxiliary covariates
  unit = ...,
  time = ...,
  data = ...,
  #progfunc = "Ridge",#  function to use to impute control outcomes (or "None")
  scm = TRUE
) # whether to use the SCM

sc_summ <- summary(att_sc)
plot(sc_summ)
```

```{r}
#| include: false

library(augsynth)

att_sc <- augsynth(
  revenue ~ treat_post | gdp + population + log(gdp),
  unit = city,
  time = period,
  data = ride,
  progfunc = "None",
  scm = TRUE
)

sc_summ <- summary(att_sc)
sc_summ
plot(sc_summ)
```

```{r}
#| include: false

att_augsc <- augsynth(
  revenue ~ treat_post | gdp + population + log(gdp),
  unit = city,
  time = period,
  data = ride,
  progfunc = "Ridge",
  scm = TRUE
)

augsc_summ <- summary(att_augsc)
augsc_summ
plot(augsc_summ)
```


## Staggered timing

Historically, synthetic control methods have been applied to single treated units, while difference-in-differences estimators were typically used for multiple treated units and staggered treatments. However, if we are unwilling to assume parallel trends and prefer to use the synthetic control approach, we can utilize the `augsynth` package again.

But first, let's get the [data](https://cloud.tuhh.de/index.php/s/RJFyT6PFqNTEmwt) where the new feature for the ride sharing platform were rolled out at multiple cities at different points of time and explore the data. A package that helps with visualization of staggered rollouts is `panelView`.

```{r}
#| message: false
#| warning: false

library(tidyverse)

ride_multi <- readRDS("ride_12_multi.rds")

print(head(ride_multi))
```

[**Task 2:**]{style="color:#FF7E15"} Read through the [tutorial](https://yiqingxu.org/packages/panelview/articles/tutorial.html) and reproduce the following plots. Describe what you can see.

```{r}
#| include: false
#| message: false
#| warning: false

library("panelView")

panelview(revenue ~ post, 
          data = ride_multi,
          index = c("city","period"), 
          xlab = "Period", 
          ylab = "City",
          cex.axis = 3,
          by.timing = TRUE,
          pre.post = TRUE
          )
```

```{r}
#| include: false
#| message: false
#| warning: false

panelview(
  revenue ~ post,
  data = ride_multi,
  index = c("city","period"),
  cex.axis = 3,
  type = "outcome",
  main = "Revenue",
  xlab = "Period",
  ylab = "Revenue"
)

```

```{r}
#| include: false
#| message: false
#| warning: false

panelview(
  revenue ~ post,
  data = ride_multi,
  index = c("city","period"), 
  cex.axis = 3,
  type = "outcome",
  main = "Revenue",
  xlab = "Period",
  ylab = "Revenue", 
  id = c("Chicago", "Phoenix", "Las Vegas", "Seattle", "Houston", "Washington"),
  by.group = TRUE
)
```

However, the problem that arises is that aggregate multiple synthetic controls into a single parameter estimate. We need to find the balance between simply estimating a synthetic control for each treated unit and pooled synthetic control for all treated units. The answer is the partially-pooled synthetic control that minimizes a weighted average of the two imbalances and includes an intercept to account for level differences.

Equation highlights two equivalent interpretations: average of unit-specific SCM estimates or SCM estimate of the average treated unit.

$$
\begin{aligned}
\hat{\tau}_k = \frac{1}{J} \sum_{j=1}^J \hat{\tau}_{jk} &= \frac{1}{J} \sum_{j=1}^J \left[ Y_{jT_j + k} - \sum_{i=1}^N \hat{w}_{ij} Y_{iT_j + k} \right] \\
&= \frac{1}{J} \sum_{j=1}^J Y_{jT_j + k} - \sum_{i=1}^N \frac{1}{J} \sum_{j=1}^J \hat{w}_{ij} Y_{iT_j + k}
\end{aligned}
$$

Unit-specific imbalance:

$$
q_X^{\text{sep}}(\mathbf{W}) = \sqrt{\frac{1}{J} \sum_{j=1}^J \left\| \mathbf{X}_j - \sum_{i=1}^N w_{ij} \mathbf{X}_i \right\|_2^2}
$$

Pooled imbalance:

$$
q_X^{\text{pool}}(\mathbf{W}) = \left\| \frac{1}{J} \sum_{j=1}^J \mathbf{X}_j - \sum_{i=1}^N w_{ij} \mathbf{X}_i \right\|_2
$$
Weighted average of imbalances:

$$
\mathbf{W}^* = \arg\min_{\mathbf{W}} \left( \nu \left( \tilde{q}^{\text{pool}}(\mathbf{W}) \right)^2 + (1 - \nu) \left( \tilde{q}^{\text{sep}}(\mathbf{W}) \right)^2 + \lambda \| \Gamma \|_F^2 \right)
$$

```{r}
#| warning: false
#| message: false

# Synthetic control with multiple treatment periods
att_msc <- multisynth(revenue ~ post | gdp + population + log(gdp), 
                             unit = city, 
                             time = period, 
                             data = ride_multi, 
                             scm = T)

msc_summ <- summary(att_msc) 
plot(msc_summ, levels = "Average")
```

## Synthetic Difference-in-Differences

Synthetic Difference in Differences (SDID) is a method that incorporates both time and unit weights, as well as unit fixed effects, to achieve parallel trends by weighting control observations. SDID combines features of Difference in Differences (DID) and Synthetic Control (SC). It reweights and matches pre-exposure trends, similar to SC, and is invariant to additive unit-level shifts, making it valid for large-panel inference, like DID. SDID provides consistent and asymptotically normal estimates, performing as well as or better than DID in traditional DID settings. Unlike DID, which only handles completely random treatment assignment, SDID can manage cases where treatment assignment correlates with some time or unit latent factors. Similarly, SDID is as effective as or better than SC in traditional SC settings. While uniformly random treatment assignment results in unbiased outcomes for all methods, SDID is more precise.

SDID's double robustness is comparable to the augmented inverse probability weighting estimator proposed by Scharfstein, Rotnitzky, and Robins (1999) and is similar to the augmented SC estimator by Ben-Michael, Feller, and Rothstein (2021) and Arkhangelsky et al. (2021).

$$
\hat{\tau}^{sdid} = \arg \min_{\tau, \mu, \alpha, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_ t - D_{it} \tau)^2 \hat{w}_i^{sdid} \hat{\lambda}_t^{sdid} \}
$$
For now, let's go back to the first setting, where we had only one treated unit. Let's build the 4 blocks as we did last week.

```{r}
# Split and extract 4 matrices
y0_pre <- ride |>
  filter(post == 0, treated == 0) |> 
  pivot_wider(id_cols = "period", names_from = "city", values_from = "revenue") |> 
  select(-period) |> 
  as.matrix()

y1_pre <- ride |>
  filter(post == 0, treated == 1) |> 
  pivot_wider(id_cols = "period", names_from = "city", values_from = "revenue") |> 
  select(-period) |> 
  as.matrix()

y0_post <- ride |>
  filter(post == 1, treated == 0) |> 
  pivot_wider(id_cols = "period", names_from = "city", values_from = "revenue") |> 
  select(-period) |> 
  as.matrix()

y1_post <- ride |>
  filter(post == 1, treated == 1) |> 
  pivot_wider(id_cols = "period", names_from = "city", values_from = "revenue") |> 
  select(-period) |> 
  as.matrix()
```

Step-by-step, we build a synthetic DiD estimator, which slightly differs from the one proposed by [Arkhangelsky et al.](https://arxiv.org/pdf/1812.09970), but gives you the intuition of the inner workings.

The first step is just to get the unit weights as we did for the synthetic control method last week.

```{r}
#| warning: false
#| message: false

library(CVXR)

# Optimization procedure
w <- Variable(ncol(y0_pre))
objective <- Minimize(sum_squares(y0_pre %*% w - y1_pre))
constraints <- list(sum(w) == 1, w >= 0)
problem <- Problem(objective, constraints)
w_star <- as.vector(solve(problem)$getValue(w))
```

Now, instead of retrieving the ATT via matrix multiplication, we store the unit weights in our data frame. For the treated unit, the weights we store are just the average of the treatment share.

```{r}
unit_w <- tibble(city = colnames(y0_pre), unit_weight = w_star)

ride_w <- ride |> 
  left_join(unit_w, by = "city") |> 
  mutate(unit_weight = replace_na(unit_weight, mean(ride$treated)))
```

With this data, we could compute an equivalent ATT by running a regression with time fixed effects that the ATT we got from matrix multiplication.

```{r}
mod_w <- lm(revenue ~ treat_post + factor(period),
          data = ride_w |> filter(unit_weight > 1e-10), 
          weights = unit_weight)
mod_w$coefficients[2]
mean(y1_post - (y0_post %*% w_star))
```

However, we don't want to have the estimate from the synthetic control method, but we want to additionally exploit information of the time dependence and get time weights. Therefore, we want to regress the average of the post-treatment outcome on the pre-treatment outcomes of the control group. Similar as before, this yields time weights for the pre-treatment period, i.e. the weight we should put on each pre-treatment period when constructing our estimator. 

While we again restrict, the weights to be non-negative and sum to 1, we also allow for the inclusion of an intercept. Therefore, we'll just add a column of ones to the matrix.

$$
\{\hat{\lambda}_0, \hat{\lambda}\} = \sum_{i = 1}^{N_c}(\lambda_0 + \sum_{t = 1}^{T_{pre}} \lambda_t Y_{it} - \frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it})^2
$$

[**Task 3:**]{style="color:#FF7E15"} Run the regression to obtain the time weights. (Hint: you might need `t()` and `colMeans()`).

```{r}
#| include: false

# Initialize weights and add intercept
t_y0_pre <- t(y0_pre) # transpose to get time coefficients/weights
l <- Variable(ncol(t_y0_pre) + 1) # one more column due to intercept
icept <- matrix(rep(1, nrow(t_y0_pre)), ncol = 1) # create intercept
t_y0_pre <- cbind(icept, t_y0_pre) # add intercept to matrix

objective <- Minimize(sum_squares(t_y0_pre %*% l - colMeans(y0_post)))
constraints <- list(sum(l[2:ncol(t_y0_pre)]) == 1, l[2:ncol(t_y0_pre)] >= 0)
problem <- Problem(objective, constraints)
l_star <- as.vector(solve(problem)$getValue(l))
```


```{r}
#| include: false

# Remove intercept
l_star <- l_star[2:ncol(t_y0_pre)]

# Table with period weights
pre_periods <- ride |> filter(post == 0, treated == 0) |> pull(period) |> unique()
time_w <- tibble(period = pre_periods, time_weight = l_star)

# Show weights
round(l_star, 2)
```

Now, we also add the time weights to our data.frame. For post-treatment periods, we take the share of the post periods.

Having obtained both unit and time weights, we multiply the weights to get the final weight each observation gets in the weighted regression.

```{r}
# Add weights to data
ride_wl <- ride_w |> 
  left_join(time_w, by = "period") |> 
  mutate(time_weight = replace_na(time_weight, mean(ride$post))) |> 
  mutate(weight = unit_weight * time_weight)
```

Running the regression yields a synthetic DiD estimate.

[**Task 4:**]{style="color:#FF7E15"} Run the regression and return the estimated treatment effect.

```{r}
# Final regression
mod_wl <- lm(revenue ~ treat_post + factor(city) + factor(period), 
   data = ride_wl |> filter(weight > 1e-10), 
   weights = weight)

mod_wl$coefficients[2]
```

In practice, the `synthdid` package does all the work. But please note, that this package is still at an early stage. It is a bit more advanced that what we just did, but the general idea is the same. It allows an intercept shift and adds an L2 penalty on the unit weights to make sure no control units gets a too large weight.

```{r}
library(synthdid)

setup = panel.matrices(
  panel = as.data.frame(ride),
  unit = "city",
  time = "period",
  outcome = "revenue",
  treatment = "treat_post",
  treated.last = TRUE
)
tau.hat <- synthdid_estimate(setup$Y, setup$N0, setup$T0)
se <- sqrt(vcov(tau.hat, method='placebo'))
sprintf('point estimate: %1.2f', tau.hat)
sprintf('95%% CI (%1.2f, %1.2f)', tau.hat - 1.96 * se, tau.hat + 1.96 * se)
plot(tau.hat)
```

# Assignment

Coming soon.

::: assignment
...
:::
