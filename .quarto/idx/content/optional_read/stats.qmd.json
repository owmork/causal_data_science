{"title":"Statistical Concepts","markdown":{"yaml":{"title":"Statistical Concepts","linktitle":"Statistical Concepts","output":{"blogdown::html_page":{"toc":true}},"menu":{"example":{"parent":"Optional read","weight":2}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"headingText":"custom ggplot theme","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = \"center\", fig.retina = 3, out.width = \"75%\")\nset.seed(11)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n\n# colors from TUHH brand identitiy\ntuhh_colors <- c(\"#D0D0CE\", \"#00C1D4\", \"#FF4F4F\", \"#5AFFC5\",\n                 \"#FFDE36\", \"#143BFF\", \"#FF7E15\", \"#FFAEA2\")\n\n# initialise theme\ncds_theme <- ggthemr::define_palette(\n  swatch = tuhh_colors,\n  gradient = c(lower = \"#FFAEA2\", upper = \"#00C1D4\"),\n  background = \"#0F2231\",\n  line = c(\"#FFFFFF\", \"#FFFFFF\"),\n  text = c(\"#FFFFFF\", \"#FFFFFF\"),\n  gridline = c(ggplot2::alpha(\"#D0D0CE\", 0.2), \n               ggplot2::alpha(\"#D0D0CE\", 0.4))\n)\n\n# set theme\nggthemr::ggthemr(cds_theme, type = \"outer\")\n\n# source custom DAG theme\nsource(\"../../code/dag_theme.R\")\n```\n\nNow we will delve into some statistical concepts, that are the foundation for statistical modeling processes used in causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to read [here](https://seeing-theory.brown.edu/basic-probability/index.html).\n\n# Random Variable\n\nWe already touched random variables in the last chapter but for starters, let's again define what it is. Often represented by a letter such as $X$, a random variable has a set of values, also called sample space, of which any could be the outcome if we draw a random sample from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). Because there is randomness and chance in the outcome, we call it a **random** variable.\n\nA random variable can either take on discrete (e.g. die) or continuous values (e.g. average height of individuals).\n\n-   **Discrete Random Variable**: it can take on a countable number of distinct values.\n\n    -   $X \\in \\{1, 2, 3, 4, 5, 6\\}$\n\n    -   $P(X=1) = P(X=2) = … = P(X=6) = \\frac{1}{6}$\n\n-   **Continuous Random Variable**: it can take on any value within a certain range or interval.\n\n    -   $X \\in [10, 250]$ (size in centimeters)\n\n    -   Typically presented using cumulative distribution function returning the probability of a variable being less than or equal to a specific value: $P(X < b),$ where $b$ could be any value in the given interval.\n\n# Expected Value\n\nA random variable is a real-valued function and has more than one possible outcome. This is why we cannot represent it as a scalar. The expected value of random variable, however, is a scalar and represents something like a \"summary\" of the random variable. Knowing the possible values (from the sample space) and the probability distribution, we can compute the expected value.\n\n::: callout-important\nThe *expected value*, also called *population mean*, of a variable $X$ is defined as the weighted average of possible values the random variable can take, where the weight is equal to the probability of the random variable taking on a specific value.\n:::\n\nIf we want to compute it for a discrete random variable, we make use of the summation operator. Considering a finite list of potential values $x_1, x_2, …, x_k$ with probabilities $p_1, p_2, …, p_k$, the expectation of $X$ can be computed by\n\n$$\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n$$\n\n::: callout-tip\nThe summation operator $\\sum$, denoted by the Greek capital Sigma is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, $x_1, x_2, …, x_n$ to a shorter and more readable form\n\n$$\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n$$\n\nwith the arbitrary index of summation $i$ being the lower limit and $n$ the upper limit.\n\nBy basic math rules, the following simplifications are possible, where $c$ is a constant:\n\n$$\n\\sum_{i=1}^nc=nc\n$$\n\nand\n\n$$\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n$$\n:::\n\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\n$$\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n$$\n\nProbabilities could also be different, as long as they sum to 1. For continuous random variables, we need a function returning the probabilities for each value. We leave that out here as we are only interested in understanding the intuition behind the expected value. However, it is very similar.\n\nLet's type in the probabilities and outcomes of rolling a die and see what the expected value is.\n\n::: callout-note\n| To replicate values and create a vector `rep()` can be used. It takes the value and number of times it should be replicated.\n:::\n\n```{r}\n# Vector of probabilities (all equal)\np <- rep(1/6, times = 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n```\n\nAs you might have expected, it's 3.5. In this case, we know the probabilities, but if we had not known them beforehand, what we could have done to get an estimate of the expected value is to roll the die a lot of times, store the results in an object and use the `mean()` function. It would yield an expected value close to 3.5 (see simulation [here](https://seeing-theory.brown.edu/basic-probability/)).\n\n::: callout-tip\nAdditional rules regarding the calculation of expected values that can be useful are:\n\n$$\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n$$\n:::\n\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc.\n\n# Conditional Expected Value\n\nThe conditional expected value is the expected value conditioned on some other value. Given the value $x$ for $X$, the expected value for $Y$ obtains as\n\n$$\nE[Y|X = x]\n$$\n\nand is a function of $x$. In other words, the conditional expected value is the best guess for $Y$ knowing only that $X=x$.\n\nAs a simple example, consider we take a representative random sample from the world population and want to compute the expected value for $height$. Denoting height with $Y$, the expected value for the whole population is the expected value $E[Y]$ over all individuals in your sample.\n\nThe conditional expected value, however, differs. For example, conditioned on individuals being younger than ten years or older than than that we expect different values.\n\n$$\nE[Y] \\neq E[Y|age < 10] \\neq E[Y|age >= 10]\n$$\n\nInstead of $age$, we can also choose another variable, e.g. $gender$. According to NCD[^1], the values in centimeter are as follows:\n\n[^1]: https://ourworldindata.org/human-height\n\n$$\n\\begin{aligned}\nE[Y|male] &= 171 \\\\\nE[Y|female] &= 159\n\\end{aligned}\n$$\n\nAnd the overall mean results as a (weighted) average between males and females.\n\n# Variance\n\nThe variance of random variable tells us how much the values of that random variable tend to spread out or vary from the expected value. Low variance means that the values are closer to the expected value, while high variance indicates values are more spread out from the expected value.\n\nMathematically, the *variance* is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The *sample variance* indicates how far a set of observed values spread out from their average value and is an estimate of the full *population variance*, that in most cases cannot be directly observed due to lack of data of the whole population.\n\nMathematically, the *population variance* is defined as\n\n$$\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n$$\n\nand the *sample variance* results as\n\n$$\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n$$\n\nYou might have noticed the term $(n-1)^{-1}$ is different from what you probably expected ($n^{-1}$). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\n\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution. The **standard deviation** obtains as the square root of the variance:\n\n$$\n\\sigma = \\sqrt{\\sigma^2}\n$$Take a look at this interactive histogram of draws from the normal distribution. You can set the mean (or: expected value) and the standard deviation. See how a high standard deviation leads to larger proportions of the data being far away from the expected value.\n\n```{ojs}\n//| echo: false\n//| panel: input\n\nviewof mean = Inputs.range(\n  [-5, 5], \n  {value: 1, step: 1, label: \"Expected value:\"}\n)\n\nviewof sd = Inputs.range(\n  [1, 20], \n  {value: 1, step: 1, label: \"Standard deviation:\"}\n)\n\n```\n\n```{ojs}\n//| echo: false\n//| fig-align: center\n\nmutable xDomain = [-50, 50]\nmutable yDomain = [0, 1000]\n\nPlot.plot({\n  ...theme,\n  //marginRight: 0,\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.rectY({length: 10000}, Plot.binX({y: \"count\"}, {x: d3.randomNormal(mean, sd),  fill: \"#00C1D4\"})),\n  ],\n  x: {domain: xDomain},\n  y: {domain: yDomain}\n})\n\n```\n\n::: callout-note\n| `rnorm(n, mean, sd)` samples values from the normal distribution. `n` specifies the number of values, `mean` and `sd` define the parameters of the normal distribution.\n\n| To compute variance and standard deviation, use `var()` and `sd()`, respectively.\n:::\n\n```{r}\nX_low_var  <- rnorm(1e+4, mean = 0, sd = 1)   # low variance\nX_high_var <- rnorm(1e+4, mean = 0, sd = 10)  # high variance\n\nprint(paste(\"Low variance case. Variance =\", var(X_low_var), \"Standard deviation =\", sd(X_low_var)))\nprint(paste(\"High variance case. Variance =\", var(X_low_var), \"Standard deviation =\", sd(X_low_var)))\n```\n\n::: callout-tip\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of $a$, then the variance will increase by $a^2$.\n\n$$\nVar(aX+b)=a^2V(X)\n$$\n\nYou can also conveniently compute a variance for the sum of two random variables\n\n$$\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n$$\n\nwhich in case of independence reduces to the sum of the individual variances due to the fact that $E(XY) = E(X)E(Y)$.\n:::\n\n# Variance of a Conditional Expected Value\n\nLet's take another look at variance but now we'll see we focus on the case of a conditional expected value - to be precise, the value of Y conditional on values of X. Playing with different values for the standard deviation in the graph below, you should notice that while the line that fits the data best (the line that is on average the closest to all data points - we will talk about it more extensively in the next chapter) stays very similar, the dispersion of the data points significantly changes.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\n\n# Create a data frame with correlated X and Y values\ncreate_corr_df <- function(n = 3e+3, correlation = .3, sd = 1) {\n  # Generate random X and Y data\n  X <- rnorm(n)\n  Y <- correlation * X + sqrt(1 - correlation^2) * rnorm(n, mean = 0, sd = sd)\n  \n  # Create a data frame\n  df <- data.frame(x = X, y = Y) %>%\n    add_column(\n      sigma = correlation,\n      sd = sd)\n  \n  # Return data frame\n  return(df)\n}\n\n# Generate data for sequence of different levels of standard deviation\nsd_range <- seq(0, 1, .05)\nsd_tbl <- map(sd_range, function(sd) create_corr_df(sd = sd)) %>% bind_rows()\n\n# Define as Observable JS data\nojs_define(sd_data = sd_tbl)\n```\n\n```{ojs}\n//| echo: false\n//| panel: input\n\nviewof std = Inputs.range(\n  [0, 1], \n  {value: 0.5, step: 0.05, label: \"Standard deviation:\"}\n)\n\n```\n\n```{ojs}\n//| echo: false\n//| panel: center\n\ntheme = ({\n  style: \"background-color: #0F2231\",\n  fill: \"#00C1D4\"\n})\n\nmutable xDomain_sd = [-3, 3]\nmutable yDomain_sd = [-3, 3]\n\nPlot.plot({\n  ...theme,\n  //marginRight: 0,\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.dot(sd_filtered, {x: \"x\", y: \"y\", fill: \"#00C1D4\", fillOpacity: .4}),\n    Plot.linearRegressionY(sd_filtered, {x: \"x\", y: \"y\", stroke: \"#FF4F4F\"})\n  ],\n  x: {domain: xDomain_sd},\n  y: {domain: yDomain_sd}\n})\n\n```\n\n```{ojs}\n//| echo: false\nsd_filtered = transpose(sd_data).filter(function(sd_tbl) {\n  return std == sd_tbl.sd;\n})\n```\n\n# Covariance and Correlation\n\nTwo more important concepts to understand variability in data are the related concepts of covariance and correlation. Covariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction. A high, positive covariance indicates that two random variables move into the same direction, a high, negative covariance means that two random variables move in opposite directions.\n\nIf you remember the rules of probability theory from the previous chapter, you won't be surprised that if the equality $E(XY) = E(X)E(Y)$ holds, it implies a *covariance* of 0 between the variables $X$ and $Y$. *Covariance* is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, we can define the variance of a sum of two random variables as sum of the individual variances of both random variables plus two times their covariance.\n\nAs a matter of form, the formula for the covariance of the random variables $X$ and $Y$ is\n\n$$\nCov(X,Y) = E(XY) - E(X)E(Y) = E[(X-\\bar{X})(Y - \\bar{Y})]\n$$\n\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the *correlation* which can be derived from the covariance if the individual variances are known.\n\n$$\n\\text{Corr}(X,Y) = \\dfrac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n$$ The *correlation* is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign.\n\n```{r}\n#| echo: false\n\n# Generate data for sequence of different degrees of correlation\ncorrelation_range <- seq(-1, 1, .1)\ncorr_tbl <- map(correlation_range, function(c) create_corr_df(correlation = c)) %>% bind_rows()\n\n# Define as Observable JS data\nojs_define(data = corr_tbl)\n```\n\nSet the degree of correlation to understand how that changes the relationship between both variables.\n\n```{ojs}\n//| echo: false\n//| panel: input\n\nviewof sigma = Inputs.range(\n  [-1, 1], \n  {value: 0, step: 0.1, label: \"Degree of correlation:\"}\n)\n\n```\n\n```{ojs}\n//| echo: false\n//| panel: center\n\nPlot.plot({\n  ...theme,\n  //marginRight: 0,\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.dot(filtered, {x: \"x\", y: \"y\", fill: \"#00C1D4\", fillOpacity: .4}),\n    Plot.linearRegressionY(filtered, {x: \"x\", y: \"y\", stroke: \"#FF4F4F\"})\n  ]\n})\n\n```\n\n```{ojs}\n//| echo: false\nfiltered = transpose(data).filter(function(corr_tbl) {\n  return sigma == corr_tbl.sigma;\n})\n```\n\n::: callout-note\n| To compute correlation, variance and covariance, respectively, you can use `cor(X, Y)`, and `cov(X, Y)`.\n:::\n\n# Conclusion\n\nMany of the rules and concepts that you have just learned will play a crucial in the upcoming chapters. Their understanding will guide you through and let you understand why we need to put a particular emphasis on causality, how we can isolate causal effects and build the foundation for many methods from our toolbox.\n\n# Assignments\n\n::: assignment\nLoad the table `random_vars.rds`. You can either\n\n-   run the command: `random_vars <- readRDS(\"your_download_folder/random_vars.rds\")`\n-   or find the file in the `Files` pane and select it.\n\nAfter you load the table, get an overview of what it contains with `View(random_vars)`.\n\n1.  For each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n    1.  expected value\n    2.  variance\n    3.  standard deviation\n2.  Explain, if it makes sense to compare the standard deviations.\n3.  Then, examine the relationship between both variables and compute:\n    1.  covariance\n    2.  correlation\n4.  What measure is easier to interpret? Please discuss your interpretation.\n5.  Compute the conditional expected value for:\n    1.  $E[income|age <= 18]$\n    2.  $E[income|age \\in [18, 65)]$\n    3.  $E[income|age >= 65]$\n:::\n\n::: callout-tip\nIn the introduction you learned how to filter data. There are different ways, you could either use base R or `tidyverse`.\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = \"center\", fig.retina = 3, out.width = \"75%\")\nset.seed(11)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n\n# custom ggplot theme\n# colors from TUHH brand identitiy\ntuhh_colors <- c(\"#D0D0CE\", \"#00C1D4\", \"#FF4F4F\", \"#5AFFC5\",\n                 \"#FFDE36\", \"#143BFF\", \"#FF7E15\", \"#FFAEA2\")\n\n# initialise theme\ncds_theme <- ggthemr::define_palette(\n  swatch = tuhh_colors,\n  gradient = c(lower = \"#FFAEA2\", upper = \"#00C1D4\"),\n  background = \"#0F2231\",\n  line = c(\"#FFFFFF\", \"#FFFFFF\"),\n  text = c(\"#FFFFFF\", \"#FFFFFF\"),\n  gridline = c(ggplot2::alpha(\"#D0D0CE\", 0.2), \n               ggplot2::alpha(\"#D0D0CE\", 0.4))\n)\n\n# set theme\nggthemr::ggthemr(cds_theme, type = \"outer\")\n\n# source custom DAG theme\nsource(\"../../code/dag_theme.R\")\n```\n\nNow we will delve into some statistical concepts, that are the foundation for statistical modeling processes used in causal inference. If you sometimes prefer to see additional visual explanations, I can also recommend you to read [here](https://seeing-theory.brown.edu/basic-probability/index.html).\n\n# Random Variable\n\nWe already touched random variables in the last chapter but for starters, let's again define what it is. Often represented by a letter such as $X$, a random variable has a set of values, also called sample space, of which any could be the outcome if we draw a random sample from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). Because there is randomness and chance in the outcome, we call it a **random** variable.\n\nA random variable can either take on discrete (e.g. die) or continuous values (e.g. average height of individuals).\n\n-   **Discrete Random Variable**: it can take on a countable number of distinct values.\n\n    -   $X \\in \\{1, 2, 3, 4, 5, 6\\}$\n\n    -   $P(X=1) = P(X=2) = … = P(X=6) = \\frac{1}{6}$\n\n-   **Continuous Random Variable**: it can take on any value within a certain range or interval.\n\n    -   $X \\in [10, 250]$ (size in centimeters)\n\n    -   Typically presented using cumulative distribution function returning the probability of a variable being less than or equal to a specific value: $P(X < b),$ where $b$ could be any value in the given interval.\n\n# Expected Value\n\nA random variable is a real-valued function and has more than one possible outcome. This is why we cannot represent it as a scalar. The expected value of random variable, however, is a scalar and represents something like a \"summary\" of the random variable. Knowing the possible values (from the sample space) and the probability distribution, we can compute the expected value.\n\n::: callout-important\nThe *expected value*, also called *population mean*, of a variable $X$ is defined as the weighted average of possible values the random variable can take, where the weight is equal to the probability of the random variable taking on a specific value.\n:::\n\nIf we want to compute it for a discrete random variable, we make use of the summation operator. Considering a finite list of potential values $x_1, x_2, …, x_k$ with probabilities $p_1, p_2, …, p_k$, the expectation of $X$ can be computed by\n\n$$\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n$$\n\n::: callout-tip\nThe summation operator $\\sum$, denoted by the Greek capital Sigma is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, $x_1, x_2, …, x_n$ to a shorter and more readable form\n\n$$\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n$$\n\nwith the arbitrary index of summation $i$ being the lower limit and $n$ the upper limit.\n\nBy basic math rules, the following simplifications are possible, where $c$ is a constant:\n\n$$\n\\sum_{i=1}^nc=nc\n$$\n\nand\n\n$$\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n$$\n:::\n\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\n$$\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n$$\n\nProbabilities could also be different, as long as they sum to 1. For continuous random variables, we need a function returning the probabilities for each value. We leave that out here as we are only interested in understanding the intuition behind the expected value. However, it is very similar.\n\nLet's type in the probabilities and outcomes of rolling a die and see what the expected value is.\n\n::: callout-note\n| To replicate values and create a vector `rep()` can be used. It takes the value and number of times it should be replicated.\n:::\n\n```{r}\n# Vector of probabilities (all equal)\np <- rep(1/6, times = 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n```\n\nAs you might have expected, it's 3.5. In this case, we know the probabilities, but if we had not known them beforehand, what we could have done to get an estimate of the expected value is to roll the die a lot of times, store the results in an object and use the `mean()` function. It would yield an expected value close to 3.5 (see simulation [here](https://seeing-theory.brown.edu/basic-probability/)).\n\n::: callout-tip\nAdditional rules regarding the calculation of expected values that can be useful are:\n\n$$\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n$$\n:::\n\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc.\n\n# Conditional Expected Value\n\nThe conditional expected value is the expected value conditioned on some other value. Given the value $x$ for $X$, the expected value for $Y$ obtains as\n\n$$\nE[Y|X = x]\n$$\n\nand is a function of $x$. In other words, the conditional expected value is the best guess for $Y$ knowing only that $X=x$.\n\nAs a simple example, consider we take a representative random sample from the world population and want to compute the expected value for $height$. Denoting height with $Y$, the expected value for the whole population is the expected value $E[Y]$ over all individuals in your sample.\n\nThe conditional expected value, however, differs. For example, conditioned on individuals being younger than ten years or older than than that we expect different values.\n\n$$\nE[Y] \\neq E[Y|age < 10] \\neq E[Y|age >= 10]\n$$\n\nInstead of $age$, we can also choose another variable, e.g. $gender$. According to NCD[^1], the values in centimeter are as follows:\n\n[^1]: https://ourworldindata.org/human-height\n\n$$\n\\begin{aligned}\nE[Y|male] &= 171 \\\\\nE[Y|female] &= 159\n\\end{aligned}\n$$\n\nAnd the overall mean results as a (weighted) average between males and females.\n\n# Variance\n\nThe variance of random variable tells us how much the values of that random variable tend to spread out or vary from the expected value. Low variance means that the values are closer to the expected value, while high variance indicates values are more spread out from the expected value.\n\nMathematically, the *variance* is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The *sample variance* indicates how far a set of observed values spread out from their average value and is an estimate of the full *population variance*, that in most cases cannot be directly observed due to lack of data of the whole population.\n\nMathematically, the *population variance* is defined as\n\n$$\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n$$\n\nand the *sample variance* results as\n\n$$\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n$$\n\nYou might have noticed the term $(n-1)^{-1}$ is different from what you probably expected ($n^{-1}$). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\n\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution. The **standard deviation** obtains as the square root of the variance:\n\n$$\n\\sigma = \\sqrt{\\sigma^2}\n$$Take a look at this interactive histogram of draws from the normal distribution. You can set the mean (or: expected value) and the standard deviation. See how a high standard deviation leads to larger proportions of the data being far away from the expected value.\n\n```{ojs}\n//| echo: false\n//| panel: input\n\nviewof mean = Inputs.range(\n  [-5, 5], \n  {value: 1, step: 1, label: \"Expected value:\"}\n)\n\nviewof sd = Inputs.range(\n  [1, 20], \n  {value: 1, step: 1, label: \"Standard deviation:\"}\n)\n\n```\n\n```{ojs}\n//| echo: false\n//| fig-align: center\n\nmutable xDomain = [-50, 50]\nmutable yDomain = [0, 1000]\n\nPlot.plot({\n  ...theme,\n  //marginRight: 0,\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.rectY({length: 10000}, Plot.binX({y: \"count\"}, {x: d3.randomNormal(mean, sd),  fill: \"#00C1D4\"})),\n  ],\n  x: {domain: xDomain},\n  y: {domain: yDomain}\n})\n\n```\n\n::: callout-note\n| `rnorm(n, mean, sd)` samples values from the normal distribution. `n` specifies the number of values, `mean` and `sd` define the parameters of the normal distribution.\n\n| To compute variance and standard deviation, use `var()` and `sd()`, respectively.\n:::\n\n```{r}\nX_low_var  <- rnorm(1e+4, mean = 0, sd = 1)   # low variance\nX_high_var <- rnorm(1e+4, mean = 0, sd = 10)  # high variance\n\nprint(paste(\"Low variance case. Variance =\", var(X_low_var), \"Standard deviation =\", sd(X_low_var)))\nprint(paste(\"High variance case. Variance =\", var(X_low_var), \"Standard deviation =\", sd(X_low_var)))\n```\n\n::: callout-tip\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of $a$, then the variance will increase by $a^2$.\n\n$$\nVar(aX+b)=a^2V(X)\n$$\n\nYou can also conveniently compute a variance for the sum of two random variables\n\n$$\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n$$\n\nwhich in case of independence reduces to the sum of the individual variances due to the fact that $E(XY) = E(X)E(Y)$.\n:::\n\n# Variance of a Conditional Expected Value\n\nLet's take another look at variance but now we'll see we focus on the case of a conditional expected value - to be precise, the value of Y conditional on values of X. Playing with different values for the standard deviation in the graph below, you should notice that while the line that fits the data best (the line that is on average the closest to all data points - we will talk about it more extensively in the next chapter) stays very similar, the dispersion of the data points significantly changes.\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\n\n# Create a data frame with correlated X and Y values\ncreate_corr_df <- function(n = 3e+3, correlation = .3, sd = 1) {\n  # Generate random X and Y data\n  X <- rnorm(n)\n  Y <- correlation * X + sqrt(1 - correlation^2) * rnorm(n, mean = 0, sd = sd)\n  \n  # Create a data frame\n  df <- data.frame(x = X, y = Y) %>%\n    add_column(\n      sigma = correlation,\n      sd = sd)\n  \n  # Return data frame\n  return(df)\n}\n\n# Generate data for sequence of different levels of standard deviation\nsd_range <- seq(0, 1, .05)\nsd_tbl <- map(sd_range, function(sd) create_corr_df(sd = sd)) %>% bind_rows()\n\n# Define as Observable JS data\nojs_define(sd_data = sd_tbl)\n```\n\n```{ojs}\n//| echo: false\n//| panel: input\n\nviewof std = Inputs.range(\n  [0, 1], \n  {value: 0.5, step: 0.05, label: \"Standard deviation:\"}\n)\n\n```\n\n```{ojs}\n//| echo: false\n//| panel: center\n\ntheme = ({\n  style: \"background-color: #0F2231\",\n  fill: \"#00C1D4\"\n})\n\nmutable xDomain_sd = [-3, 3]\nmutable yDomain_sd = [-3, 3]\n\nPlot.plot({\n  ...theme,\n  //marginRight: 0,\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.dot(sd_filtered, {x: \"x\", y: \"y\", fill: \"#00C1D4\", fillOpacity: .4}),\n    Plot.linearRegressionY(sd_filtered, {x: \"x\", y: \"y\", stroke: \"#FF4F4F\"})\n  ],\n  x: {domain: xDomain_sd},\n  y: {domain: yDomain_sd}\n})\n\n```\n\n```{ojs}\n//| echo: false\nsd_filtered = transpose(sd_data).filter(function(sd_tbl) {\n  return std == sd_tbl.sd;\n})\n```\n\n# Covariance and Correlation\n\nTwo more important concepts to understand variability in data are the related concepts of covariance and correlation. Covariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction. A high, positive covariance indicates that two random variables move into the same direction, a high, negative covariance means that two random variables move in opposite directions.\n\nIf you remember the rules of probability theory from the previous chapter, you won't be surprised that if the equality $E(XY) = E(X)E(Y)$ holds, it implies a *covariance* of 0 between the variables $X$ and $Y$. *Covariance* is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, we can define the variance of a sum of two random variables as sum of the individual variances of both random variables plus two times their covariance.\n\nAs a matter of form, the formula for the covariance of the random variables $X$ and $Y$ is\n\n$$\nCov(X,Y) = E(XY) - E(X)E(Y) = E[(X-\\bar{X})(Y - \\bar{Y})]\n$$\n\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the *correlation* which can be derived from the covariance if the individual variances are known.\n\n$$\n\\text{Corr}(X,Y) = \\dfrac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n$$ The *correlation* is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign.\n\n```{r}\n#| echo: false\n\n# Generate data for sequence of different degrees of correlation\ncorrelation_range <- seq(-1, 1, .1)\ncorr_tbl <- map(correlation_range, function(c) create_corr_df(correlation = c)) %>% bind_rows()\n\n# Define as Observable JS data\nojs_define(data = corr_tbl)\n```\n\nSet the degree of correlation to understand how that changes the relationship between both variables.\n\n```{ojs}\n//| echo: false\n//| panel: input\n\nviewof sigma = Inputs.range(\n  [-1, 1], \n  {value: 0, step: 0.1, label: \"Degree of correlation:\"}\n)\n\n```\n\n```{ojs}\n//| echo: false\n//| panel: center\n\nPlot.plot({\n  ...theme,\n  //marginRight: 0,\n  grid: true,\n  color: {legend: true},\n  marks: [\n    Plot.dot(filtered, {x: \"x\", y: \"y\", fill: \"#00C1D4\", fillOpacity: .4}),\n    Plot.linearRegressionY(filtered, {x: \"x\", y: \"y\", stroke: \"#FF4F4F\"})\n  ]\n})\n\n```\n\n```{ojs}\n//| echo: false\nfiltered = transpose(data).filter(function(corr_tbl) {\n  return sigma == corr_tbl.sigma;\n})\n```\n\n::: callout-note\n| To compute correlation, variance and covariance, respectively, you can use `cor(X, Y)`, and `cov(X, Y)`.\n:::\n\n# Conclusion\n\nMany of the rules and concepts that you have just learned will play a crucial in the upcoming chapters. Their understanding will guide you through and let you understand why we need to put a particular emphasis on causality, how we can isolate causal effects and build the foundation for many methods from our toolbox.\n\n# Assignments\n\n::: assignment\nLoad the table `random_vars.rds`. You can either\n\n-   run the command: `random_vars <- readRDS(\"your_download_folder/random_vars.rds\")`\n-   or find the file in the `Files` pane and select it.\n\nAfter you load the table, get an overview of what it contains with `View(random_vars)`.\n\n1.  For each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n    1.  expected value\n    2.  variance\n    3.  standard deviation\n2.  Explain, if it makes sense to compare the standard deviations.\n3.  Then, examine the relationship between both variables and compute:\n    1.  covariance\n    2.  correlation\n4.  What measure is easier to interpret? Please discuss your interpretation.\n5.  Compute the conditional expected value for:\n    1.  $E[income|age <= 18]$\n    2.  $E[income|age \\in [18, 65)]$\n    3.  $E[income|age >= 65]$\n:::\n\n::: callout-tip\nIn the introduction you learned how to filter data. There are different ways, you could either use base R or `tidyverse`.\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":{"blogdown::html_page":{"toc":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["lightbox"],"css":["../../styles.css"],"toc":true,"output-file":"stats.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.536","lightbox":{"match":"auto"},"editor":"source","theme":["darkly","../../theme-darkly.scss"],"mainfont":"arial","linestretch":1.7,"title":"Statistical Concepts","linktitle":"Statistical Concepts","menu":{"example":{"parent":"Optional read","weight":2}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}