{"title":"Probability Theory","markdown":{"yaml":{"title":"Probability Theory","linktitle":"Probability Theory","output":{"blogdown::html_page":{"toc":true}},"menu":{"example":{"parent":"Optional read","weight":1}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"headingText":"custom ggplot theme","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = \"center\", fig.retina = 3, out.width = \"75%\")\nset.seed(11)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n\n# colors from TUHH brand identitiy\ntuhh_colors <- c(\"#D0D0CE\", \"#00C1D4\", \"#FF4F4F\", \"#5AFFC5\",\n                 \"#FFDE36\", \"#143BFF\", \"#FF7E15\", \"#FFAEA2\")\n\n# initialise theme\ncds_theme <- ggthemr::define_palette(\n  swatch = tuhh_colors,\n  gradient = c(lower = \"#FFAEA2\", upper = \"#00C1D4\"),\n  background = \"#0F2231\",\n  line = c(\"#FFFFFF\", \"#FFFFFF\"),\n  text = c(\"#FFFFFF\", \"#FFFFFF\"),\n  gridline = c(ggplot2::alpha(\"#D0D0CE\", 0.2), \n               ggplot2::alpha(\"#D0D0CE\", 0.4))\n)\n\n# set theme\nggthemr::ggthemr(cds_theme, type = \"outer\")\n\n# source custom DAG theme\nsource(\"../../code/dag_theme.R\")\n```\n\n# Introduction\n\nFeel free to review some basic concepts of probability and statistics. All methods that used in this course are based on statistical models and these require probability theory.\n\n# Probability\n\nWe will start by reviewing some basic concepts of probability theory, drawing probability trees, introducing set theory and applying Bayes theorem.\n\n## **Basic rules of probability**\n\nConsider the most simple example: flipping coins. We define the outcome of the flip of a coin as a **random variable** as we are uncertain about what side the coin lands on. To express this uncertainty, we make us of probability theory.\n\nAfter flipping the coin, we will see what side the coin has landed on and our random variable has taken on of the **two possible events** $\\{H, T\\} \\subseteq \\Omega$. It will be either *Head* or *Tail*.\n\nSo we have already defined two terms: random variable and events. Now what is a probability? A **probability is always linked to an event** typically denoted by a capital letter, here either $H$ and $T$, and expresses **how likely this event is to happen**. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\n$$\nP(H) = P(T) = 0.5\n$$\n\n[Extreme cases]{.underline}: If an event $A$ is impossible, its probability is $P(A) = 0$ and if it is certain to occur, it is $P(A)=1$.\n\n::: callout-important\n**Axiom 1:** Probability is a real number greater or equal to 0.\n:::\n\nWe can also introduce the **compliment** $\\overline{A}$, which is what happens when $A$ does not happen and consequently, $P(A) + P(\\overline{A}) = 1$. $A$ and $\\overline{A}$ are mutually exclusive, by definition. But there could also be two events $A$ and $B$ that are mutually exclusive, i.e. only one of those events can happen, then $P(A \\cup B) = P(A) + P(B)$, where $\\cup$ represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\n$$\nP(H \\cup T) = P(H) + P(T) = 1\n$$\n\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n::: callout-important\n**Axiom 2:** Total probability is equal to 1.\n:::\n\n::: callout-important\n**Axiom 3:** Probability of mutually exclusive events is the sum of the probabilities. (Mutually exclusive: events can't happen at the same time)\n:::\n\nTo understand what not mutually exclusive events are, consider events $studying$ and $working$. For a random person, we don't know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\n\nThen, the probability of at least one of the events happening is calculated by\n\n$$\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n$$\n\nwith $P(A \\cap B)$ being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the **addition rule**.\n\n::: callout-tip\n$\\cup$ : Union, can be translated as \"or\".\n\n$\\cap$ : Intersection, can be translated as \"and\".\n:::\n\n![](/images/01_venn_excl.png){fig-align=\"center\" width=\"650\"}\n\nFor mutually exclusive events:\n\n$$\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B)\n$$\n\nThe aforementioned intersection $P(A \\cap B)$ can be calculated by the **multiplication rule**,\n\n$$\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n$$\n\nwhere $P(A|B)$ denotes the probability of $A$ happening given that $B$ has happened. It is called a **conditional probability** and is defined by:\n\n$$\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n$$\n\nIt can be thought of as the probability of an event $A$ after you know that $B$ is true. Essentially, it computes the possibility of event $A$ and $B$, normalized by the probability of $B$ occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\n\nUsing the example with workers and students: without knowing exact numbers, we can assume that students are less likely to work than individuals who are not studying.\n\n$$\nP(working|studying) < P(working|\\overline{studying})\n$$\n\nEssentially, we are looking at probabilities restricted to a subset of the sample, which in this comparison are the subsamples of studying persons and non-studying persons.\n\nAnother important concept when dealing with probabilities of events is **stochastic independence**. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways. Let's think of rolling a die twice (first roll $R_1$ and second roll $R_2$).\n\n$$\nP(R_2 \\mid R_1) = P(R_2)\n$$\n\nThe second roll does not depend on the first one. With each roll the outcomes ${1, 2, 3, 4, 5, 6}$ have the same probability likely independent of the previous roll. If we want to compute the probability of both rolls being a $6$, we would just have to multiply the probabilities for each roll.\n\n$$\nP(R_1 = 6 \\cap R_2 = 6) = P(R_1 = 6) \\ P(R_2 = 6)\n$$\n\n## **Probability Tree**\n\nLet's go back to the case where events are dependent on each other. An intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to $1$ in probability as one (and only one) of the events happens. The probability of two consecutive events is obtained by multiplying the probabilities.\n\nConsider the following example: you are project manager and based on your are interested in the probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Using historical data about past projects, you come up with the following tree.\n\n![](/images/01_tree.png){fig-align=\"center\" width=\"450\"}\n\n## Assignment I\n\n::: assignment\nDefine being on time as event $T$, being not on time as $\\overline{T}$, having a change in scope as $S$ and having no change in scope as $\\overline{S}$. (*Hint: Check [here](https://en.wikipedia.org/wiki/Tree_diagram_%28probability_theory%29#/media/File:Probability_tree_diagram.svg), if you are not sure what is shown in the probability tree.)*\n\nThen, compute the following probabilities and the sum of all four probabilities.\n\n-   $P(T \\cap S)$\n\n-   $P(T \\cap \\overline{S})$\n\n-   $P(\\overline{T} \\cap S)$\n\n-   $P(\\overline{T} \\cap \\overline{S})$\n:::\n\n::: callout-tip\nWith some browsers and specific operating systems, the compliment probability is not shown correctly (missing the horizontal bar above the letter). In that case it often helps to zoom in or out.\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n\n## **Set Theory**\n\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams that are based on set theory. We already used a simple one above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\n\nLet's use an example to understand some other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices: smartphones, tablets and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\n\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current usage distribution.\n\nInstead of using actual data, we simulate the data collection process here. If you are interested how to do it in `R`, you can expand and check out the code by clicking on `Code`. But you don't have to. And don't worry, if it looks too complicated at this point, just move on.\n\n::: callout-note\n| `library()` loads external packages/libraries containing functions that are not built in base `R`.\n\n| `tibble()` is the most convenient way to create tablets. You specify column name and content and assign your `tibble` to an object to store it.\n\n| `ifelse(test, yes, no)` is a short function for if...else statements. The first argument is a condition that is either `TRUE` or `FALSE` and determines whether the second or third argument is returned.\n\n| `rbinom(n, size, prob)` samples `n` values from a binomial distribution of a given `size` and with given probabilities `prob`.\n\n| `mutate()` is one of the most important functions for data manipulation in tablets. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, `mutate(table, new_variable = existing_var / 100)`, which is equivalent to `table %>% mutate(new_variable = existing_var / 100)`.\n:::\n\n```{r}\n#| code-fold: true\n#| message: false\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Number of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  rowwise() %>% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n```\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer. Please note that $1$ indicates usage and $0$ indicates no usage.\n\n::: callout-note\n| To see the first lines of a table (for example a `tibble()` or a `data.frame()`, you can use the `head(table, n)` function, where `n` specifies how many rows you want to see.\n:::\n\n```{r}\n# Show first ten lines\nhead(app_usage, 10)\n```\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n::: callout-note\n| Summing all values by column is done by `colSums(table)`. To sum rows, you can use `rowSums(table)`.\n:::\n\n```{r}\n# Show column sums\ncolSums(app_usage)\n```\n\nThe sum of $user\\_id$ does not really tell us anything. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n::: callout-note\n| To access only specified columns, you can provide the location or names in square brackets or you can use the `select()` function.\n:::\n\n```{r}\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n```\n\nNow let's see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n![Generic Venn diagram](/images/01_venn_diag.png){fig-align=\"center\" width=\"520\"}\n\n::: callout-note\n| `which()` checks a condition and returns the indices.\n:::\n\n```{r}\n# Set of phone, tablet and computer users\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all <- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n```\n\n### Assignment II\n\n::: assignment\nUsing the Venn diagram above, answer the following questions.\n\n-   What is the percentage of customers using all three devices?\n\n-   What is the percentage of customers using at least two devices?\n\n-   What is the percentage of customers using only one device?\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n\n------------------------------------------------------------------------\n\nYou can also use the example to go through the basic probability rules defined above (that does not belong to the assignment anymore).\n\n**Addition rule**:\n\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\n$P(T \\cup S) = P(T) + P(S) - P(T \\cap S)$\n\n**Multiplication rule**:\n\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\n$P(T|C) = \\frac{P(T \\cap C)}{P(C)}$\n\n**Total probability rule**:\n\nWhat is the fraction of customers using a computer?\n\n$P(C) = P(C \\cap T) + P(C \\cap \\overline{T})$\n\n## **Bayes Theorem**\n\n### Math\n\nA very important theorem in probability theory is **Bayes theorem**. In fact, it has been called the most powerful rule of probability and statistics. Let's quickly go through the math. By reformulating the multiplication rule\n\n$$\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n$$\n\nand using the equality of $P(A ∩ B)$ and $P(B ∩ A)$ we arrive at\n\n$$\nP(B|A)*P(A) = P(A|B)*P(B)\n$$\n\nand finally at the Bayes theorem:\n\n$$\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n$$\n\nBayes theorem expresses a conditional probability, exemplary the likelihood of $A$ occurring conditioned on $B$ having happened before. With the Bayes theorem you can answer questions like:\n\n-   How likely is it that it will rain, when there are clouds in the morning?\n\n-   How likely is it that an email is spam if certain keywords appear?\n\n::: callout-tip\nYou will often hear Bayes theorem in connection with the terms *updating beliefs*. You start with a prior probability $P(A)$ and collecting evidence $P(B)$ and the likelihood $P(B|A)$, you update your prior probability to get a posterior probability $P(A|B)$. That is in fact the foundation of Bayesian inference. Look it up if you want, but you won't need Bayesian inference for this course.\n\n$$\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n$$\n:::\n\n### **Application**\n\nTo understand how useful Bayes theorem is, let's use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\n\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\n1.  What is the probability that when the alarm is triggered the product is found to be flawless?\n\n2.  What is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\n\nWe should start by defining the events and event sets:\n\n$A$: product is faulty vs. $\\overline{A}$: product is flawless\n\n$B$: alarm is triggered vs. $\\overline{B}$: no alarm\n\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\n$P(B|A) = 0.97$ <!--# and consequently $P(\\overline{B}|A) = 0.03$ -->\n\n$P(B|\\overline{A}) = 0.01$ <!--# and consequently $P(\\overline{B}|\\overline{A}) = 0.99$ -->\n\n$P(A) = 0.04$ <!--# and consequently $P(\\overline{A}) = 0.96$ -->\n\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is $P(\\overline{A}|B)$ (1) and $P(A|B)$ (2) and we will need Bayes theorem to obtain those probabilities.\n\nLet's recall Bayes theorem:\n\n$$\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n$$\n\n### Assignment III\n\n::: assignment\nCompute\n\n-   $P(\\overline{A}|B)$ (1)\n-   $P(A|B)$ (2)\n\nand fill the gaps in the following sentence:\n\n*These results show that in case the alarm is triggered, there is a possibility of about \\_\\_% that the product is flawless and a probability of \\_\\_% that the product is faulty.*\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, fig.align = \"center\", fig.retina = 3, out.width = \"75%\")\nset.seed(11)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n\n# custom ggplot theme\n# colors from TUHH brand identitiy\ntuhh_colors <- c(\"#D0D0CE\", \"#00C1D4\", \"#FF4F4F\", \"#5AFFC5\",\n                 \"#FFDE36\", \"#143BFF\", \"#FF7E15\", \"#FFAEA2\")\n\n# initialise theme\ncds_theme <- ggthemr::define_palette(\n  swatch = tuhh_colors,\n  gradient = c(lower = \"#FFAEA2\", upper = \"#00C1D4\"),\n  background = \"#0F2231\",\n  line = c(\"#FFFFFF\", \"#FFFFFF\"),\n  text = c(\"#FFFFFF\", \"#FFFFFF\"),\n  gridline = c(ggplot2::alpha(\"#D0D0CE\", 0.2), \n               ggplot2::alpha(\"#D0D0CE\", 0.4))\n)\n\n# set theme\nggthemr::ggthemr(cds_theme, type = \"outer\")\n\n# source custom DAG theme\nsource(\"../../code/dag_theme.R\")\n```\n\n# Introduction\n\nFeel free to review some basic concepts of probability and statistics. All methods that used in this course are based on statistical models and these require probability theory.\n\n# Probability\n\nWe will start by reviewing some basic concepts of probability theory, drawing probability trees, introducing set theory and applying Bayes theorem.\n\n## **Basic rules of probability**\n\nConsider the most simple example: flipping coins. We define the outcome of the flip of a coin as a **random variable** as we are uncertain about what side the coin lands on. To express this uncertainty, we make us of probability theory.\n\nAfter flipping the coin, we will see what side the coin has landed on and our random variable has taken on of the **two possible events** $\\{H, T\\} \\subseteq \\Omega$. It will be either *Head* or *Tail*.\n\nSo we have already defined two terms: random variable and events. Now what is a probability? A **probability is always linked to an event** typically denoted by a capital letter, here either $H$ and $T$, and expresses **how likely this event is to happen**. Probabilities are always between 0 and 1 and for flipping the coin, as long as it is a fair coin (which we assume), the probabilities are\n\n$$\nP(H) = P(T) = 0.5\n$$\n\n[Extreme cases]{.underline}: If an event $A$ is impossible, its probability is $P(A) = 0$ and if it is certain to occur, it is $P(A)=1$.\n\n::: callout-important\n**Axiom 1:** Probability is a real number greater or equal to 0.\n:::\n\nWe can also introduce the **compliment** $\\overline{A}$, which is what happens when $A$ does not happen and consequently, $P(A) + P(\\overline{A}) = 1$. $A$ and $\\overline{A}$ are mutually exclusive, by definition. But there could also be two events $A$ and $B$ that are mutually exclusive, i.e. only one of those events can happen, then $P(A \\cup B) = P(A) + P(B)$, where $\\cup$ represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities. For example,\n\n$$\nP(H \\cup T) = P(H) + P(T) = 1\n$$\n\nwhich shows two things, that the total probability is equal to 1 and that the probability of mutually exclusive events is the sum of the individual probabilities.\n\n::: callout-important\n**Axiom 2:** Total probability is equal to 1.\n:::\n\n::: callout-important\n**Axiom 3:** Probability of mutually exclusive events is the sum of the probabilities. (Mutually exclusive: events can't happen at the same time)\n:::\n\nTo understand what not mutually exclusive events are, consider events $studying$ and $working$. For a random person, we don't know what values these random variables take on. But we know the probability for the event that someone is studying or someone is working. And there are also individuals who do both or neither.\n\nThen, the probability of at least one of the events happening is calculated by\n\n$$\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n$$\n\nwith $P(A \\cap B)$ being the intersection of both events, i.e. the probability of both studying and working. This formula is based on the **addition rule**.\n\n::: callout-tip\n$\\cup$ : Union, can be translated as \"or\".\n\n$\\cap$ : Intersection, can be translated as \"and\".\n:::\n\n![](/images/01_venn_excl.png){fig-align=\"center\" width=\"650\"}\n\nFor mutually exclusive events:\n\n$$\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B) = P(A) + P(B)\n$$\n\nThe aforementioned intersection $P(A \\cap B)$ can be calculated by the **multiplication rule**,\n\n$$\nP(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\n$$\n\nwhere $P(A|B)$ denotes the probability of $A$ happening given that $B$ has happened. It is called a **conditional probability** and is defined by:\n\n$$\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n$$\n\nIt can be thought of as the probability of an event $A$ after you know that $B$ is true. Essentially, it computes the possibility of event $A$ and $B$, normalized by the probability of $B$ occurring. The conditional probability is crucial when talking about causality which you will later see as it for example yields probabilities for specific groups.\n\nUsing the example with workers and students: without knowing exact numbers, we can assume that students are less likely to work than individuals who are not studying.\n\n$$\nP(working|studying) < P(working|\\overline{studying})\n$$\n\nEssentially, we are looking at probabilities restricted to a subset of the sample, which in this comparison are the subsamples of studying persons and non-studying persons.\n\nAnother important concept when dealing with probabilities of events is **stochastic independence**. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways. Let's think of rolling a die twice (first roll $R_1$ and second roll $R_2$).\n\n$$\nP(R_2 \\mid R_1) = P(R_2)\n$$\n\nThe second roll does not depend on the first one. With each roll the outcomes ${1, 2, 3, 4, 5, 6}$ have the same probability likely independent of the previous roll. If we want to compute the probability of both rolls being a $6$, we would just have to multiply the probabilities for each roll.\n\n$$\nP(R_1 = 6 \\cap R_2 = 6) = P(R_1 = 6) \\ P(R_2 = 6)\n$$\n\n## **Probability Tree**\n\nLet's go back to the case where events are dependent on each other. An intuitive way to think about (conditional) probabilities is a probability tree. Branches from one node always sum to $1$ in probability as one (and only one) of the events happens. The probability of two consecutive events is obtained by multiplying the probabilities.\n\nConsider the following example: you are project manager and based on your are interested in the probability of a project being delivered on time. Based on your experience, you know that whether a project is on time depends on whether there is a change in scope. Using historical data about past projects, you come up with the following tree.\n\n![](/images/01_tree.png){fig-align=\"center\" width=\"450\"}\n\n## Assignment I\n\n::: assignment\nDefine being on time as event $T$, being not on time as $\\overline{T}$, having a change in scope as $S$ and having no change in scope as $\\overline{S}$. (*Hint: Check [here](https://en.wikipedia.org/wiki/Tree_diagram_%28probability_theory%29#/media/File:Probability_tree_diagram.svg), if you are not sure what is shown in the probability tree.)*\n\nThen, compute the following probabilities and the sum of all four probabilities.\n\n-   $P(T \\cap S)$\n\n-   $P(T \\cap \\overline{S})$\n\n-   $P(\\overline{T} \\cap S)$\n\n-   $P(\\overline{T} \\cap \\overline{S})$\n:::\n\n::: callout-tip\nWith some browsers and specific operating systems, the compliment probability is not shown correctly (missing the horizontal bar above the letter). In that case it often helps to zoom in or out.\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n\n## **Set Theory**\n\nAnother useful tool to visualize the occurrence and relationship between events are Venn diagrams that are based on set theory. We already used a simple one above to illustrate the difference of mutually exclusive and non-mutually exclusive events.\n\nLet's use an example to understand some other rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices: smartphones, tablets and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\n\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current usage distribution.\n\nInstead of using actual data, we simulate the data collection process here. If you are interested how to do it in `R`, you can expand and check out the code by clicking on `Code`. But you don't have to. And don't worry, if it looks too complicated at this point, just move on.\n\n::: callout-note\n| `library()` loads external packages/libraries containing functions that are not built in base `R`.\n\n| `tibble()` is the most convenient way to create tablets. You specify column name and content and assign your `tibble` to an object to store it.\n\n| `ifelse(test, yes, no)` is a short function for if...else statements. The first argument is a condition that is either `TRUE` or `FALSE` and determines whether the second or third argument is returned.\n\n| `rbinom(n, size, prob)` samples `n` values from a binomial distribution of a given `size` and with given probabilities `prob`.\n\n| `mutate()` is one of the most important functions for data manipulation in tablets. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, `mutate(table, new_variable = existing_var / 100)`, which is equivalent to `table %>% mutate(new_variable = existing_var / 100)`.\n:::\n\n```{r}\n#| code-fold: true\n#| message: false\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n# Number of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  rowwise() %>% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n```\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer. Please note that $1$ indicates usage and $0$ indicates no usage.\n\n::: callout-note\n| To see the first lines of a table (for example a `tibble()` or a `data.frame()`, you can use the `head(table, n)` function, where `n` specifies how many rows you want to see.\n:::\n\n```{r}\n# Show first ten lines\nhead(app_usage, 10)\n```\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\n\n::: callout-note\n| Summing all values by column is done by `colSums(table)`. To sum rows, you can use `rowSums(table)`.\n:::\n\n```{r}\n# Show column sums\ncolSums(app_usage)\n```\n\nThe sum of $user\\_id$ does not really tell us anything. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\n\n::: callout-note\n| To access only specified columns, you can provide the location or names in square brackets or you can use the `select()` function.\n:::\n\n```{r}\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n```\n\nNow let's see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\n\n![Generic Venn diagram](/images/01_venn_diag.png){fig-align=\"center\" width=\"520\"}\n\n::: callout-note\n| `which()` checks a condition and returns the indices.\n:::\n\n```{r}\n# Set of phone, tablet and computer users\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all <- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\nlibrary(ggVennDiagram)\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n```\n\n### Assignment II\n\n::: assignment\nUsing the Venn diagram above, answer the following questions.\n\n-   What is the percentage of customers using all three devices?\n\n-   What is the percentage of customers using at least two devices?\n\n-   What is the percentage of customers using only one device?\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n\n------------------------------------------------------------------------\n\nYou can also use the example to go through the basic probability rules defined above (that does not belong to the assignment anymore).\n\n**Addition rule**:\n\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\n$P(T \\cup S) = P(T) + P(S) - P(T \\cap S)$\n\n**Multiplication rule**:\n\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\n$P(T|C) = \\frac{P(T \\cap C)}{P(C)}$\n\n**Total probability rule**:\n\nWhat is the fraction of customers using a computer?\n\n$P(C) = P(C \\cap T) + P(C \\cap \\overline{T})$\n\n## **Bayes Theorem**\n\n### Math\n\nA very important theorem in probability theory is **Bayes theorem**. In fact, it has been called the most powerful rule of probability and statistics. Let's quickly go through the math. By reformulating the multiplication rule\n\n$$\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n$$\n\nand using the equality of $P(A ∩ B)$ and $P(B ∩ A)$ we arrive at\n\n$$\nP(B|A)*P(A) = P(A|B)*P(B)\n$$\n\nand finally at the Bayes theorem:\n\n$$\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n$$\n\nBayes theorem expresses a conditional probability, exemplary the likelihood of $A$ occurring conditioned on $B$ having happened before. With the Bayes theorem you can answer questions like:\n\n-   How likely is it that it will rain, when there are clouds in the morning?\n\n-   How likely is it that an email is spam if certain keywords appear?\n\n::: callout-tip\nYou will often hear Bayes theorem in connection with the terms *updating beliefs*. You start with a prior probability $P(A)$ and collecting evidence $P(B)$ and the likelihood $P(B|A)$, you update your prior probability to get a posterior probability $P(A|B)$. That is in fact the foundation of Bayesian inference. Look it up if you want, but you won't need Bayesian inference for this course.\n\n$$\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n$$\n:::\n\n### **Application**\n\nTo understand how useful Bayes theorem is, let's use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\n\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\n1.  What is the probability that when the alarm is triggered the product is found to be flawless?\n\n2.  What is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\n\nWe should start by defining the events and event sets:\n\n$A$: product is faulty vs. $\\overline{A}$: product is flawless\n\n$B$: alarm is triggered vs. $\\overline{B}$: no alarm\n\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\n$P(B|A) = 0.97$ <!--# and consequently $P(\\overline{B}|A) = 0.03$ -->\n\n$P(B|\\overline{A}) = 0.01$ <!--# and consequently $P(\\overline{B}|\\overline{A}) = 0.99$ -->\n\n$P(A) = 0.04$ <!--# and consequently $P(\\overline{A}) = 0.96$ -->\n\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is $P(\\overline{A}|B)$ (1) and $P(A|B)$ (2) and we will need Bayes theorem to obtain those probabilities.\n\nLet's recall Bayes theorem:\n\n$$\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n$$\n\n### Assignment III\n\n::: assignment\nCompute\n\n-   $P(\\overline{A}|B)$ (1)\n-   $P(A|B)$ (2)\n\nand fill the gaps in the following sentence:\n\n*These results show that in case the alarm is triggered, there is a possibility of about \\_\\_% that the product is flawless and a probability of \\_\\_% that the product is faulty.*\n:::\n\n::: callout-warning\n## Optional assignments!\n\nAs this is part of the optional section, you do not have to submit any solutions. But feel free to test your knowledge and understandy by solving the assignments. \n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"blogdown::html_page":{"toc":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["lightbox"],"css":["../../styles.css"],"toc":true,"output-file":"prob.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.536","lightbox":{"match":"auto"},"editor":"source","theme":["darkly","../../theme-darkly.scss"],"mainfont":"arial","linestretch":1.7,"title":"Probability Theory","linktitle":"Probability Theory","menu":{"example":{"parent":"Optional read","weight":1}},"type":"docs","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}