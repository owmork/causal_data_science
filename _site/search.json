[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "causal_ds_ws22",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "content/part_i/01_prob.html",
    "href": "content/part_i/01_prob.html",
    "title": "Probability Theory and Statistics",
    "section": "",
    "text": "Before we dive into topics of causal inference, we review some basic concepts of probability and statistics. All methods that we will use later in this course are based on statistical models and these in turn, require probability theory. But we will keep it as short as possible as our focus and learning goal lies more on applications and coding than on the theoretical part."
  },
  {
    "objectID": "content/part_i/01_prob.html#basic-rules-of-probability",
    "href": "content/part_i/01_prob.html#basic-rules-of-probability",
    "title": "Probability Theory and Statistics",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nA probability is always linked to an event typically denoted by a capital letter, e.g. \\(A\\) and expresses how likely this event is to happen. Several events build a set of events \\(A,B \\subseteq \\Omega\\). Probabilities to describe the chance of occurrence of event \\(A\\) are always between \\(0\\) and \\(1\\). The occurrence of an event \\(A\\) implies \\(P(A) = 1\\), while the impossibility of event \\(A\\) occurring implies \\(P(A) = 0\\).\n\\(P(\\overline{A})\\) is the compliment of \\(P(A)\\), so \\(\\overline{A}\\) is what happens, when \\(A\\) does not happen. Consequently as either \\(A\\) or \\(\\overline{A}\\) occurs, the probability of either event occurring is \\(P(A) + P(\\overline{A}) = 1\\). Generally, if two events \\(A\\) and \\(B\\) are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities.\nIn contrast, if the two events are not mutually exclusive, the probability of both events happening is calculated by \\(P(A \\cup B) = P(A) + P(B) + P(A \\cap B)\\) with \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both events happening. This formula is based on the addition rule.\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule: \\(P(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\\), where \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened.\nThe total probability rule states that \\(P(A) = P(A \\cap B) + P(A \\cap \\overline{B})\\).\nAlready used above, \\(P(A|B)\\) is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways:\n\\[\nP(A \\mid B) = P(A)\n\\]\nA typical example of independence is to roll a die twice. The second roll does not depend on the first one and each outcome is as likely independent of the first roll. This also means that\n\\[\nP(A \\cap B) = P(A) \\ P(B)\n\\]"
  },
  {
    "objectID": "content/part_i/01_prob.html#probability-tree",
    "href": "content/part_i/01_prob.html#probability-tree",
    "title": "Probability Theory and Statistics",
    "section": "Probability Tree",
    "text": "Probability Tree\nAn intuitive way to think about probabilities are probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events has to happen. The probability of two consecutive events is obtained by multiplying the probabilities. Here, all events are independent and therefore all probabilities are \\(0.5\\) and therefore: \\(P(A \\mid B) = P(A)\\).\n\n\n\nhttps://www.mashupmath.com/blog/probability-tree-diagrams\n\n\n!!! Explanation conditional probability (dependent example)\n!!! Assignment: some empty gaps"
  },
  {
    "objectID": "content/part_i/01_prob.html#set-theory",
    "href": "content/part_i/01_prob.html#set-theory",
    "title": "Probability Theory and Statistics",
    "section": "Set Theory",
    "text": "Set Theory\nAn extremely useful tool to visualize the occurrence and relationship between events are Venn diagrams from set theory.\nLet’s use an example to understand the rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices, smartphones, tables and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current distribution.\nlibrary() loads external packages/libraries containing functions that are not built in base R.\ntibble() is the most convenient way to create tables. You specify column name and content and assign your tibble to an object to store it.\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\nmutate() is one of the most important functions for data manipulation in tables. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %>% mutate(new_variable = existing_var / 100).\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.8\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.2     ✓ forcats 0.5.1\n\n\n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n# Nmber of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.6),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.7)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.5))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer.\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\nhead(app_usage, 10)\n\n# A tibble: 10 × 4\n   user_id smartphone tablet computer\n     <int>      <int>  <int>    <int>\n 1       1          1      1        0\n 2       2          1      0        1\n 3       3          1      0        1\n 4       4          1      0        0\n 5       5          1      0        1\n 6       6          1      0        0\n 7       7          1      1        0\n 8       8          1      1        0\n 9       9          1      1        0\n10      10          1      1        0\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\nSumming all values by column is done by colSums(table). For rows, you would use rowSums(table).\n\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500       1000        399        334 \n\n\nSumming the \\(user\\_id\\) does not make any sense. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\ncolSums(app_usage[, 2:4])\n\nsmartphone     tablet   computer \n      1000        399        334 \n\ncolSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\n\nsmartphone     tablet   computer \n      1000        399        334 \n\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n\nsmartphone     tablet   computer \n      1000        399        334 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\nwhich() checks a condition and returns the indices.\n\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\nsets_all <- list(set_phon, set_tabl, set_comp)\n\nlibrary(ggVennDiagram)\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\")) +\n  theme(legend.position = \"none\", panel.background = element_rect(\"#1C1C1C\"))\n\n\n\n\n\n\n\n\n\nUsing the Venn diagram, we are able to answer questions like the following:\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\nWe can also use the example to go through the basic probability rules defined above.\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)"
  },
  {
    "objectID": "content/part_i/01_prob.html#bayes-theorem",
    "href": "content/part_i/01_prob.html#bayes-theorem",
    "title": "Probability Theory and Statistics",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nA very important theorem in probability theory is Bayes theorem that we get by reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nUsing the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem states the mathematical formulation for determining a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before.\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]"
  },
  {
    "objectID": "content/part_i/01_prob.html#application",
    "href": "content/part_i/01_prob.html#application",
    "title": "Probability Theory and Statistics",
    "section": "Application",
    "text": "Application\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) and consequently \\(P(\\overline{B}|A) = 0.03\\)\n\\(P(B|\\overline{A}) = 0.01\\) and consequently \\(P(\\overline{B}|\\overline{A}) = 0.99\\)\n\\(P(A) = 0.04\\) and consequently \\(P(\\overline{A}) = 0.96\\)\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nNow we can see, that we have all the information we need to compute the desired probabilities. \\(P(A|B)\\) (2) is calculated by\n\n(0.97 * 0.04) / ((0.97 * 0.04) + (0.01 * 0.96))\n\n[1] 0.8\n\n\nand \\(P(\\overline{A}|B)\\) (1) by\n\n(0.01 * 0.96) / ((0.01 * 0.96) + (0.97 * 0.04))\n\n[1] 0.2\n\n\nThese results show that in case the alarm is triggered, there is still a possibility of about 20% that the product is flawless and in only 80% of cases the product is faulty. Depending on your current method of quality assurance, these numbers might make you think more about buying the tool. And also note, how these numbers give a different perspective compared to what the manufacturer said."
  },
  {
    "objectID": "content/part_i/01_prob.html#random-variable",
    "href": "content/part_i/01_prob.html#random-variable",
    "title": "Probability Theory and Statistics",
    "section": "Random Variable",
    "text": "Random Variable\nFor starters, let’s define what a random variable is. Often represented by letters such as \\(X\\), a random variable has a set of values, also called sample space, of which any could be the outcome if we draw from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). A random variable can either take on discrete (e.g. die( or continuous values (e.g. average height of random people)."
  },
  {
    "objectID": "content/part_i/01_prob.html#expected-value",
    "href": "content/part_i/01_prob.html#expected-value",
    "title": "Probability Theory and Statistics",
    "section": "Expected value",
    "text": "Expected value\nBecause a random variable can take on different values, we cannot represent it as a scalar. However, the expected value of random variable is a scalar and represents something like a “summary” of the random variable with its values and its probability distribution.\nBefore we define the expected value, we need to introduce the summation operator \\(\\sum\\), denoted by the Greek capital Sigma.\nIn general, it is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, \\(x_1, x_2, …, x_n\\) to a shorter and more readable form\n\\[\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n\\]\nwith the arbitrary index of summation \\(i\\) being the lower limit and \\(n\\) the upper limit.\nBy basic math rules, the following simplifications are possible, where \\(c\\) is a constant:\n\\[\n\\sum_{i=1}^nc=nc\n\\]\nand\n\\[\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n\\]\nA statement, that you will see a lot in many applications and formulas is the average of a sequence of numbers, typically denoted by a line over a variable. Some equivalent forms of writing down the calculation for the average are shown here:\n\\[\n\\overline{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i \\\\ = \\sum_{i=1}^n \\dfrac{1}{n} x_i \\\\ = \\sum_{i=1}^n \\dfrac{x_i}{n} \\\\ =\\dfrac{x_1+x_2+\\dots+x_n}{n} \\\\ = n^{-1} \\sum_{i=1}^{n} x_i\n\\]\nThis brings us to the first important basic concept we will need throughout the course, the expected value, also called population mean. The expected value of a random variable \\(X\\) is defined as the weighted average of possible values the random variable can take. The weight is equal to the probability of the random variable taking a specific value.\nConsidering a finite list of potential values \\(x_1, x_2, …, x_k\\) with probabilities \\(p_1, p_2, …, p_k\\), the expectation of \\(X\\) can be computed by\n\\[\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n\\]\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\\[\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n\\]\nTo replicate values and create a vector rep() can be used. It takes the value and number of times it should be replicated.\n\n# Vector of probabilities (all equal)\np <- rep(1/6, 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n\n[1] 3.5\n\n\nAdditional rules regarding the calculation of expected values that can be useful are:\n\\[\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n\\]\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc."
  },
  {
    "objectID": "content/part_i/01_prob.html#variance",
    "href": "content/part_i/01_prob.html#variance",
    "title": "Probability Theory and Statistics",
    "section": "Variance",
    "text": "Variance\nBefore we define variance, let’s see why it is important to know it. On both graphs we see almost the same line (small difference because of sampling) going through the data points. It is the line that fits the data best. However, there is a difference in how the data points are distributed. On the left graph, there is high variance compared to the right graph. That means, the data is more dispersed.\n!!! conditional expectation\nseq(from, to, by) or seq(from, to, length.out) returns a vectors with a sequence as specified by the arguments.\nrnorm(n, mean, sd) samples values from the normal distribution. n specifies the number of values, mean and sd define the parameters of the normal distribution.\nmap() is a very useful function when you want to apply a function to each element of a list or a vector.\npivot_longer() changes the format of a table by pivoting columns into rows. To pivot rows into columns, you need pivot_wider().\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nMathematically, the variance is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The sample variance indicates how far a set of observed values spread out from their average value and is an estimate of the full population variance, that in most cases cannot be directly observed due to lack of data of the whole population.\nMathematically, the population variance is defined as\n\\[\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n\\]\nand the sample variance results as\n\\[\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n\\]\nYou might have noticed the term \\((n-1)^{-1}\\) is different from what you probably expected (\\(n^{-1}\\)). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution.\nThe standard deviation obtains as the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of \\(a\\), then the variance will increase by \\(a^2\\).\n\\[\nVar(aX+b)=a^2V(X)\n\\]\nYou can also conveniently compute a variance for the sum of two random variables\n\\[\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n\\]\nwhich in case of independence reduces to the sum of the individual variances due to the fact that \\(E(XY) = E(X)E(Y)\\)."
  },
  {
    "objectID": "content/part_i/01_prob.html#covariance",
    "href": "content/part_i/01_prob.html#covariance",
    "title": "Probability Theory and Statistics",
    "section": "Covariance",
    "text": "Covariance\nCovariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction.\nas_tibble() or as.data.frame() can be used to change e.g. matrices or lists into tables.\nrbind() or bind_rows() are used to bind rows, vectors or tables to one table. They behave slightly different, so you should know both functions. The counterparts for columns are cbind() and bind_cols().\nTo rename columns, you can use rename(new_name = old_name).\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nUsing compatibility `.name_repair`.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.8  0.0\n[2,]  0.8  1.0 -0.5\n[3,]  0.0 -0.5  1.0\n\n\nFrom left to right, the graph shows a positive covariance, a negative covariance and no covariance at all.\n\n\n\n\n\n\n\n\n\nIf the equality \\(E(XY) = E(X)E(Y)\\) holds, then it implies a covariance of 0 between the variables \\(X\\) and \\(Y\\). Covariance is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, it thus can be said that it is the sum of variances of both random variables plus two times their covariance.\nAs a matter of form, the formula for the covariance of the random variables \\(X\\) and \\(Y\\) is\n\\[\nCov(X,Y) = E(XY) - E(X)E(Y)\n\\]\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the correlation which can be derived from the covariance if the individual variances are known.\n\\[\n\\text{Corr}(X,Y) = \\dfrac{C(X,Y)}{\\sqrt{V(X)V(Y)}}\n\\]\nThe correlation is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign."
  },
  {
    "objectID": "content/fundamentals/01_prob.html",
    "href": "content/fundamentals/01_prob.html",
    "title": "Probability Theory and Statistics",
    "section": "",
    "text": "Before we dive into topics of causal inference, we review some basic concepts of probability and statistics. All methods that we will use later in this course are based on statistical models and these in turn, require probability theory. But we will keep it as short as possible as our focus and learning goal lies more on applications and coding than on the theoretical part."
  },
  {
    "objectID": "content/fundamentals/01_prob.html#basic-rules-of-probability",
    "href": "content/fundamentals/01_prob.html#basic-rules-of-probability",
    "title": "Probability Theory and Statistics",
    "section": "Basic rules of probability",
    "text": "Basic rules of probability\nA probability is always linked to an event typically denoted by a capital letter, e.g. \\(A\\) and expresses how likely this event is to happen. Several events build a set of events \\(A,B \\subseteq \\Omega\\). Probabilities to describe the chance of occurrence of event \\(A\\) are always between \\(0\\) and \\(1\\). The occurrence of an event \\(A\\) implies \\(P(A) = 1\\), while the impossibility of event \\(A\\) occurring implies \\(P(A) = 0\\).\n\\(P(\\overline{A})\\) is the compliment of \\(P(A)\\), so \\(\\overline{A}\\) is what happens, when \\(A\\) does not happen. Consequently as either \\(A\\) or \\(\\overline{A}\\) occurs, the probability of either event occurring is \\(P(A) + P(\\overline{A}) = 1\\). Generally, if two events \\(A\\) and \\(B\\) are mutually exclusive, i.e. only one of those events can happen, then \\(P(A \\cup B) = P(A) + P(B)\\), where \\(\\cup\\) represents the union of both events. The probability of either event happening is equal to the sum of the individual probabilities.\nIn contrast, if the two events are not mutually exclusive, the probability of both events happening is calculated by \\(P(A \\cup B) = P(A) + P(B) + P(A \\cap B)\\) with \\(P(A \\cap B)\\) being the intersection of both events, i.e. the probability of both events happening. This formula is based on the addition rule.\nThe aforementioned intersection \\(P(A \\cap B)\\) can be calculated by the multiplication rule: \\(P(A \\cap B) = P(A|B) * P(B) = P(B|A) * P(A)\\), where \\(P(A|B)\\) denotes the probability of \\(A\\) happening given that \\(B\\) has happened.\nThe total probability rule states that \\(P(A) = P(A \\cap B) + P(A \\cap \\overline{B})\\).\nAlready used above, \\(P(A|B)\\) is called a conditional probability and is defined by:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt can be thought of as the probability of an event \\(A\\) after you know that \\(B\\) is true. Essentially, it computes the possibility of event \\(A\\) and \\(B\\), normalized by the probability of \\(B\\) occurring.\nAnother important concept when dealing with probabilities of events is stochastic independence. In case of two events being independent, the conditional probability is equal to the probability of the event happening anyways:\n\\[\nP(A \\mid B) = P(A)\n\\]\nA typical example of independence is to roll a die twice. The second roll does not depend on the first one and each outcome is as likely independent of the first roll. This also means that\n\\[\nP(A \\cap B) = P(A) \\ P(B)\n\\]"
  },
  {
    "objectID": "content/fundamentals/01_prob.html#probability-tree",
    "href": "content/fundamentals/01_prob.html#probability-tree",
    "title": "Probability Theory and Statistics",
    "section": "Probability Tree",
    "text": "Probability Tree\nAn intuitive way to think about probabilities are probability tree. Branches from one node always sum to \\(1\\) in probability as one (and only one) of the events has to happen. The probability of two consecutive events is obtained by multiplying the probabilities. Here, all events are independent and therefore all probabilities are \\(0.5\\) and therefore: \\(P(A \\mid B) = P(A)\\).\n\n\n\nhttps://www.mashupmath.com/blog/probability-tree-diagrams\n\n\n!!! Explanation conditional probability (dependent example)\n!!! Assignment: some empty gaps"
  },
  {
    "objectID": "content/fundamentals/01_prob.html#set-theory",
    "href": "content/fundamentals/01_prob.html#set-theory",
    "title": "Probability Theory and Statistics",
    "section": "Set Theory",
    "text": "Set Theory\nAn extremely useful tool to visualize the occurrence and relationship between events are Venn diagrams from set theory.\nLet’s use an example to understand the rules mentioned above using a Venn diagram: suppose you are working in a company that has developed an application available on three different kind of devices, smartphones, tables and computers. So far your pricing plan is very simple and you have just charged the same amount from all customers, regardless of what and how many devices they use.\nBut now you want to review your pricing plan and evaluate whether it could make sense to offer pricing plans that differ in the device and number of maximum devices that can be used per account. So first of all you collect usage data of a random sample of 1000 customers from the last month to get an idea of the current distribution.\nlibrary() loads external packages/libraries containing functions that are not built in base R.\ntibble() is the most convenient way to create tables. You specify column name and content and assign your tibble to an object to store it.\nifelse(test, yes, no) is a short function for if…else statements. The first argument is a condition that is either TRUE or FALSE and determines whether the second or third argument is returned.\nrbinom(n, size, prob) samples n values from a binomial distribution of a given size and with given probabilities prob.\nmutate() is one of the most important functions for data manipulation in tables. It is used to either create or change variables/columns. You provide the column name (new or existing) and then specify how to create or change the values in that specific column. For example, mutate(table, new_variable = existing_var / 100), which is equivalent to table %>% mutate(new_variable = existing_var / 100).\n\n# Load tidyverse package\nlibrary(tidyverse)\n\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.8\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.2     ✓ forcats 0.5.1\n\n\n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n# Nmber of obervations\nn <- 1000\n\n# Create tibble\napp_usage <- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.6),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.7)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.5))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage <- app_usage %>%\n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\nHere, we simulated some artificial data. Seeing the formulas used for constructing the data, we already know that e.g. customers tend not to use the app on both tablet and computer.\nTo see the first lines of a table (for example a tibble() or a data.frame(), you can use the head(table, n) function, where n specifies how many rows you want to see.\n\nhead(app_usage, 10)\n\n\n\n  \n\n\n\nA general overview of total customers per device category shows that in the smartphone category there are the most users and in the computer category there are the least.\nSumming all values by column is done by colSums(table). For rows, you would use rowSums(table).\n\ncolSums(app_usage)\n\n   user_id smartphone     tablet   computer \n    500500       1000        399        334 \n\n\nSumming the \\(user\\_id\\) does not make any sense. We could ignore it, but we can also just access the columns we want to sum. There are several ways.\nTo access only specified columns, you can provide the location or names in square brackets or you can use the select() function.\n\ncolSums(app_usage[, 2:4])\n\nsmartphone     tablet   computer \n      1000        399        334 \n\ncolSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\n\nsmartphone     tablet   computer \n      1000        399        334 \n\napp_usage %>% select(smartphone, tablet, computer) %>% colSums()\n\nsmartphone     tablet   computer \n      1000        399        334 \n\n\nNow let’s see what the Venn diagram says, which is a diagram showing the relation between sets. We can see the union, intersection differences and complements in the diagram.\nwhich() checks a condition and returns the indices.\n\nset_phon <- which(app_usage$smartphone == 1)\nset_tabl <- which(app_usage$tablet == 1)\nset_comp <- which(app_usage$computer == 1)\nsets_all <- list(set_phon, set_tabl, set_comp)\n\nlibrary(ggVennDiagram)\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\")) +\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\"))\n\n\n\n\n\n\n\n\n\n\n\nGeneric Venn diagram\n\n\nUsing the Venn diagram, we are able to answer questions like the following:\n\nWhat is the percentage of customers using all three devices?\nWhat is the percentage of customers using at least two devices?\nWhat is the percentage of customers using only one device?\n\nWe can also use the example to go through the basic probability rules defined above.\nAddition rule:\nWhat is the percentage of customers using a smartphone, a tablet or both devices?\n\\(P(T \\cup S) = P(T) + P(S) - T \\cap S)\\)\nMultiplication rule:\nGiven that a customer uses a computer, how likely is he/she to use a tablet as well?\n\\(P(T|C) = \\frac{P(T \\cap C)}{P(C)}\\)\nTotal probability rule:\nWhat is the fraction of customers using a computer?\n\\(P(C) = P(C \\cap T) + P(C \\cap \\overline{T})\\)"
  },
  {
    "objectID": "content/fundamentals/01_prob.html#bayes-theorem",
    "href": "content/fundamentals/01_prob.html#bayes-theorem",
    "title": "Probability Theory and Statistics",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nA very important theorem in probability theory is Bayes theorem that we get by reformulating the multiplication rule\n\\[\nP(A ∩ B) = P(A|B)*P(B) \\\\\nP(B ∩ A) = P(B|A)*P(A)\n\\]\nUsing the equality of \\(P(A ∩ B)\\) and \\(P(B ∩ A)\\) we arrive at\n\\[\nP(B|A)*P(A) = P(A|B)*P(B)\n\\]\nand finally at the Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nBayes theorem states the mathematical formulation for determining a conditional probability, exemplary the likelihood of \\(A\\) occurring conditioned on \\(B\\) having happened before.\nYou will often hear Bayes theorem in connection with the terms updating beliefs. You start with a prior probability \\(P(A)\\) and collecting evidence \\(P(B)\\) and the likelihood \\(P(B|A)\\), you update your prior probability to get a posterior probability \\(P(A|B)\\). That is in fact the foundation of Bayesian inference. Look it up if you want, but you won’t need Bayesian inference for this course.\n\\[\nPosterior = \\frac{Likelihood * Prior}{Evidence}\n\\]"
  },
  {
    "objectID": "content/fundamentals/01_prob.html#application",
    "href": "content/fundamentals/01_prob.html#application",
    "title": "Probability Theory and Statistics",
    "section": "Application",
    "text": "Application\nTo understand how useful Bayes theorem is, let’s use an example: Imagine, you are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\nTo assess the usefulness of the tool in practice you want to know the following probabilities:\n\nWhat is the probability that when the alarm is triggered the product is found to be flawless?\nWhat is the probability that when the alarm is triggered the product is found to have flaws?\n\nUsing Bayes theorem and the formulas will help you to arrive at the correct answers and guide your decision whether to buy the tool.\nWe should start by defining the events and event sets:\n\\(A\\): product is faulty vs. \\(\\overline{A}\\): product is flawless\n\\(B\\): alarm is triggered vs. \\(\\overline{B}\\): no alarm\nAlso, from our past experience and the producers specifications we already know some probabilities:\n\\(P(B|A) = 0.97\\) and consequently \\(P(\\overline{B}|A) = 0.03\\)\n\\(P(B|\\overline{A}) = 0.01\\) and consequently \\(P(\\overline{B}|\\overline{A}) = 0.99\\)\n\\(P(A) = 0.04\\) and consequently \\(P(\\overline{A}) = 0.96\\)\nNote, that what we are looking for is not the same as what the manufacturer states in his/her specifications. What we are looking for is \\(P(\\overline{A}|B\\) (1) and \\(P(A|B)\\) (2) and we will need Bayes theorem to obtain those probabilities.\nLet’s recall Bayes theorem:\n\\[\nP(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\n\\]\nNow we can see, that we have all the information we need to compute the desired probabilities. \\(P(A|B)\\) (2) is calculated by\n\n(0.97 * 0.04) / ((0.97 * 0.04) + (0.01 * 0.96))\n\n[1] 0.8\n\n\nand \\(P(\\overline{A}|B)\\) (1) by\n\n(0.01 * 0.96) / ((0.01 * 0.96) + (0.97 * 0.04))\n\n[1] 0.2\n\n\nThese results show that in case the alarm is triggered, there is still a possibility of about 20% that the product is flawless and in only 80% of cases the product is faulty. Depending on your current method of quality assurance, these numbers might make you think more about buying the tool. And also note, how these numbers give a different perspective compared to what the manufacturer said."
  },
  {
    "objectID": "content/fundamentals/01_prob.html#random-variable",
    "href": "content/fundamentals/01_prob.html#random-variable",
    "title": "Probability Theory and Statistics",
    "section": "Random Variable",
    "text": "Random Variable\nFor starters, let’s define what a random variable is. Often represented by letters such as \\(X\\), a random variable has a set of values, also called sample space, of which any could be the outcome if we draw from this random variable. Think for example about a die (six possible outcomes). The likelihood of outcomes are defined by a probability distribution that assigns each outcome a probability (for a die, 1/6 for each outcome). A random variable can either take on discrete (e.g. die( or continuous values (e.g. average height of random people)."
  },
  {
    "objectID": "content/fundamentals/01_prob.html#expected-value",
    "href": "content/fundamentals/01_prob.html#expected-value",
    "title": "Probability Theory and Statistics",
    "section": "Expected value",
    "text": "Expected value\nBecause a random variable can take on different values, we cannot represent it as a scalar. However, the expected value of random variable is a scalar and represents something like a “summary” of the random variable with its values and its probability distribution.\nBefore we define the expected value, we need to introduce the summation operator \\(\\sum\\), denoted by the Greek capital Sigma.\nIn general, it is used to reduce the sum of a sequence of numbers, like sampled values from a random variable, \\(x_1, x_2, …, x_n\\) to a shorter and more readable form\n\\[\n\\sum_{i=1}^nx_i \\equiv x_1+x_2+\\ldots+x_n\n\\]\nwith the arbitrary index of summation \\(i\\) being the lower limit and \\(n\\) the upper limit.\nBy basic math rules, the following simplifications are possible, where \\(c\\) is a constant:\n\\[\n\\sum_{i=1}^nc=nc\n\\]\nand\n\\[\n\\sum_{i=1}^ncx_i=c\\sum_{i=1}^nx_i  \n\\]\nA statement, that you will see a lot in many applications and formulas is the average of a sequence of numbers, typically denoted by a line over a variable. Some equivalent forms of writing down the calculation for the average are shown here:\n\\[\n\\overline{x} = \\dfrac{1}{n} \\sum_{i=1}^n x_i \\\\ = \\sum_{i=1}^n \\dfrac{1}{n} x_i \\\\ = \\sum_{i=1}^n \\dfrac{x_i}{n} \\\\ =\\dfrac{x_1+x_2+\\dots+x_n}{n} \\\\ = n^{-1} \\sum_{i=1}^{n} x_i\n\\]\nThis brings us to the first important basic concept we will need throughout the course, the expected value, also called population mean. The expected value of a random variable \\(X\\) is defined as the weighted average of possible values the random variable can take. The weight is equal to the probability of the random variable taking a specific value.\nConsidering a finite list of potential values \\(x_1, x_2, …, x_k\\) with probabilities \\(p_1, p_2, …, p_k\\), the expectation of \\(X\\) can be computed by\n\\[\nE(X) = x_1*p_1 + x_2*p_2 + ... + x_k*p_k = \\sum_{j}^{k} x_i*p_i\n\\]\nAs an example, the expected value of a roll of a fair six-sided die, i.e. all outcomes are equally probable with probability of 1/6, is:\n\\[\nE(X) = \\frac{1}{6}*1 + \\frac{1}{6}*2 + ... + \\frac{1}{6}*6 = \\frac{1}{6} \\sum (1 + 2 + ... + 6)\n\\]\nTo replicate values and create a vector rep() can be used. It takes the value and number of times it should be replicated.\n\n# Vector of probabilities (all equal)\np <- rep(1/6, 6)\n\n# Vector of possible outcomes\nx <- 1:6\n\n# Expected value\nsum(p*x)\n\n[1] 3.5\n\n\nAdditional rules regarding the calculation of expected values that can be useful are:\n\\[\nE(aW+b) = aE(W)+b\\ \\text{for any constants $a$, $b$} \\\\\nE(W+H) = E(W)+E(H) \\\\E\\Big(W - E(W)\\Big) = 0\n\\]\nKnowing how to compute the expected value of a random variable is essential for computing other statistics such as variance, standard deviation, covariance, correlation etc."
  },
  {
    "objectID": "content/fundamentals/01_prob.html#variance",
    "href": "content/fundamentals/01_prob.html#variance",
    "title": "Probability Theory and Statistics",
    "section": "Variance",
    "text": "Variance\nBefore we define variance, let’s see why it is important to know it. On both graphs we see almost the same line (small difference because of sampling) going through the data points. It is the line that fits the data best. However, there is a difference in how the data points are distributed. On the left graph, there is high variance compared to the right graph. That means, the data is more dispersed.\n!!! conditional expectation\nseq(from, to, by) or seq(from, to, length.out) returns a vectors with a sequence as specified by the arguments.\nrnorm(n, mean, sd) samples values from the normal distribution. n specifies the number of values, mean and sd define the parameters of the normal distribution.\nmap() is a very useful function when you want to apply a function to each element of a list or a vector.\npivot_longer() changes the format of a table by pivoting columns into rows. To pivot rows into columns, you need pivot_wider().\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nMathematically, the variance is defined as the expectation of the squared deviation of a random variable from its population or sample mean. The sample variance indicates how far a set of observed values spread out from their average value and is an estimate of the full population variance, that in most cases cannot be directly observed due to lack of data of the whole population.\nMathematically, the population variance is defined as\n\\[\nVar(W)=\\sigma^2=E\\Big[\\big(W-E(W)\\big)^2\\Big]\\\n\\]\nand the sample variance results as\n\\[\n\\widehat{\\sigma}^2=(n-1)^{-1}\\sum_{i=1}^n(x_i - \\overline{x})^2\n\\]\nYou might have noticed the term \\((n-1)^{-1}\\) is different from what you probably expected (\\(n^{-1}\\)). This is due to a correction, which at this point you should not have to worry about. However, the larger the sample is, the less important this correction is.\nA related measure is the standard deviation, which does not have as many desirable properties for computational purposes but is often reported after all calculations to show the spread of distribution.\nThe standard deviation obtains as the square root of the variance:\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]\nA useful and convenient properties of the variance is that constants have a variance of 0. But if you want to scale a random variable by a constant factor of \\(a\\), then the variance will increase by \\(a^2\\).\n\\[\nVar(aX+b)=a^2V(X)\n\\]\nYou can also conveniently compute a variance for the sum of two random variables\n\\[\nVar(X+Y)=Var(X)+Var(Y)+2\\Big(E(XY) - E(X)E(Y)\\Big)\n\\]\nwhich in case of independence reduces to the sum of the individual variances due to the fact that \\(E(XY) = E(X)E(Y)\\)."
  },
  {
    "objectID": "content/fundamentals/01_prob.html#covariance",
    "href": "content/fundamentals/01_prob.html#covariance",
    "title": "Probability Theory and Statistics",
    "section": "Covariance",
    "text": "Covariance\nCovariance determines the relationship between two or more random variables, i.e. how they behave to each other. For example, when the weather is hot, there are more ice cream sales, so these two random variables move in the same direction. Others do not have any statistical association or move into opposite direction.\nas_tibble() or as.data.frame() can be used to change e.g. matrices or lists into tables.\nrbind() or bind_rows() are used to bind rows, vectors or tables to one table. They behave slightly different, so you should know both functions. The counterparts for columns are cbind() and bind_cols().\nTo rename columns, you can use rename(new_name = old_name).\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nUsing compatibility `.name_repair`.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.8  0.0\n[2,]  0.8  1.0 -0.5\n[3,]  0.0 -0.5  1.0\n\n\nFrom left to right, the graph shows a positive covariance, a negative covariance and no covariance at all.\n\n\n\n\n\n\n\n\n\nIf the equality \\(E(XY) = E(X)E(Y)\\) holds, then it implies a covariance of 0 between the variables \\(X\\) and \\(Y\\). Covariance is a measure of linear dependency and hence, independence implies a covariance of 0. Looking back at the formula of the variance of the sum of two random variables, it thus can be said that it is the sum of variances of both random variables plus two times their covariance.\nAs a matter of form, the formula for the covariance of the random variables \\(X\\) and \\(Y\\) is\n\\[\nCov(X,Y) = E(XY) - E(X)E(Y)\n\\]\nBut, similar to variance, the interpretation of a covariance is not very easy and in most cases, for the purpose of interpretation, it is preferred to look at the correlation which can be derived from the covariance if the individual variances are known.\n\\[\n\\text{Corr}(X,Y) = \\dfrac{C(X,Y)}{\\sqrt{V(X)V(Y)}}\n\\]\nThe correlation is a standardized measure and is by construction bound between -1 and 1. High values in magnitude (close to 1 or -1) indicate a very strong linear relationship, while the direction of this relationship is represented by the algebraic sign."
  },
  {
    "objectID": "content/fundamentals/04_dag.html",
    "href": "content/fundamentals/04_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "We have already learned that correlation and causation can easily be confused. Now, we will define concepts and acquire tools that help us in developing credible identification strategies to separate correlation from causation. One essential part is graphically modeling your theoretical knowledge about the data-generating process.\nIn causal inference, directed acylic graphs (DAGs) do the graphic modeling part. They are the foundation of any analysis strategy and moreover communicate your research plan.\nDAGs show what variables are important for your analysis and how you think they are related. Information to draw a DAG can come things like:\n\nDomain knowledge\nState-of-the art theory\nPlausible assumptions and hypotheses\nObservations and experiences\nConversations with experts\n\nA DAG should map what you know about the phenomena you are studying into a visual representation. By deciding how to draw your graph you have to ask yourself:\n\nBetween what variables do you think is a causal relationship?\nBetween what variables there is no causal relationship?\n\nBesides being helpful in guiding your analysis and identification strategy, DAGs also show your research design to your audience.\nA simple example of a DAG could be the effect of having an university degree on future salary: at first, it might be intuitive to say that future salary increases when you get an university degree.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut zooming out and thinking about why a university degree is correlated with higher salaries could lead you to the idea that both university degree and salary are influenced by individuals’ ability. People who are more capable tend to go to university and will be more successful in their later career regardless of the university degree.\nIt is very likely that the truth is that both ability and university degree are factors for future salary, but just to get your assumptions clear and guide you in your research strategy, DAGs are of a great benefit."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#chain",
    "href": "content/fundamentals/04_dag.html#chain",
    "title": "Directed Acyclic Graphs",
    "section": "Chain",
    "text": "Chain\nOne element is a chain of random variables where the causal effect flows in one direction.\n\n\n\n\n\n\n\n\n\nAn example of such a causal mechanism (page 37, Pearl) could be the effect of work hours on training and training on race time. In the DAG, the variables would be:\n\n\\(X\\): work hours\n\\(Z\\): training\n\\(Y\\): race time\n\nThis mechanism is also sometimes called mediation, because \\(Z\\) mediates the effect of \\(X\\) on \\(Y\\).\nIn terms of dependencies,\n\n\\(X\\) and \\(Z\\): dependent, as indicated by the arrow.\n\\(Z\\) and \\(Y\\): dependent, as indicated by the arrow.\n\\(X\\) and \\(Y\\): dependent, as indicated by the arrow (going through \\(Z\\)).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, because when we condition on the training amount, that means we hold training amount fixed at a particular level, then there is no effect from work hours to race time as there is no direct effect, but only an effect through the amount of training. In other words, for individuals that differ in the hours they work but still have the same amount of training, there is no association between working hours and race time.\n\nRule: Two variables, \\(X\\) and \\(Y\\), are conditionally independent given \\(Z\\), if there is only one unidirectional path between \\(X\\) and \\(Y\\) and \\(Z\\) is any set of variables that intercepts that path."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#fork",
    "href": "content/fundamentals/04_dag.html#fork",
    "title": "Directed Acyclic Graphs",
    "section": "Fork",
    "text": "Fork\nAnother mechanism is the fork, also called common cause.\n\n\n\n\n\n\n\n\n\nThe reason it is called common cause is that, as depicted above, both \\(X\\) and \\(Y\\) are caused by \\(Z\\).\nTo illustrate it, consider the following scenario: \\(Z\\) represents the temperature in a particular town and \\(X\\) and \\(Y\\) represent ice cream sales and number of crimes in that same town, respectively.\nThen, you could hypothesize that with increasing temperature people start to eat and buy more ice cream and also more crimes will happen as more people are outside which presents a greater opportunity for crime. Therefore ice cream sales and number of crimes tend to behave similarly in terms of direction and magnitude, they correlate.\nHowever, there is no reason to assume there is a causal relationship between ice cream sales and the number of crimes.\nAgain, let’s check in term of dependencies:\n\n\\(Z\\) and \\(X\\): dependent, as indicated by arrow.\n\\(Z\\) and \\(Y\\): dependent, as indicated by arrow.\n\\(X\\) and \\(Y\\): dependent, as both are influenced by \\(Z\\). \\(X\\) and \\(Y\\) change both with variation in \\(Z\\).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): independent, as for a fixed level of temperature, there is no association anymore.\n\nRule: If variable \\(Z\\) is a common cause of variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\)are independent conditional on X."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collision",
    "href": "content/fundamentals/04_dag.html#collision",
    "title": "Directed Acyclic Graphs",
    "section": "Collision",
    "text": "Collision\nThe last mechanism is the collision, which is also called common effect.\n\n\n\n\n\n\n\n\n\nIt is the reflection of the fork and both \\(X\\) and \\(Y\\) have a common effect on the collision node \\(Z\\).\nThis time, as we are already used to it, we will start to list the dependencies and then use an example for illustration:\n\n\\(X\\) and \\(Z\\): dependent, as indicated by arrow.\n\\(Y\\) and \\(Z\\): dependent, as indicated by arrow.\n\\(X\\) and \\(Y\\): independent, there is no path between \\(X\\) and \\(Y\\).\n\\(X\\) and \\(Y\\) conditional on \\(Z\\): dependent.\n\nA popular way to illustrate the common effect, especially the last dependency, is to take an example that is related to Berkson’s paradox.\nFor example, imagine the variables to be:\n\n\\(X\\): attractiveness\n\\(Y\\): talent\n\\(Z\\): celebrity\n\nFirst of all, in the general population, there is no correlation between attractiveness and talent (3rd dependency). Second, being either attractive or having a talent will help you to become a celebrity (1st and 2nd dependency).\nBut what about the last dependency? Why are attractiveness and talent suddenly correlated when conditioned on e.g. being a celebrity? That is because when you know someone is a celebrity and has no talent, the likelihood that he/she is attractive increases because otherwise he/she would likely not be a celebrity. Vice versa, if you know someone is a celebrity and is not attractive, he/she is probably talented in some form.\nRule: If a variable \\(Z\\) is the collision node between two variables \\(X\\) and \\(Y\\), and there is only one path between \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are unconditionally independent but are dependent conditional on \\(Z\\) (and any descendants of \\(Z\\))."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#confounding",
    "href": "content/fundamentals/04_dag.html#confounding",
    "title": "Directed Acyclic Graphs",
    "section": "Confounding",
    "text": "Confounding\nA very common problem when trying to prove causal effects is confounding.\n\n\n\n\n\n\n\n\n\nIt is the same DAG as in the introduction. So just recall and imagine \\(X\\) was university degree, \\(Y\\) future salary and \\(Z\\) ability.\nThere are two paths from \\(X\\) to \\(Y\\):\n\ndirect path \\(X \\rightarrow Y\\)\nbackdoor path \\(X \\leftarrow Z \\rightarrow Y\\)\n\nFirst of all, it is important to clarify what effect we are actually interested in and that is the direct causal effect of university degree on future salary, \\(X\\) on \\(Y\\).\nThe indirect effect is not causal but is only spurious correlation induced by \\(Z\\) which we are generally not interested in. It is called backdoor path.\nSo how do we proceed to extract that effect? We need to somehow remove the association between \\(X\\) and \\(Y\\) that is only due to variation in \\(Z\\). And from the previous section we know how to do that. \\(X\\) and \\(Y\\) need to be independent conditional on \\(Z\\) and block the path from \\(X\\) to \\(Y\\) over \\(Z\\).\nBut what does blocking the path mean? It means that we have to condition on \\(Z\\), to keep it at a fixed level. Then, the variations in \\(X\\) that cause \\(Y\\) to vary are not due to \\(Z\\) because it does not vary at all and cannot have an impact on either \\(X\\) or \\(Y\\). Doing that we closed the backdoor and are able to retrieve the causal effect.\nNot blocking the path would falsify our results and is what is called the omitted variable bias. That is why \\(Z\\) is called confounder, because it confounds the ability to measure the causal effect.\nHowever, the main problem is that in many cases you might not be able to block the path for two different reasons:\n\nYou are aware of the confounder, but you did not collect data for it\nYou are not aware of the confounder (and probably did not collect data for it)\n\nThis stresses the importance of theoretical knowledge about the phenomenon you are researching. Without it, it is very unlikely that you can prove truly causal effects. The risk of not paying attention to confounders gets clear when we look at graphs visualizing an example of Simpson’s paradox: accounting for a third variable reverses the sign of correlation.\nWe can illustrate it with an imaginary example. Let’s assume you want to measure how a specific characteristic affects salary. So you start to collect data about both variables, throw them into a regression and your result tells you that there is a positive correlation. But what happens if you take the education level into account? You can see how the lines show a positive correlation on the left and a negative correlation on the right. When you include a third variable, the relationship reverses.\nIt is neither wrong or right to always include or exclude variables, but it depends on the application and question you want to answer. This is why causal reasoning is so important.\n\n\nCode\n# Simpson's paradox ----\nn <- 1e+03\neducation <- rbinom(n, 2, 0.5)\ncharacter <- rnorm(n) + education\nsalary <- education * 2 + rnorm(n) - character * 0.3\n\nsalary <- sample(10000:11000,1) + scales::rescale(salary, to = c(0, 100000))\ncharacter <- scales::rescale(character, to = c(0, 7))\neducation <- factor(education, labels = c(\"Low\", \"Medium\", \"High\"))\n\ndf <- tibble(\n  salary,\n  character,\n  education\n)\n\n# Not conditioning on education\nsimps_not_cond <- ggplot(df, aes(x = character, y = salary)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on education  \nsimps_cond <- ggplot(df, aes(x = character, y = salary, color = education)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsimps_not_cond\nsimps_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#collider",
    "href": "content/fundamentals/04_dag.html#collider",
    "title": "Directed Acyclic Graphs",
    "section": "Collider",
    "text": "Collider\nYou probably noticed that the confounding example was related to the common cause in the previous section. The next example is related to the common effect mechanism.\n\n\n\n\n\n\n\n\n\nNow consider the following example: a research group wants to examine if there is a causal relationship between grade point averages (\\(X\\)) and musical talent (\\(Y\\)). They can use data from their own university where both is measured for every student as scoring high in one of these characteristics substantially increases your chance of being accepted.\nNow the researchers, who hypothesized a positive or no correlation between \\(X\\) and \\(Y\\), perform a simple analysis and, to their surprise, find out that there is a strong negative correlation. This strange result can be explained by collider bias. Implicitly, as they only used data from their own university students, they also conditioned on a collider \\(Z\\), which is “accepted to university” and is fixed on a constant level of actually being accepted to the university.\nAgain, we have two paths from \\(X\\) to \\(Y\\), but one thing is different: the backdoor path is already closed because \\(Z\\) is a collider, both arrows point toward it.\nTo correct their results they would need to have data from the whole population and not only students from a university that puts focus on either grade point averages or musical talent. Having a full population as a sample would probably lead to the result that there is no correlation between \\(X\\) and \\(Y\\).\nThis case (collider) is different to the case before (confounding), where conditioning was the correct solution. But if you have a collider in you DAG, make sure not to condition on it as it creates a dependence between \\(X\\) and \\(Y\\). You “open” a backdoor path that was closed before, just due to the presence of the collider.\nCollider bias often arises when your sample is not very representative of the population you are making claims about. How that could change your result can be seen in an example Berkson’s paradox. While there is no correlation for the whole population, for smaller subgroups there are. And therefore, it is crucial that you clearly state what effect you are interested in.\n\n# Berkson's paradox ----\nn <- 1e+03\nability     <- rnorm(n)\nmotivation  <- rnorm(n)\naptitude    <- 1/2 * ability + 1/2 * motivation + rnorm(n, 0, .1)\n\ndf <- tibble(\n  ability    = ability,\n  motivation = motivation,\n  aptitude   = aptitude,\n  student    = ifelse(aptitude > 0, \"student\", \"no_student\")\n)\n\n# Not conditioning on student\nberk_not_cond <- ggplot(df, aes(x = motivation, y = ability)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on student  \nberk_cond <- ggplot(df, aes(x = motivation, y = ability,\n                            color = student, \n                            alpha = student)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F) +\n  scale_color_manual(values = c(\"student\" = ggthemr::swatch()[4],\n                                \"no_student\" = ggthemr::swatch()[5])) +\n  scale_alpha_manual(values = c(\"student\" = 1, \"no_student\" = 0.2)) +\n  theme(legend.position = \"right\")\n\n# Plot both\nberk_not_cond\nberk_cond"
  },
  {
    "objectID": "content/fundamentals/04_dag.html#d-separation",
    "href": "content/fundamentals/04_dag.html#d-separation",
    "title": "Directed Acyclic Graphs",
    "section": "d-separation",
    "text": "d-separation\nOne concept, that we have not named yet but implicitly used, is d-separation. If an effect of \\(X\\) on \\(Y\\) is d-separated, there is no statistical association that can flow between \\(X\\) and \\(Y\\) except for the direct effect. In fact, d-separation determines conditional independence.\nD-separation formalizes what we have already learned when going through the tree types of association.\nPractically (http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html) , it is easier to define the opposite, d-connection:\n\nRule 1: unconditional separation: \\(X\\) and \\(Y\\) are d-connected if there is an unblocked path between them. (As an example, imagine a confounder, that is not conditioned on.)\nRule 2: blocking by conditioning: \\(X\\) and \\(Y\\) are d-connected, conditioned on a set of \\(Z\\) nodes, if there is a collider-free path between \\(X\\) and \\(Y\\) that traverses no member. (Think of a mediated effect that takes away parts from the direct effect.)\nRule 3: conditioning on colliders: If a collider is a member of conditioning set \\(Z\\), or has a descendant in \\(Z\\), then it no longer blocks any path that traces this collider. (Image the collider example in the previous section.)\n\n!!! MAYBE CHANGE TO https://de.wikipedia.org/wiki/D-Separation\nKnowing these rules and having mapped our assumptions into the DAG allows us to treat observational data like experimental data and simulate interventions as we would have conducted an experiment. However, it is not a silver bullet as in many cases data availability will stop you from isolating the causal effect. For example, if you do not observer a confounder, you cannot control for it. All empirical work requires theory and with observational data we need to be extra careful to make sure to actually extract the effects we are interested in."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "href": "content/fundamentals/04_dag.html#backdoorfrontdoor-criterion",
    "title": "Directed Acyclic Graphs",
    "section": "Backdoor/Frontdoor Criterion",
    "text": "Backdoor/Frontdoor Criterion\nSpecial cases that are derived from d-separation rules are the backdoor and frontdoor criterion/adjustment.\nTo satisfy the backdoor criterion, we have to make sure all backdoors are closed, which, as already mentioned, differs for confounders and colliders.\n\n\n\n\n\nAgain, we want to block all other paths between \\(X\\) (treatment) and \\(Y\\) (outcome). So depending on the structure of the DAG, the following can block a path:\n\na chain or a fork whose middle node is in \\(Z\\)\na collider that is not conditioned on, which means it is not in \\(Z\\)\n\nThe frontdoor criterion, which is actually a consecutive application of the backdoor criterion, is a bit more complicated and we will leave it out for now, but in a the section about instrumental variables we will deal with it extensively."
  },
  {
    "objectID": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "href": "content/fundamentals/04_dag.html#algorithms-to-identify-causally-valid-estimates",
    "title": "Directed Acyclic Graphs",
    "section": "Algorithms to identify causally valid estimates",
    "text": "Algorithms to identify causally valid estimates\nIf DAGs become more complex because there are a lot of variables that are somehow related, we can make use of algorithms to check all the rules for us.\nOne application to help in such cases is http://dagitty.net/dags.html, where you can draw your DAG, define what is the treatment and outcome, which variables are observed and unobserved and many other things. Then it will show you what kind of adjustment is necessary to estimate the causal effect of interest.\n\n\n\n\n\nTry to build this graph on dagitty.net and look what useful information you can get from the site.\ndagitty is also implemented in R and combined with ggdag you are also able to plot your DAGs in a easy manner and obtain information needed for designing your research.\nFirst, let’s just plot the DAG. When we define exposure, which is a different term for intervention or treatment, and the outcome, they are highlighted in another color.\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n# create DAG from dagitty\ndag_model <- 'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.075,0.4\"]\nY [outcome,pos=\"0.4,0.4\"]\nZ1 [pos=\"0.2,0.2\"]\nZ2 [pos=\"0.3,0.5\"]\nZ3 [pos=\"0.2,0.6\"]\nZ4 [pos=\"0.4,0.6\"]\nD -> Y\nD -> Z3\nZ1 -> D\nZ1 -> Y\nZ2 -> Y\nZ2 -> Z3\nZ3 -> Z4\n}\n'\n# draw DAG\nggdag_status(dag_model) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\nLet’s check what paths there are from treatment \\(D\\) to \\(Y\\). Of course, there is a direct path (the causal path) and there are two other paths, of which one is closed.\n\n# find all paths\npaths(dag_model)\n\n$paths\n[1] \"D -> Y\"             \"D -> Z3 <- Z2 -> Y\" \"D <- Z1 -> Y\"      \n\n$open\n[1]  TRUE FALSE  TRUE\n\n\nWe can also plot the open paths. One path is already blocked by a collider (remember: we do not want to open that path).\n\n# plot paths\nggdag_paths(dag_model) +\n  theme_dag_cds()\n\n\n\n\n\n\n\n\nTo see what we have to adjust for to isolate the causal effect, we use adjustmentsSets(). As you might have figured out already, it is \\(Z1\\) that needs to be conditioned on.\n\n# find all nodes that need to be adjusted\nadjustmentSets(dag_model)\n\n{ Z1 }\n\n\nA very concise summary plot is returned by the function ggdag_adjustment_set(), which shows what needs to be adjusted, the open paths and the whole DAG.\n\n# plot adjustment sets\nggdag_adjustment_set(dag_model, shadow = T) +\n  theme_dag_cds() +\n  geom_dag_edges(edge_color = \"white\")\n\n\n\n\n\n\n\n\nBut what does it actually mean in practice? How do we block a path or condition on a variable?\nIf you use a linear regression, including a variable as an independent variable is the sames as conditioning on it. If you use other models, you might have to use subsets and average them or do some kind of matching where you only compare units that have the same value for the variables that has to be conditioned on.\nIn the following chapters, we will deal with a large variety of techniques and figure out clever ways to isolate causal effects."
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Content",
    "section": "",
    "text": "You can use the sidebar to navigate between topics. Have fun!\n\nR\n!!! intro studip"
  },
  {
    "objectID": "content/fundamentals/03_caus.html",
    "href": "content/fundamentals/03_caus.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Data science has gained extreme popularity in the last years and particularly in the field of machine learning, a large number of new methods and algorithms has been developed. Many of the methods are built to perform well at prediction tasks like predicting whether a customer is likely to churn, natural language processing (extracting sentiments, translating etc.), guiding self-driving cars, recognizing objects and many other applications. Those algorithms belong to the category of supervised learning and are highly data-driven (on historical data) and optimized to predict as accurate as possible. Due to increased computing power, these models have proven to be very successful in many contexts.\nHowever, there are many other contexts, where prediction is not the main focus but instead making sense of the data, understanding mechanism and processes or guiding decisions and policies plays the most important role. For example, you are not only interested in whether a customer is likely to churn, but you want to know why he/she is likely to churn. Then, we find ourselves in the realm of causality. Here, many of the newer methods are likely to fail due to their prediction-centric structure\nInstead of throwing a lot of data to a black box searching for patterns between independent variables and outcome to get a model that predicts very well, in this course, we will try to understand characteristics of the data-generating process, i.e. the system of cause and effect and extract useful information from the data. That is what science is about, explaining why things are happening.\n!!! Illustration Prediction vs Explanation\n\n\n\nA simple application, where a data-driven machine learning model would fail to improve our understanding is a naive examination of relationship between hotel room prices and hotel room bookings. Imagine, having a sample of historical data about prices and number of bookings at your hand and you would train/fit a model to that data. A prediction-focused model would now look for correlations and patterns in the data and would conclude that in times of high prices there were more bookings.\nBut what can we derive from such a model? That higher prices lead to higher bookings? This is most certainly not a correct causal relationship. Because we know that is not true and it is actually the other way around. People are more likely to book when prices are low. There are other factors playing a role like for example tourist seasons, particular events or economic factors. Only if we take these other factors into account, we will be able to obtain a valid estimate of the causal effect. And being able to understand the underlying mechanism will help us in taking the right actions and decisions.\nAnother example is the direction of causation. In models based solely on correlations, we can’t be sure in which direction the causation works. A classic example is the strong correlation between roosters crowing and the sun rising. Without knowing anything about how the world works, we could come to the conclusion that the rooster causes the sun to rise. Obviously, this is wrong.\nBased on these small examples, you should already understand the risk of relying on purely data-driven approaches. In domains, particularly in complex domains that demand a lot of theoretical consideration, data-driven approaches are not sufficient to help us in understanding and guiding our decisions. In business, management and economics, which we put our focus on, wrong conclusions might come with costly consequences. We will therefore explore how putting emphasis on causality is beneficial to business analytics and how we can move from correlation to causation."
  },
  {
    "objectID": "content/fundamentals/02_reg.html",
    "href": "content/fundamentals/02_reg.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Statistical inference aims to draw conclusions about relationships between variables in the whole population. Whole population, in this context, does not necessarily mean the whole world population but instead the set of all units we want to draw conclusions about. Units could be for example all students in a country, all children in a specific institution or things like stores, restaurants etc. In the business context, we will often deal with populations that comprise customers, employees, stores and a lot of other business-related units.\nIn practice, it is often impossible to collect data about the whole population, which is why we draw (ideally random) samples from the whole population and use statistical inference to draw conclusions about the whole population using the smaller sample. This is one main reason why we needed to introduce concepts from probability theory and statistics in the previous chapter."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#r",
    "href": "content/fundamentals/02_reg.html#r",
    "title": "Regression and Statistical Inference",
    "section": "R",
    "text": "R\nWe start using a simple example to get an intuition of how the linear regression is estimated and how it is implemented in R. For presentation purposes, we sample the data ourselves and also define the relationship between the variables.\nBefore we start, we load the R package tidyverse, which actually is collection of several useful packages for data manipulation, visualization and other data science purposes. Throughout the course, we will almost always load it.\n\nlibrary(tidyverse)\n\nWe create a tibble, which is a table containing data, with two columns, \\(x\\) and \\(y\\). For \\(x\\), we draw ten random samples from the normal distribution with a mean of 3 and a standard deviation of 1. The outcome variable \\(y\\), we make dependent from \\(x\\), i.e. for each unit \\(i\\), we create a \\(y_i\\) as\n\\[\ny_i = 0.3x_i + \\epsilon_i \\,\\,\\,\\,\\,,\n\\]\nwhere \\(\\epsilon_i\\) is random noise.\nLet’s have a look at how the data is presented. We have a table containing 100 rows and two columns. Each row is an observation for a different unit, which could be a person, a point in time or another kind of measurement. It is only important that the values in a particular row belong together.\n\n# Simluate data\nn <- 10\nlm_dat <- tibble(\n  x = rnorm(n, mean = 3, sd = 1),\n  y = 0.3*x + rnorm(n, 0, 0.2)\n)\n\nlm_dat\n\n\n\n  \n\n\n\nA handy first step if you work with a new data set is always to plot the data in a sensible way. Dealing with two-dimensional continuous data, a scatter plot is usually the best choice.\n\n# Scatter plot of x and y\nggplot(lm_dat, aes(x = x, y = y)) + \n  geom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\n\nAn experienced analyst could already see how the variables are related. There seems to be a positive correlation between \\(X\\) and \\(Y\\). However, it not a perfect correlation and there is a certain degree of noise, meaning that not all points lie on an imaginary line.\nThe goal of linear regression is now to find a line that goes through the points. But not any line, in fact, it has to be the line with the best fit. Differently put, it has to be the line that is - on average - as close to the observation points as possible.\nLet’s have a look at some random lines.\n\n\n\n\n\n\n\n\n\nYou can see there is an infinite amount of potential lines that could be chosen to go through the data. But only one of them is the line minimizing the sum of squares. The residual, which is the distance between the line and an observation, should be minimized. This also means that on average, the error is zero.\n\n\n\n\n\n\n\n\n\n!!! should be same lines in both plots\nThe resulting line is highlighted in blue.\nIf we want to mathematically compute the line in R, we have to use the lm() function and provide data and the assumed functional relationship as arguments. lm() is a function you will see a lot and it is used to fit linear models. It returns a fitted object (here: lm_mod), which we can interpret best when using summary() to show the resulting coefficients and other statistical information.\nlm() is a function that fits a linear model. You have to provide data and a regression equation in the form of for example outcome ~ regressor_1 + regressor_2 or outcome ~ ., if you want to include all variables except for the outcome as regressors. To see the computed coefficients and their statistical significance, you need to call summary().\nLooking at the regression summary, we see that the line is modeled by \\(y = -0.11797 + 0.3466*x\\). It means that for the fitted model, an increase of one unit in \\(x\\) is related to an 0.3466 increase in \\(y\\).\n\n# Fit model and print summary\nlm_mod <- lm(y ~ x, lm_dat)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = y ~ x, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18597 -0.05210 -0.00783  0.06584  0.24115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1918     0.1300   -1.48     0.18    \nx             0.3354     0.0443    7.57  6.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.14 on 8 degrees of freedom\nMultiple R-squared:  0.878, Adjusted R-squared:  0.862 \nF-statistic: 57.3 on 1 and 8 DF,  p-value: 6.48e-05\n\n\nNow, let’s check how far we are off with our predictions by plotting the regression line against the actual observations. There are two ways to to do it, by either plotting the observations \\(y_i\\) and predictions \\(\\hat{y_i}\\) for each \\(i\\) or plotting the residuals \\(r_i = y_i - \\hat{y_i}\\) and comparing it to the \\(x\\)-axis.\n\n# Add fitted values and residuals to data\nlm_dat_fit <- lm_dat %>% \n  mutate(y_fit = predict(lm_mod),\n         r   = y - y_fit)\n\n# Plot distance of actual to fit\nggplot(lm_dat_fit, aes(x = x, y = y)) + \n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = y_fit), color = ggthemr::swatch()[2]) +\n  labs(title = \"Predicted observations vs actual observations\")\n# Plot residuals\nggplot(lm_dat_fit, aes(x = x, y = r)) +\n  geom_point(size = 3) +\n  geom_smooth(method='lm', formula= y ~ x, se = F) +\n  geom_segment(aes(xend = x, yend = 0), color = ggthemr::swatch()[2]) +\n  labs(title = \"Residuals vs zero\")"
  },
  {
    "objectID": "content/fundamentals/02_reg.html#math",
    "href": "content/fundamentals/02_reg.html#math",
    "title": "Regression and Statistical Inference",
    "section": "Math",
    "text": "Math\nMathematically, the best line is found by the ordinary least squares (OLS) method.\nNote that estimation is always done in software programs or language as it gets too complex to be solved by hand very fast. However, to get a good understanding of what is going on and what is optimized, it is worth to look at the equations and conditions.\nGiven \\(n\\) samples of observed pairs of dependent and independent variables \\(\\big\\{(x_i,\\ \\textrm{and}\\ y_i): i=1,2,\\dots,n \\big\\}\\), we plug any of them into the equation\n\\[\ny_i=\\beta_0+\\beta_1x_i+u_i\n\\]\nand together with our assumptions \\(E(u) = 0\\) and \\(E(u|x)=0\\) we obtain the equations to be solved to retrieve estimates for \\(\\beta_0\\) and \\(\\beta_1\\).\nFrom the independence of \\(x\\) and \\(u\\) and our understanding of probabilities and expectations, we also know that the expected value of the product of \\(x\\) and \\(u\\) has to be zero: \\(E(xu)=0\\). Substituting \\(u\\) with $y-\\beta_0-\\beta_1$, we obtain the two conditions that when being solved give us the optimal estimates for our \\(\\beta\\) parameters.\n\\[\nE(y-\\beta_0-\\beta_1x) =0 \\\\  \nE\\Big(x[y-\\beta_0-\\beta_1x]\\Big)=0\n\\]\nTranslated into its sample counterpart:\n\\[\n\\dfrac{1}{n}\\sum_{i=1}^n\\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big) = 0 \\\\\n\\dfrac{1}{n}\\sum_{i=1}^n  \\Big(x_i \\Big[y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i \\Big]\\Big) =0\n\\]\nLooking at the sample equations, we know our sample size \\(n\\), our sampled values \\(y_i\\) and $x_i$. The coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), where the hat denotes that the parameter is not the population parameters but coming from a sample, are unknown. However, two unknowns and two equations makes the problem algebraically feasible.\nSkipping a few transformation steps, we obtain\n\\[\n\\widehat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\sum_{i=1}^n(x_i-\\overline{x})^2 } =\\dfrac{\\widehat{Cov}(x_i,y_i) }{\\widehat{Var}(x_i)}\n\\]\nWhat is very interesting to see (although we actually know it from the previous chapter) is that the OLS estimate for our \\(\\beta_1\\) is defined as the covariance of \\(X\\) and \\(Y\\) divided by the variance of $X$. It also shows that the variance of \\(X\\) has to be greater than zero, which means that not all values of \\(x_i\\) can be the same. You need to observe different values to be able to estimate how \\(y_i\\) reacts to \\(x_i\\).\n\\(\\beta_0\\) follows directly by plugging \\(\\beta_1\\) into \\(\\widehat{\\beta}_0=\\overline{y} - \\widehat{\\beta}1\\overline{x}\\). A bar above a variable always represents the sample value of that particular variable. Thus,\\(\\beta_0\\) is (as expected) constant and linear in \\(\\beta_1\\).\nKnowing the equation for the regression line, we can compute fitted values \\(y_i\\) for all \\(i\\)\n\\[\n\\begin{align}   \\widehat{y_i}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_i\\end{align}\n\\]\nIn almost all cases however, \\(\\widehat{y}_i\\) won’t be equal to \\(y_i\\) but there will be a prediction error, commonly referred to as residual \\(\\widehat{u}_i\\). Make sure that you don’t mix it up with \\(u\\), the error term, which is always unobserved.\nWhat should we already know about the residuals? As already mentioned and visualized we have been looking for the regression line that is on average as close to the observed values as possible.\nA slightly different perspective, but with the exact same implications, is therefore to look at the sum of squared residuals and bring their sum as close to zero as possible by changing the coefficients for the regression line.\nInfo: Squares are used to avoid that positive and negative errors balance each other out. You could also use absolute deviations from the fitted line, but squares have some desirable properties when doing calculus.\n\\[\n\\sum_{i=1}^n \\widehat{u_i}^2 =\\sum_{i=1}^n (y_i - \\widehat{y_i})^2                                 \\\\= \\sum_{i=1}^n \\Big(y_i-\\widehat{\\beta_0}-\\widehat{\\beta_1}x_i\\Big)^2\n\\]\nAgain, most of the residuals won’t be zero, but on average the line going through all observations is the best fitting line with residuals being zero on average."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#interpretation",
    "href": "content/fundamentals/02_reg.html#interpretation",
    "title": "Regression and Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\nSo let’s run the first regression. We will start by using all available characteristics as independent variables. That is what you will often find in studies. All variables that are available are included in the regression. We will see in later chapters why that might be dangerous.\n\n# Include all potential regressors\nlm_all <- lm(expected_cost ~ ., data = df)\nsummary(lm_all)\n\n\nCall:\nlm(formula = expected_cost ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-383.8 -128.6  -25.1  106.3  934.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 456.77687   28.55378   16.00   <2e-16 ***\nregionE      -7.64241   16.30859   -0.47   0.6394    \nregionS     -20.49277   16.60455   -1.23   0.2174    \nregionW       5.73164   16.98095    0.34   0.7358    \nsex           6.03595   11.80362    0.51   0.6092    \nsmoking     185.59358   13.32027   13.93   <2e-16 ***\nage           9.25803    0.52034   17.79   <2e-16 ***\nincome       -0.05043    0.00385  -13.11   <2e-16 ***\nbmi          -2.07466    0.76463   -2.71   0.0068 ** \nchildren    -11.06608   14.59210   -0.76   0.4484    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 186 on 990 degrees of freedom\nMultiple R-squared:  0.395, Adjusted R-squared:  0.39 \nF-statistic: 71.9 on 9 and 990 DF,  p-value: <2e-16\n\n\nThere is a lot we can learn from the regression summary. For each included coefficient and the intercept, there is a row containing the estimated coefficient, its standard error, t-statistic and p-value. The names in the R summary differ a bit, but have the same meaning.\nThe estimate is what we know as regression coefficient from before, typically denoted by \\(\\widehat{\\beta}_i\\) or other Greek letters. As it is the estimated version, it has a hat. The estimated for \\(\\beta_i\\) It tells you by how much the dependent variable \\(\\widehat{y}\\) varies when a particular variable is increased by one unit while all other variables in the model are held at a constant level. A negative coefficient suggests a negative relationship, while a positive coefficient points to a positive relationship.\nHolding all other variables constant and deriving the effect of a single variable is often described with the effect of \\(x_i\\) ceteris paribus, Latin for “the others equal”. It is really important to keep that in mind, as it allows to view the coefficient as an estimate of an isolated effect. Sometimes it is also expressed as controlling for the other variables."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#statistical-significance",
    "href": "content/fundamentals/02_reg.html#statistical-significance",
    "title": "Regression and Statistical Inference",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe standard error indicates the variation around the estimated coefficient. A high standard error indicates a lot of variation and high uncertainty while low standard errors provide more confidence in the estimate.\nThe other values are also concerned with the level of uncertainty there is in the estimation. They are related to the coefficient and standard error.\nMost widely used is the p-value, or probability value. It tests the so called null hypothesis against the observed data. The null hypothesis states that there is no correlation between the dependent variable \\(y\\) and the independent variable \\(x_1\\). The p-value shows, based on the observed data, how likely it is that your data would have occurred just by random chance. Thus, a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis. Here, the alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable.\nStatistical significance can directly be derived from the p-value and an arbitrary significance level $\\alpha$. However, the most widely used level of \\(\\alpha\\) is \\(0.05\\). Less often used are levels of \\(.1\\), \\(.01\\) or \\(.001\\).\nAn estimate with a p-value less than \\(\\alpha\\) is considered statistically significant. Expressed in statistical jargon, we reject the null hypothesis of random results when the respective p-value is lower than our significance level \\(\\alpha\\). Rejecting the null hypothesis indicates support for the alternative hypothesis (our observed estimate). Looking at the summary above, we see that \\(age\\), \\(income\\), \\(bmi\\), and \\(smoking\\) are statistically significant (at different levels though, indicated by the number of stars).\nAnother way to look at the significance of our estimates is to compute is to look at confidence intervals which derive from the estimate, standard error and the t-distribution - the same inputs as needed for p-values. A \\((1-\\alpha)\\) confidence interval has a probability of \\((1-\\alpha)*100 \\%\\) to contain the true value of our estimated coefficient. That means, if we would sample \\(100\\) times, \\(\\beta_i\\) would be contained in the sample \\((1-\\alpha)*100\\) times.\n\n# Show CIs at different levels of alpha\n# alpha = 0.05\nconfint(lm_all, level = 0.95)\n\n              2.5 %  97.5 %\n(Intercept) 400.744 512.810\nregionE     -39.646  24.361\nregionS     -53.077  12.091\nregionW     -27.591  39.054\nsex         -17.127  29.199\nsmoking     159.454 211.733\nage           8.237  10.279\nincome       -0.058  -0.043\nbmi          -3.575  -0.574\nchildren    -39.701  17.569\n\n\n\n# alpha = 0.9\nconfint(lm_all, level = 0.90)\n\n                5 %    95 %\n(Intercept) 409.766 503.788\nregionE     -34.493  19.208\nregionS     -47.830   6.845\nregionW     -22.226  33.689\nsex         -13.397  25.469\nsmoking     163.663 207.524\nage           8.401  10.115\nincome       -0.057  -0.044\nbmi          -3.334  -0.816\nchildren    -35.090  12.958\n\n\nAn estimate whose interval is either completely positive or completely negative is different from zero and rejects the null hypothesis. Simply put, that means that we expect an effect in the outcome variable when we change the independent variable associated with the positive coefficient."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#model-selection",
    "href": "content/fundamentals/02_reg.html#model-selection",
    "title": "Regression and Statistical Inference",
    "section": "Model selection",
    "text": "Model selection\nThere is variety of measures to check the model fit. Some models better suit the observed data than others and it is the researchers task to find the best model for his/her data.\nLooking at the spread of residuals (\\(\\widehat{u}_i = y_i - \\widehat{y}_i\\)) we want them to be spread evenly around zero.\n\nggplot(tibble(res = lm_all$residuals), aes(x = res)) + \n  geom_histogram(color=\"white\", alpha = 0.8, binwidth = 30) +\n  labs(x = \"residuals\", y = \"frequency\")\n\n\n\n\n\n\n\n\nWe can see that the residuals are in fact almost normally distributed.\nAfter having analyzed the residuals and our assumptions we can take a look at a measure indicating the so called goodness-of-fit is \\(R^2\\). It measures how much of the variance of the dependent variable can be explained by the independent variables. Formally:\n\\[\nR^2 = \\frac{\\text{Explained variatoin}}{\\text{Total variation}}\n\\]\nConveniently, \\(R^2\\) is always between 0 and 1 and a higher value indicates a better model fit. However, you have to treat the values with caution. Sometimes a very high \\(R^2\\) can even point to a biased model while a model with a low \\(R^2\\) can provide an adequate fit. For example, in some discipline of sciences involving human behavior like social sciences, there is inherently a greater amount of unexplained variation. Opposed to that, physical or chemical process might be easier to predict. The size of \\(R^2\\) does also not change the interpretation of the regression coefficients.\nA problem with \\(R^2\\) is that it always increases as more independent variables are included - even if they are random and have no effect at all. To correct for that behavior, it is advisable to use the \\(\\text{Adjusted} \\, R^2\\). It includes a term for the number of independent variables used.\n\\[\n\\text{Adjusted} \\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} \\,,\n\\]\nwhere \\(n\\) is the sample size and \\(p\\) the number of independent variables. This way, you can compare models and account for their scarcity.\nLet’s build a second regression model, where we only include variables that were statistically significant in the previous model.\n\n# Include only significant regressors\nlm_imp <- lm(expected_cost ~ age + bmi + smoking, data = df)\nsummary(lm_imp)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + smoking, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-470.8 -138.5  -26.4  109.7  940.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  398.516     27.818    14.3   <2e-16 ***\nage            7.115      0.482    14.8   <2e-16 ***\nbmi           -1.982      0.825    -2.4    0.016 *  \nsmoking      183.495     14.403    12.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201 on 996 degrees of freedom\nMultiple R-squared:  0.287, Adjusted R-squared:  0.285 \nF-statistic:  134 on 3 and 996 DF,  p-value: <2e-16\n\n\nExcept for \\(bmi\\), coefficients are very similar. We’ll look into that in just a second. But first let us compare both models with regard to \\(\\text{(Adjusted)} \\, R^2\\).\nTo elegantly print variables in a specified format, you can use sprintf().\n\n# Compare R^2\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_all)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.39\"\n\nsprintf(\"Adjusted R^2: %.2f\", broom::glance(lm_imp)$adj.r.squared)\n\n[1] \"Adjusted R^2: 0.29\"\n\n\nOther metrics used to select the best model out of a class of models tackling the same problem (with the same data) are Akaike’s Information Criteria (AIC) and Bayesian Information Criteria (BIC). Both AIC and BIC penalize the inclusion of additional parameters. The exact computation we will disregard for now.\n\n# AIC\nsprintf(\"AIC: %.2f\", AIC(lm_all))\n\n[1] \"AIC: 13298.62\"\n\nsprintf(\"AIC: %.2f\", AIC(lm_imp))\n\n[1] \"AIC: 13451.17\"\n\n\nFor both criteria, the model with the lowest value is preferred.\nIn many applications, it is not advisable to include all potential independent variables but to go through steps of theoretical consideration and model selection to find the best model. Throughout the course we wills stress the importance of theoretical knowledge to build valid models that allow to draw the right conclusion.\nFor example, is it correct to assume a linear relationship between \\(bmi\\) and the outcome \\(expected\\text{_}cost\\)? One could say, that a health insurance expects higher costs for individuals with a very low and a very high BMI. We can plot both variables and see whether the graph indicates some form of non-linearity.\nAnd actually (not surprisingly, because we simulated the data ourselves), there is a non-linear relationship between the variables. As hypothesized, individuals with a low and a high BMI are expected to be more costly. However, this analysis disregards all other variables and should be just an indication. We still need to model this indicated relationship in our model.\n\n# Plot relationship between BMI and expected cost\nggplot(df, aes(x = bmi, y = expected_cost)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\n\nBut can we include non-linear terms in our linear regression? In its name, there is the term “linear”, so what can we do about it?\nIn fact, it is quite simple to include non-linear terms into the regression equation. When the relationship is assumed to be like depicted in the graph above, a squared term is usually included, i.e. \\(bmi^2\\).\n\n# Include quadratic term for BMI\nlm_sq <- lm(expected_cost ~ age + bmi + I(bmi^2) + smoking, data = df)\nsummary(lm_sq)\n\n\nCall:\nlm(formula = expected_cost ~ age + bmi + I(bmi^2) + smoking, \n    data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-400.3  -82.1   -2.0   86.7  356.4 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1948.6027    41.6106    46.8   <2e-16 ***\nage            7.7799     0.2953    26.4   <2e-16 ***\nbmi         -121.7879     2.9779   -40.9   <2e-16 ***\nI(bmi^2)       2.0683     0.0507    40.8   <2e-16 ***\nsmoking      198.4633     8.8185    22.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 123 on 995 degrees of freedom\nMultiple R-squared:  0.734, Adjusted R-squared:  0.732 \nF-statistic:  685 on 4 and 995 DF,  p-value: <2e-16\n\n\nFrom the summary, we can see that including the square term significantly improves the model fit. We can take from it, that it is extremely important to rely on theoretical considerations when building models."
  },
  {
    "objectID": "content/fundamentals/02_reg.html#assumptions",
    "href": "content/fundamentals/02_reg.html#assumptions",
    "title": "Regression and Statistical Inference",
    "section": "Assumptions",
    "text": "Assumptions\n\nLinearity: Relationship between \\(X\\) and \\(Y\\) is linear.\nHomoscedasticity: Variance of residual is the same for any value of \\(X\\).\nIndependence: Observations are independent of each other. Residuals are independent of each other.\nNormality: For any fixed value of \\(X\\), \\(Y\\) is normally distributed. Residuals of the model are normally distributed."
  },
  {
    "objectID": "content/toolbox/05_rct.html",
    "href": "content/toolbox/05_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Let’s recall the fundamental problem of causal inference: we are not able to observe individual treatment effects. Only one potential outcome can be observed because there is only one state of the world.\nArguably, the most promising way to deal with it is randomization of the observation units and in particular randomized experiments, also known as randomized controlled trials (RCTs). Due to their statistical rigor and simplicity, RCTs are called the gold standard of causal inference.\nRCTs do not solve the fundamental problem of only observing one potential outcome but instead treatment and control group are randomized such that both groups are expected to be very similar. Having similar groups that either received or not received treatment, we can calculate a valid causal estimate, the Average Treatment Effect (ATE). But it is only due to the randomization of observation units (e.g. individuals) that we are able to interpret it causally."
  },
  {
    "objectID": "content/toolbox/05_rct.html#identification",
    "href": "content/toolbox/05_rct.html#identification",
    "title": "Randomized Controlled Trials",
    "section": "Identification",
    "text": "Identification\nTwo assumptions are crucial for the ATE to be interpreted causally.\n\n(1) Independence assumption\nWe have to assume independence between the potential outcomes and the treatment assignment, i.e. treatment assignment to a unit hast nothing to do with the size of treatment effect for a unit:\n\\[\nD_i \\perp (Y_{i0}, Y_{i1})\n\\]\nThis is where we exploit randomization. We can actually ensure that there is no association between potential outcomes and treatment by randomly assigning observation units to control and treatment group.\nThis way, both groups will be very similar on average, both in observed and in unobserved characteristics. They will only differ in their treatment and possibly in the observed outcome, which makes the estimation of a causal effect possible.\nPlease make sure, that you understand the formula correctly. It does not mean that there is no treatment effect. The (potential) outcome under \\(D=0\\) or \\(D=1\\) is not affected by whether a particular observation unit does or does not receive the treatment. However, the observed outcome \\(Y_i\\) might depend on \\(D_i\\), and in fact, that is the effect we are interested in.\nA related way to express it is\n\\[\nE[Y_0|D=0] = E[Y_0|D=1]\n\\]\nRegardless of the treatment value a unit receives, the expected (but not always observed) potential outcome is the same in both treatment (\\(D=1\\)) and control group (\\(D=0\\)). The mean potential outcome is equal for both groups.\nThis does also imply equality of ATE and ATT, as there is no bias and the association we see is equal to the causation.\nWhen is the independence assumption violated?\nAn example, where the independence between treatment and potential outcomes is not given is if the treatment assignment is not randomized but people are able to self-select into on of the groups.\nThen, it could happen that for e.g. more motivated people would choose the treatment and when motivation had an impact on the potential outcome, e.g. more motivated people have a higher outcome for both potential outcomes compared to less motivated people, that are more likely to be in the control group. Under these circumstances, the independence assumption would be violated.\n\n\n(2) SUTVA\nThe second assumption that needs to be fulfilled is the stable unit treatment value assumption (SUTVA).\nIt ensures that there is no interference between units. In other words, one unit’s treatment does not affect outcomes of other units. If unit \\(i\\) received a treatment, than this treatment of unit \\(i\\) should have no effect on another unit.\nImplicitly, the assumption states that there are only two potential outcomes for each unit and they only depend on a unit’s own treatment status.\nWhen is the SUTVA violated?\nIn situations where observation units are somehow clustered like e.g. in classrooms, departments or other kind of groups, violations of SUTVA can occur.\nAs an example, imagine you are running a company and select a few of your employees to participate in a program that teaches them about safety measures. After the program, it is very likely that they share some of the program content with their colleagues in their department, who might not have been selected for participation. Then, there are spillover effects.\nTo deal with violations of SUTVA you could change your selection process or change the level of analysis (analyzing clusters instead of individuals)."
  },
  {
    "objectID": "content/toolbox/05_rct.html#randomization",
    "href": "content/toolbox/05_rct.html#randomization",
    "title": "Randomized Controlled Trials",
    "section": "Randomization",
    "text": "Randomization\nIn practice, randomization is done automatically by software programs but to get an intuition, you could also think of it as e.g. flipping a coin for each observation unit or individual and assigning units that get head to the treatment group, while units that get tail are assigned to the control group (or the other way around).\nIn fact, that is already a special case because the probability of being treated and being not treated is 50% for both cases. But treatment probabilities could also take different values for a variety of reasons, for example because treatment is costly. However, you need to ensure that both groups are large enough to be comparable in order to fulfill the independence assumption.\nLet’s see what that means. We assume that we have a population of 100’000 individuals which we want to learn something. Using runif() and rbinom, we synthetically generate this population with random value for the characteristics \\(age\\) and \\(sex\\).\n\nlibrary(tidyverse)\n\n# population size\nn <- 1e+5 \n\n# create population with two characteristics\nX <- tibble(\n  age = runif(n, 18, 65), # draw random values from uniform distribution\n  sex = rbinom(n, 1, 0.5) # draw random values from binomial distribution\n)\n\n# show first values\nhead(X)\n\n# A tibble: 6 × 2\n    age   sex\n  <dbl> <int>\n1  31.0     0\n2  18.0     1\n3  42.0     1\n4  18.7     0\n5  21.0     1\n6  62.9     0\n\n\nUntil now, we have not assigned units to treatment and control group and actually, we do not want to assign our whole population to any group. As a matter of fact, in many applications, you are just able to draw a sample from the population and almost never the whole population.\nRemember, randomization of treatment should achieve that we are able to interpret the average treatment effect causally and for that, both groups need to be as similar as possible. The image illustrates the randomization process. Try to think what could happen if you have just very few units in both groups. How likely is is that they are very similar regarding their characteristics? You can probably already sense that this might not be sufficient to make groups comparable.\n\nBut let’s try it out and see how average group characteristics develop when we change the sample size.\nIn R, it is very easy to generate a random vector that we can use for randomization. Here, we wan to have a random vector that contains either 1 (treatment group) or 0 (control group) with a treatment probability of 50%. We can make use of the rbinom function that can randomly generate outcomes of a Bernoulli trial, which you can just imagine as flipping the coin \\(n\\) times.\nAs we have 100’000 people in our population, we will vary sample sizes from 100 to 100’000 to understand the impact sample size has.\n\n# vector of sample sizes\nsss <- c(50, 100, 500, seq(1000, 1e+5, 1000))\n\n# empty list to store average tables in\navg_tbl_age_lst <- list()\navg_tbl_sex_lst <- list()\ntbl_sampled_lst <- list()\n\n# for sample size in sample sizes\nfor (ss in sss) {\n  # sample from population\n  X_sampled <- sample_n(X, ss)\n  \n  # perform random assignment\n  D <- rbinom(ss, 1, 0.5)\n  \n  # combine characteristics and assignment in one table\n  tbl_sampled <- X_sampled %>% mutate(treatment = D)\n  \n  # store in list\n  tbl_sampled_lst[[paste(ss)]] <- tbl_sampled\n  \n  # get average characteristics ...\n  # ... for age\n  avg_tbl_age <- tbl_sampled %>%\n    group_by(treatment) %>%\n    summarise(mean_age = mean(age)) %>% \n    ungroup %>% \n    add_column(sample_size = ss,\n               variable = \"age\") %>% \n    pivot_wider(names_from = treatment,\n                names_prefix = \"D_\",\n                values_from = mean_age) %>% \n    mutate(delta_abs = abs(D_1 - D_0),\n           delta_rel = delta_abs/D_0)\n  \n  # store table in list\n  avg_tbl_age_lst[[paste(ss)]] <- avg_tbl_age\n  \n  # ... for sex\n  avg_tbl_sex <- tbl_sampled %>%\n    group_by(treatment) %>%\n    summarise(mean_sex = mean(sex)) %>% \n    ungroup %>% \n    add_column(sample_size = ss,\n               variable = \"sex\") %>% \n    pivot_wider(names_from = treatment,\n                names_prefix = \"D_\",\n                values_from = mean_sex) %>% \n    mutate(delta_abs = abs(D_1 - D_0),\n           delta_rel = delta_abs/D_0)\n  \n  # store table in list\n  avg_tbl_sex_lst[[paste(ss)]] <- avg_tbl_sex\n  \n}\n\nAs you can see in the plot, group average characteristics converge with increasing sample size. The more units are assigned to either group, the less differences are between the groups and thus, the independence assumption, stating that groups only differ by their treatment status, is fulfilled. But although you need a minimum amount of units, there is not much improvement after increasing the sample size way beyond that.\n\n# combine tables to one larger table\navg_age <- avg_tbl_age_lst %>% bind_rows()\navg_sex <- avg_tbl_sex_lst %>% bind_rows()\n\navgs_tbl <- avg_age %>% bind_rows(avg_sex)\n\n# plot convergence\nggplot(avgs_tbl, aes(x = sample_size, y = delta_abs)) +\n  geom_line() +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(x = \"sample size\", y = \"absolute difference\") +\n  ggtitle(\"Absolute difference of characteristics between groups by sample size\")"
  },
  {
    "objectID": "content/toolbox/05_rct.html#average-treatment-effect",
    "href": "content/toolbox/05_rct.html#average-treatment-effect",
    "title": "Randomized Controlled Trials",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nLet’s just use a sample size of 40’000 units. That means there should be about ~20’000 units per group. There are many suggested rules and guidelines to choose the right sample size, but for now, we will disregard it as our data is simulated and therefore, we do not have any data problems.\nSo far, we have just looked at the covariate balance but have not included the outcome variable. Let’s do that now. In the background we simulated the outcome after treatment and added the column outcome to our table.\n\nhead(df_out)\n\n# A tibble: 6 × 4\n    age   sex treatment outcome\n  <dbl> <int>     <int>   <dbl>\n1  41.0     1         0  0.0103\n2  63.1     0         1  0.679 \n3  53.3     1         0 -0.138 \n4  51.4     1         1  1.61  \n5  30.9     0         0 -0.109 \n6  56.9     0         1  0.748 \n\n\nAs already mentioned, having balanced baseline characteristics between treatment and control group allows us to estimate the average treatment effect.\nBut how do we calculate the average treatment effect? We can just take a simple difference in means to estimate it. By the way, groups can be of different group size. It is only important, that they are comparable in their characteristics.\nLet’s compute the average outcome per group. We see that there seems to be a difference, the average outcome in the treatment group is higher.\n\n# group by treatment group and compute average outcome\ndf_out %>% \n  group_by(treatment) %>% \n  summarise(mean_outcome = mean(outcome))\n\n# A tibble: 2 × 2\n  treatment mean_outcome\n      <int>        <dbl>\n1         0    -0.000289\n2         1     1.12    \n\n\nGenerally, it is recommendable to use a linear regression to get an estimate of the treatment effect. You don’t have to manually compute the difference and additionally, the output of lm() and summary() yields information regarding its statistical inference. Then, we see that this effect is in fact highly statistically significant. The effect is equal to the difference of the two values just seen above. Check it out!\n\n# compute ATE with linear regression\nlm_ate <- lm(outcome ~ treatment, data = df_out)\nsummary(lm_ate)\n\n\nCall:\nlm(formula = outcome ~ treatment, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0245 -0.2046 -0.0005  0.2077  1.0459 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.000289   0.002682   -0.11     0.91    \ntreatment    1.116174   0.003796  294.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.38 on 39998 degrees of freedom\nMultiple R-squared:  0.684, Adjusted R-squared:  0.684 \nF-statistic: 8.65e+04 on 1 and 39998 DF,  p-value: <2e-16\n\n\nOne way to support your results could be a boxplot that on the one hand shows the difference of regressors by group and on the other hand the difference of outcomes. Here we will show the 95% confidence intervals for our estimates and it can be seen that there is a substantial difference between both groups. However, for our independent variables, age and sex , both groups are very similar.\n\n# plot independent and and depdent difference\ncompare_age <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = age, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Age\", title = \"Difference in age\")\n\ncompare_sex <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = sex, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Sex\", title = \"Difference in sex\")\n\ncompare_outcome <- \n  ggplot(df_out, \n         aes(x = treatment, \n             y = outcome, \n             color = as.factor(treatment))) +\n  stat_summary(geom = \"pointrange\", \n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Outcome\", title = \"Difference in outcome\")\n\n# plot age, sex and outcome differences for both groups\nggpubr::ggarrange(compare_age, compare_sex, compare_outcome, ncol = 3) \n\n\n\n\n\n\n\n\nBut why did we not include \\(age\\) and \\(sex\\) into our regression? Because they are similarly distributed across both groups it should not change the treatment effect. But still, they might have an impact on the outcome, as well. Although being similarly distributed in both groups, it does not mean that they still vary within each group. So let’s see what happens if we include them.\nBoth regressors turn out to be significant. However, as expected, the treatment effect is almost unchanged.\n\n# include other regressors\nlm_all <- lm(outcome ~ treatment + age + sex, data = df_out)\nsummary(lm_all)\n\n\nCall:\nlm(formula = outcome ~ treatment + age + sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6738 -0.2500  0.0019  0.2510  0.6913 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.459642   0.004874   -94.3   <2e-16 ***\ntreatment    1.115773   0.002774   402.3   <2e-16 ***\nage          0.005052   0.000102    49.5   <2e-16 ***\nsex          0.499399   0.002774   180.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.28 on 39996 degrees of freedom\nMultiple R-squared:  0.831, Adjusted R-squared:  0.831 \nF-statistic: 6.56e+04 on 3 and 39996 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/05_rct.html#subgroup-analysis",
    "href": "content/toolbox/05_rct.html#subgroup-analysis",
    "title": "Randomized Controlled Trials",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nThe significance of \\(age\\) and \\(sex\\) could also indicate that there are different treatment effects across different levels of both covariates. Then, a so called interaction/moderation effect would be covered behind the statistical coefficients.\nA moderation effect expresses different strengths of the treatment for different subgroups. For example older women might benefit relatively more and younger males relatively less.\nIn R, we include interaction effects by using either using a product x1*x2 or a colon x1:x2.\n\n# include interaction\nlm_mod <- lm(outcome ~ treatment * age + treatment * sex, data = df_out)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = outcome ~ treatment * age + treatment * sex, data = df_out)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4436 -0.0666 -0.0003  0.0665  0.4769 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.35e-03   2.37e-03   -0.57     0.57    \ntreatment      2.02e-01   3.34e-03   60.49   <2e-16 ***\nage            2.02e-05   5.17e-05    0.39     0.70    \nsex            4.34e-04   1.40e-03    0.31     0.76    \ntreatment:age  9.97e-03   7.30e-05  136.49   <2e-16 ***\ntreatment:sex  9.99e-01   1.98e-03  503.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.099 on 39994 degrees of freedom\nMultiple R-squared:  0.978, Adjusted R-squared:  0.978 \nF-statistic: 3.62e+05 on 5 and 39994 DF,  p-value: <2e-16\n\n\nNote also that \\(R^2\\) has increased with each addition to the regression.\nAs the data is simulated, we can check what the data-generating process is and based on that evaluate what regression equation provides the best solution. Check for yourself what model should be used.\n\\[\noutcome = 0.2*treatment + 0.01*treatment*age + treatment*sex + \\epsilon\n\\]\nAgain, it shows how crucial theoretical knowledge of the phenomenon you are studying is. Imagine a situation with a high number of regressors. Testing out all potential variables as moderators requires some effort and might even lead to results just due to chance."
  },
  {
    "objectID": "content/toolbox/09_rdd.html",
    "href": "content/toolbox/09_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "The next tool we introduce is called regression discontinuity design (RDD). Fist used in 1960 (Campbell), it did not play a large role until 1999 and since then has experienced growing acceptance due to the advance of more rigorous requirements regarding credibility and causality in social sciences. Another factor that made many researchers use RDDs is the increased availability of digitized administrative data that is often linked to arbitrary rules that can be exploited to capture “as-if” randomization processes for treatment assignment.\nAssuming a data-generating process, where we have a variable \\(X\\) that is a confounder as it has an impact on treatment assignment \\(D\\) and the outcome \\(Y\\). Additionally, we could have an unobserved confounders between \\(X\\) and \\(Y\\).\nThen, as can be seen in the second DAG, the regression discontinuity design exploits the fact that \\(X\\) determines \\(D\\) and data is filtered such that there are only observations that were close to a cut-off value determining their treatment status. This way, treated and untreated units are very similar and comparable and RDD is able to eliminate selection bias for that subpopulation. Note that the treatment effect you calculate using this method is an average treatment effect for a subgroup rather than for the whole population.\n\\(X\\) is called the running variable and is a continuous variable assigning units to treatment \\(D\\) based on a cut-off score \\(c_0\\). Because it has an impact on \\(Y\\) as well, it is a confounder and opens a backdoor path. Now, the problem is that due to the cut-off determining the treatment \\(D\\), the backdoor cannot be closed with regular ways like e.g. matching as there is no overlap, i.e. there are no treated and untreated units for all levels of \\(X\\).\n!!! Summarizing:\n\nRunning/forcing variable: …\nCut-off/threshold: …\n\n\nlibrary(tidyverse)\n\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.8\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.2     ✓ forcats 0.5.1\n\n\n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(dagitty)\n\n# Initial situation\ndag_model_1 <- dagitty(\n  'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.2,0.25\"]\nU [pos=\"0.4,0.1\"]\nX [pos=\"0.15,0.1\"]\nY [outcome,pos=\"0.45,0.25\"]\nD -> Y\nU -> X\nU -> Y\nX -> D\nX -> Y\n}\n')\n \n# Directed Acyclic Graph\ndag_1 <- ggdag_status(dag_model_1) +\n  guides(color = \"none\") +\n  theme_dag()\n\n# Exploiting cut-off\ndag_model_2 <- dagitty(\n  'dag {\nbb=\"0,0,1,1\"\nD [exposure,pos=\"0.2,0.25\"]\nU [pos=\"0.4,0.1\"]\nX_c [pos=\"0.15,0.1\"]\nY [outcome,pos=\"0.45,0.25\"]\nD -> Y\nU -> Y\nX_c -> D\n}\n')\n \n# Directed Acyclic Graph\ndag_2 <- ggdag_status(dag_model_2) +\n  guides(color = \"none\") +\n  theme_dag()\n\ngridExtra::grid.arrange(dag_1, dag_2, ncol = 2)\n\n\n\n\n\n\n\n\nTherefore, as the second graph shows, the causal effect is identified by analyzing only observations that are in close neighborhood to \\(c_0\\). The identified treatment effect is the local average treatment effect (LATE).\n\\[\nLATE_{RDD} = E[Y_1 - Y_0| X \\rightarrow c_0]\n\\]"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#research-design",
    "href": "content/toolbox/09_rdd.html#research-design",
    "title": "Regression Discontinuity",
    "section": "Research Design",
    "text": "Research Design\nRDDs are quite intuitive and very graphical. For this reason, we will go through estimation and inference using an application and explain at each step what has to be considered.\nIn the application, we want to analyze the effect of being graded “very good” instead of “good” on a restaurant review site. Customers leave reviews at this site for restaurants they have visited and the resulting average score (from 1 to 10) is phrased with a label. Restaurants with a score \\([8,9)\\) will receive the label “good”, while restaurants with a score of \\([9, 10]\\) receive the label “very good”.We will only focus on restaurants with a score larger than 8.\nYou might already see how we can use RDD in this application to identify the effect of a “very good” label on revenue. We can exploit the rule that there is a sharp cut-off for restaurants having a score just above or below 9. That means our cut-off value is \\(c0 = 9\\).\nLet’s have a look at what the data looks like.\n!!! INCLUDE COVARIATES?\n\nhead(df)\n\n# A tibble: 6 × 6\n  user_rating user_rating_ct rating_label management  size revenue\n        <dbl>          <dbl> <fct>             <int> <dbl>   <dbl>\n1        9.12          0.117 TRUE                  1    65   119. \n2        8.63         -0.369 FALSE                 0    23    51.8\n3        8.25         -0.746 FALSE                 0    15    33.2\n4        8.70         -0.300 FALSE                 1    90   166. \n5        9.72          0.722 TRUE                  1    84   173. \n6        9.42          0.422 TRUE                  1    81   151. \n\n\n!!! DAG?\nAt first glance, it looks like all restaurants below the cut-off don’t have a “very good” label (indicated by FALSE) and the restaurants above cut-off do have it. We can visualize if that applies to all restaurants. As we have expected, to the left of the cut-off and to the right of the cut-off, there is always just one label type. It means, we are dealing with a sharp cut-off.\n\n# [2] Visualization ----\n# [2.1] Compliance ----\n# As expected, perfect \"compliance\" and sharp cutoff. All \n# restaurants below the cutoff get a \"good\" rating, while all restaurants above\n# the cutoff get a \"very good\" rating.\ncompl <- \n  ggplot(df, aes(x = user_rating, y = rating_label, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[4]) +\n  geom_point(alpha = 1000/n, position = position_jitter(width = NULL, height = 0.05)) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"Good\", \"Very Good\"))+\n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"\") +\n  theme(legend.title = element_blank())\ncompl"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#random-assignment",
    "href": "content/toolbox/09_rdd.html#random-assignment",
    "title": "Regression Discontinuity",
    "section": "Random assignment",
    "text": "Random assignment\nAs already mentioned, for RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\nWe can see that there is no decline or incline at the cut-off and therefore can assume that the continuity assumption holds.\n\n# [2.2] Random assignment test ----\n# identifying assumption: random assignment to either side of cut-off\n# Manual plot\nggplot(df, aes(x = user_rating, fill = label)) +\n  geom_histogram(binwidth = .1, color = \"white\", boundary = c0, alpha = .6) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5], size = 2, linetype = \"solid\")\n\n\n\n\n\n\n\n\nTo check the continuity assumption more thoroughly, we can also use functions of the rddensity package. It relies on a test to check the assumption. as the p-value is large, we can reject the null hypothesis that the number of units at either side are different.\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd <- rddensity(df$user_rating, c = c0)\nsummary(rddd)\n\n\nManipulation testing using local polynomial density estimation.\n\nNumber of obs =       5000\nModel =               unrestricted\nKernel =              triangular\nBW method =           estimated\nVCE method =          jackknife\n\nc = 9                 Left of c           Right of c          \nNumber of obs         2488                2512                \nEff. Number of obs    709                 840                 \nOrder est. (p)        2                   2                   \nOrder bias (q)        3                   3                   \nBW est. (h)           0.178               0.199               \n\nMethod                T                   P > |T|             \nRobust                0.5829              0.5599              \n\n\nP-values of binomial tests (H0: p=0.5).\n\nWindow Length / 2          <c     >=c    P>|T|\n0.003                       8      12    0.5034\n0.006                      18      22    0.6358\n0.008                      27      35    0.3742\n0.011                      37      46    0.3800\n0.014                      50      58    0.5008\n0.017                      60      72    0.3384\n0.019                      79      85    0.6963\n0.022                      89      96    0.6592\n0.025                     100     111    0.4913\n0.028                     118     122    0.8465\n\n\nIt can also be shown graphically, where you can see that the confidence intervals overlap. If they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to get valid results.\n\n# Visually check continuity at running variable\nrdd_plot <- rdplotdensity(rddd, df$user_rating, plotN = 100)"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#plot-treatment-effect",
    "href": "content/toolbox/09_rdd.html#plot-treatment-effect",
    "title": "Regression Discontinuity",
    "section": "Plot Treatment Effect",
    "text": "Plot Treatment Effect\nHaving checked potential violations of the continuity assumptions, we can move on and estimate the treatment effect. We start with selecting a bandwidth, i.e. we select what and how many observations should be compared. The larger the bandwidth, the more observations are taken into consideration but it also reduces the comparability. On the other hand, choosing a small bandwidth results in a lower number of observations while ensuring similarity and comparability.\nThere is no safe rule how to best select the bandwidth, although there are algorithms attempting to look for the optimal bandwidth. For now, we just use common sense and select a bandwidth of 0.15, resulting in an analysis window \\([8.85, 9.15]\\), which still leaves us with about 1000 observations.\n\n# [3] Dependent variable ----\n# [3.1] Average Treatment Effect ----\n# Plot regression lines for full and specified bandwidth.\n# Specify bandwidth\nbw <- c0 + c(-0.15, 0.15)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below <- df %>% filter(user_rating %>% between(bw[1], c0))\ndf_bw_above <- df %>% filter(user_rating %>% between(c0, bw[2]))\ndf_bw <- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n[1] 1268    6\n\n\nTo illustrate the difference between using only a small window and all data, we plot the resulting regression lines. You can see that both approaches would lead to different results.\n\n# Plot dependent variable vs running variable\ndep_var <-\n  ggplot(df, aes(x = user_rating, y = revenue, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_vline(xintercept = bw[1], color = ggthemr::swatch()[5], linetype = \"dashed\") +\n  geom_vline(xintercept = bw[2], color = ggthemr::swatch()[5], linetype = \"dashed\") +\n  geom_point(alpha = 1000/n, size = 0.5) +\n  # add lines for the full range\n  geom_smooth(data = filter(df, user_rating <= c0), \n              method = \"lm\", se = F, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(df, user_rating > c0), \n              method = \"lm\", se = F, size = 1, linetype = \"dashed\") +\n  # add lines for specified bandwidth\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, size = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, size = 2) +\n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"revenue\") +\n  theme(legend.title = element_blank())\ndep_var\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom the plot above, it is hard to see what the difference between observations close to the cut-off is. So what we can do is to compute to regressions, one for the observations in \\([8.85, 9)\\) and another one for the observations in \\([9,9.15]\\).\nThen, using the resulting coefficients, we compute what both models predict for the cut-off value \\(c0\\) and take the difference. The difference is the local average treatment effect (LATE).\n\n# [3.2] Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below <- lm(revenue ~ user_rating, df_bw_below)\nmodel_bw_above <- lm(revenue ~ user_rating, df_bw_above)\n\ny0 <- predict(model_bw_below, tibble(user_rating = c0))\ny1 <- predict(model_bw_above, tibble(user_rating = c0))\n\nlate <- y1 - y0\nlate\n\n 1 \n25 \n\n\nIt’s a bit messy when we plot all observations, so let’s zoom in to see if we can detect the local average treatment effect graphically. Not surprisingly, it is equal to what we have just computed.\n\n# Minimum and maximum for y-axis limits\nmin_y <- min(df_bw$revenue)\nmax_y <- max(df_bw$revenue)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw <- \n  ggplot(df_bw, aes(x = user_rating, y = revenue, color = rating_label)) +\n  geom_vline(xintercept = c0, color = ggthemr::swatch()[5]) +\n  geom_point(alpha = 0.2, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, size = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, size = 2) +\n  theme(legend.position = \"bottom\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y0, yend = y0),\n             linetype = \"dashed\", color = ggthemr::swatch()[7], size = 1.5) +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dashed\", color = ggthemr::swatch()[7], size = 1.5) +\n  annotate(\"text\", x = c0 - 0.05, y = mean(c(y1, y0)),\n           label = sprintf(\"Difference: %.2f\", late), fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"Good\", \"Very Good\")) +\n  xlab(\"user rating\") +\n  ylab(\"revenue\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "content/toolbox/09_rdd.html#estimate-treatment-effect",
    "href": "content/toolbox/09_rdd.html#estimate-treatment-effect",
    "title": "Regression Discontinuity",
    "section": "Estimate Treatment Effect",
    "text": "Estimate Treatment Effect\n\nParametric Estimation\nWhat you will see most in studies is a regression to compute the LATE. Here, we prefer to use user_rating_ct, which is the raw user_rating variable centered, i.e. subtracted by the cut-off value \\(c0\\). That simplifies the interpretation, however, it does not change the coefficient of interest, the LATE.\nThe coefficient we are most interested in is the one for rating_labelTRUE. It is equal to the effect in the plot above, but the regression summary also yields additional statistical information. We see that the LATE is statistically significant.\n\n# [4] Estimation ----\n# [4.1] Parametric ----\n# Compute coefficients for specified bandwidth.\nparam_bw <- lm(revenue ~ user_rating_ct + rating_label, df_bw)\nsummary(param_bw)\n\n\nCall:\nlm(formula = revenue ~ user_rating_ct + rating_label, data = df_bw)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-106.67  -28.59    0.19   28.25  103.00 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         90.00       2.36   38.18  < 2e-16 ***\nuser_rating_ct       2.84      25.62    0.11     0.91    \nrating_labelTRUE    24.96       4.31    5.80  8.5e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38 on 1265 degrees of freedom\nMultiple R-squared:   0.1,  Adjusted R-squared:  0.0987 \nF-statistic: 70.4 on 2 and 1265 DF,  p-value: <2e-16\n\n\n\n\nNon-parametric Estimation\nYou can also use non-parametric estimation techniques, i.e. we do not fit a line through the data but instead a curve. The R package rdrobust is a good resource that is easy to use. We just have to provide the dependent variable, the running variable and the cut-off value.\nBy default, the rdrobust() function automatically selects a bandwidth and uses a kernel weighting function (triangular kernel).\n\n# [4.2] Non-parametric ----\nlibrary(rdrobust)\nnparam_bw <- rdrobust(y = df_bw$revenue, x = df_bw$user_rating, c = c0)\nsummary(nparam_bw)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1268\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  636          632\nEff. Number of Obs.             214          207\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.048        0.048\nBW bias (b)                   0.076        0.076\nrho (h/b)                     0.634        0.634\nUnique Obs.                     636          632\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    29.892     8.322     3.592     0.000    [13.582 , 46.203]    \n        Robust         -         -     3.111     0.002    [11.487 , 50.600]    \n=============================================================================\n\n\nTo check how the curve goes through the data, we can use rdplot(), which splits the data into bins and for each bin shows an average data point. We see that the curve fits the data very well.\n\nrdplot(y = df$revenue, x = df$user_rating, c = c0)\n\n\n\n\n\n\n\n\nThere are a lot of options with the rdrobust() function, e.g. we can change the kernel and see how it affects the result.\n\n# Use other kernel\nnparam_bw_kernel <- rdrobust(y = df_bw$revenue, x = df_bw$user_rating, \n                             c = c0, kernel = \"epanechnikov\")\nsummary(nparam_bw_kernel)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1268\nBW type                       mserd\nKernel                   Epanechnikov\nVCE method                       NN\n\nNumber of Obs.                  636          632\nEff. Number of Obs.             204          199\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.046        0.046\nBW bias (b)                   0.075        0.075\nrho (h/b)                     0.606        0.606\nUnique Obs.                     636          632\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    30.892     8.286     3.728     0.000    [14.652 , 47.132]    \n        Robust         -         -     3.257     0.001    [12.846 , 51.681]    \n============================================================================="
  },
  {
    "objectID": "content/toolbox/06_match.html",
    "href": "content/toolbox/06_match.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggdag)\nlibrary(dagitty)"
  },
  {
    "objectID": "content/toolbox/06_match.html#single-matching-variable",
    "href": "content/toolbox/06_match.html#single-matching-variable",
    "title": "Matching and Subclassification",
    "section": "Single Matching Variable",
    "text": "Single Matching Variable\nProbably the most simple method, though rarely applied in practice, is matching on a single variable. Its rare use in practice is due to the fact that matching on a single variable is only applicable if there is only one backdoor path that can be closed by this matching variable.\nBut for ease of explanation, we’ll have a look at it.\n\nconfounder <- 'dag {\nD [exposure,pos=\"0.000,0.000\"]\nY [outcome,pos=\"2.000,0.000\"]\nZ [pos=\"1.000,1.000\"]\nZ -> D\nZ -> Y\nD -> Y\n}'\n\nggdag(confounder) + \n  annotate(\"text\", x = 0.5, y = 1, label = \"Z fixed at constant level\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n!!! EXAMPLE:\ncredit card holders, monthly bills, repayment status (pay on time, delay payment) April - September\n\nbeing late April <- size of bill in April -> (Y) being late Sept\n\n\n(Coarsened) Exact Matching\n!!! pick one observation. define distance. single or multiple comparables.\nWe are trying to select observations in the control group that are similar to those in the treated group. To do so, we need to define what similar means. Here, we will say that similar observations are observations that share similar values for our matching variable (size of bill in April). By enforcing treatment and control group to have little variation in the matching variable, we close the backdoor. Only when the backdoor variable varies, it can induce changes in treatment and outcome and when we keep it at a certain level, the only effect from treatment to outcome is the direct effect.\nHow many? With our without replacement? Tradeoff between bias and variance (more matches, less variance and more bias) (Matching with replacement less bias because better matches and is not order-dependent)\n\none-to-one matching: selecting the best match\ntop \\(k\\) matching: k-nearest-neighbor matching\nall “acceptable” matches: every match by some metric of acceptability (hard to define).\n\n\n\nMatched weighted sample\nNow we will move away from a an observation being in or out depending on whether it is a best match to some unit. Instead we use an alternate approach that works by looking at all control observations and checking how close they are to treated observations. An observation is neither in our out but receives a weight depending on its similarity to a treated observation. The closer a control observation is to a treatment observation, the higher its weight.\nHow to construct weights?\n\nKernel matching: kernel function takes difference and returns weight. Weight eventually gets to zero. Better matches obtain higher weights than less-good matches, bad matches obtain weight of zero. Different kinds of kernels.\nInverse probability weighting: specifically designed for use with propensity scores. Each observation is weighted by the inverse of the probability for its own treatment status. Simply put, atypical observations receive a high weight, so if you were actually treated which was unlikely based on your covariates, you receive a high weight.\n\n!!! PLOT: One-to-one matching (color of controls) vs weight matching (size of controls)\nComparison: selecting matches more intuitive and easier to implement but more sensitive. weights to account for quality of matches"
  },
  {
    "objectID": "content/toolbox/06_match.html#application-multiple-matching-variables",
    "href": "content/toolbox/06_match.html#application-multiple-matching-variables",
    "title": "Matching and Subclassification",
    "section": "Application: Multiple Matching Variables",
    "text": "Application: Multiple Matching Variables\nLet us imagine, you want to reduce the number of sick days in your company by implementing a health program that employees are free to participate in. By learning about how to improve their health, you expect your employees to call in sick less frequently.\nNow you already see that the treatment, participation in the health program, is on a voluntary basis and therefore treatment assignment might be confounded by variables such as age and initial health status. Older and sicker people might be more interested to learn about techniques and procedures to improve their health and also might benefit more from the program. Also, initial health status might be affected by age.\nWe can use a DAG to think about the correct identification strategy.\n\n# define DAG\ndag_model <- 'dag {\n  bb=\"0,0,1,1\"\n  \"Health Program\" [exposure,pos=\"0.25,0.2\"]\n  \"Initial Health Status\" [pos=\"0.35,0.25\"]\n  \"Sick Days\" [outcome,pos=\"0.35,0.2\"]\n  Age [pos=\"0.25,0.25\"]\n  \"Initial Health Status\" -> \"Health Program\"\n  \"Initial Health Status\" -> \"Sick Days\"\n  Age -> \"Health Program\"\n  Age -> \"Initial Health Status\"\n  Age -> \"Sick Days\"\n}'\n\n# Directed Acyclic Graph\nggdag_status(dag_model, text = FALSE, use_labels = \"name\") +\n  guides(color = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n# Directed Acyclic Graph with adjustment sets\nggdag_adjustment_set(dag_model, shadow = T, use_labels = \"name\", text = F) +\n  guides(color = \"none\") +  # Turn off legend\n  theme_dag()\n\n\n\n\n\n\n\n\nThere are two backdoor paths that we need to close, initial health status and age.\n\nadjustmentSets(dag_model)\n\n{ Age, Initial Health Status }\n\n\nA naive estimate would be\n\n# Naive estimation (not accounting for backdoors)\nmodel_naive <- lm(sick_days ~ health_program, data = df)\nbroom::tidy(model_naive)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic   p.value\n  <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)            7.26    0.0331     219.  0        \n2 health_programTRUE     1.25    0.0465      26.9 2.30e-154\n\n\nwhich is different from the !!! TRUE TREATMENT EFFECT.\n\n(Coarsened) Exact Matching\nIt is not difficult to extend the approaches defined above to multiple matching variables. Regarding the “selecting matches” approach, multiple variables are used for the computations instead of a single. Again, in case of exact matching, only observations that share the same values are matched in.\nTo perform Coarsened Exact Matching (CEM) you can use the MatchIt package in R. If you do not specify how to coarsen the data, it will be done automatically based on an algorithm.\n\nlibrary(MatchIt)\n# Without specifying coarsening\n# (1) Matching\ncem <- matchit(health_program ~ age + sick_days_before,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\n\n# Covariate balance\nsummary(cem)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\")\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.6            0.18       0.99     0.054    0.087\nsick_days_before           4.2           3.5            0.36       1.39     0.033    0.183\n\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.6          45.5           0.001       1.00     0.002    0.009            0.10\nsick_days_before           3.8           3.8           0.021       0.97     0.002    0.042            0.22\n\nSample Sizes:\n              Control Treated\nAll              4918    5082\nMatched (ESS)    4600    4825\nMatched          4914    5046\nUnmatched           4      36\nDiscarded           0       0\n\n\nWe are already closer to our true treatment effect.\n\ndf_cem <- match.data(cem)\n\n# (2) Estimation\nmodel_cem <- lm(sick_days ~ health_program, data = df_cem, weights = weights)\nbroom::tidy(model_cem)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           7.60     0.0325     234.  0       \n2 health_programTRUE    0.522    0.0457      11.4 5.16e-30\n\n\nWe can also provide values to coarsen the data in order to control for the number of subsamples. Again, we also check the balance.\n\n# Custom coarsening\n# (1) Matching\ncutpoints <- list(age = seq(25, 65, 15), sick_days_before = seq(3, 22, 5))\ncem_coars <- matchit(health_program ~ age + sick_days_before,\n                     data = df, \n                     method = 'cem', \n                     estimand = 'ATE',\n                     cutpoints = cutpoints)\n\n# Covariate balance\nsummary(cem_coars)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.6            0.18       0.99     0.054    0.087\nsick_days_before           4.2           3.5            0.36       1.39     0.033    0.183\n\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       45.6          45.4           0.025        1.0     0.008    0.025            0.48\nsick_days_before           3.9           3.7           0.124        1.1     0.012    0.094            0.58\n\nSample Sizes:\n              Control Treated\nAll              4918    5082\nMatched (ESS)    4762    4926\nMatched          4916    5080\nUnmatched           2       2\nDiscarded           0       0\n\n\nWe can also visualize the subsamples and see how data points are weighted.\n\ndf_cem_coars <- match.data(cem_coars)\n\n# Plot grid\nggplot(df_cem_coars, aes(x = age, y = sick_days_before,\n                         size = weights, color = as.factor(health_program))) +\n  geom_point(alpha = .2) +\n  geom_abline(data.frame(y = cutpoints$sick_days_before),\n              mapping = aes(intercept = y, slope = 0)) +\n  geom_vline(data.frame(y = cutpoints$age),\n              mapping = aes(xintercept = y)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow, with fewer subsamples, the estimate is worse. It seems, the backdoors are not properly closed.\n\n# (2) Estimation\nmodel_cem_coars <- lm(sick_days ~ health_program, data = df_cem_coars, \n                      weights = weights)\nbroom::tidy(model_cem_coars)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           7.52     0.0334     225.  0       \n2 health_programTRUE    0.730    0.0468      15.6 2.94e-54\n\n\n\n\nNearest-Neighbor matching\nFor nearest neighbor matching, the difference between two observations based on multiple variables is computed and reduced to a scalar. One of the most popular techniques used to find so called “nearest neighbors” is the euclidean distance.\nWe just have to change a few arguments and decide to use the Mahalanobis distance. Then, we check how similar treatment and control group are after matching.\n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn <- matchit(health_program ~ age + sick_days_before,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n\n\nCall:\nmatchit(formula = health_program ~ age + sick_days_before, data = df, \n    method = \"nearest\", distance = \"mahalanobis\", replace = T)\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nage                       46.4          44.6            0.18       0.99     0.054    0.087\nsick_days_before           4.2           3.5            0.33       1.39     0.033    0.183\n\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\nage                       46.4          46.4          -0.001          1     0.001    0.005           0.009\nsick_days_before           4.2           4.2           0.001          1     0.000    0.000           0.002\n\nSample Sizes:\n              Control Treated\nAll              4918    5082\nMatched (ESS)    1783    5082\nMatched          2693    5082\nUnmatched        2225       0\nDiscarded           0       0\n\n\nThis method also brings us closer to the true treatment effect.\n\ndf_nn <- match.data(nn)\n\n# (2) Estimation\nmodel_nn <- lm(sick_days ~ health_program, data = df_nn, weights = weights)\nbroom::tidy(model_nn)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           8.00     0.0472    169.   0       \n2 health_programTRUE    0.510    0.0584      8.72 3.28e-18\n\n\n\n\nCurse of dimensionality\nWith exact matching and nearest-neighbor matching you quickly run into the curse of dimensionality as your number of covariates grows. If you want to find matches based on very few dimensions, you are way more likely to find them as opposed to matches on a high number of dimensions, where it is very likely that you actually don’t find any matches at all.\nRegarding exact matching, consider for example the situation with two covariates with each five different values. Then any observations will fall into one of 25 different cells that are given by the covariate value grid. And now imagine ten covariates with three different values: it already creates ~60k cells, which increases the likelihood of a cell being populated by only one or zero observations substantially. Then, estimation of treatment effects is not possible for many of the observations.\nNearest-neighbor matching is similarly affected by the curse of dimensionality. The more covariates you include, the less likely you are to find a good match. A quick solution, leaving out some covariates, can only be done if you are sure to not throw out any confounders.\n\n\nInverse Probability weighting\nOne way to deal with the curse of dimensionality is to use inverse probability weighting (IPW). We already mentioned it above, but let’s go into more detail.\nWe start by understanding what “probability” in inverse probability means. It is the predicted probability of treatment assignment based on the matching variables. So staying in the health program example, we use age and initial health status to predict how likely an employee is to participate in the health program. What we expect is that older and initially more sick people are more likely to participate opposed to younger and healthy people. To model this relationship, we could use for example logistic regression, a regression that predicts an outcome between zero and one. But you are also free to use any classification model that is out there, as here we are not only interested in explaining effects but only in obtaining the probability of treatment, also known as “propensity score”.\nHere, we will use a logistic regression:\n\n# (1) Propensity scores\nmodel_prop <- glm(health_program ~ age + sick_days_before,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nbroom::tidy(model_prop)\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)       -1.20     0.0997     -12.0  2.15e-33\n2 age                0.0120   0.00210      5.69 1.27e- 8\n3 sick_days_before   0.183    0.0116      15.7  7.89e-56\n\n\n\n# Add propensities to table\ndf_aug <- broom::augment_columns(\n  model_prop, \n  df,\n  type.predict = \"response\") %>% \n  rename(propensity = .fitted)\n\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHaving obtained the propensity score, you could again measure distances like described above and select matches. In fact, that is widely used matching method, known as propensity score matching. However, there are several reasons why this is not a good identification strategy1, mainly, because same propensity score does not imply that observations have the same covariate values and this could actually increase the imbalance. Note, however, that same covariate values indeed imply the same propensity score.\nInstead inverse probability has proven to be a more precise method, particularly when the sample is large enough. So what do we do with the probability/propensity scores in IPW? We use propensity score of an observation unit to in- or decrease its weights and thereby make some observations more important than others. The weight obtains as\n\\[\nw_i = \\frac{D_i}{\\pi_i} + \\frac{(1-D_i)}{(1-\\pi_i)}\n\\]\nwhere only one of the terms is always active as \\(D_i\\) is either one or zero. Now we should better understand what “inverse probability weighting” actually means. It weights each observation by its inverse of its treatment probability.\n\ndf_ipw <- df_aug %>% mutate(\n  ipw = (health_program/propensity) + ((1-health_program) / (1-propensity)))\n\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw)\n\n# A tibble: 10,000 × 5\n   health_program   age sick_days_before propensity   ipw\n   <lgl>          <dbl>            <dbl>      <dbl> <dbl>\n 1 FALSE           42.5                4      0.510  2.04\n 2 FALSE           48.8                6      0.618  2.62\n 3 FALSE           27.0                3      0.419  1.72\n 4 FALSE           34.9                3      0.442  1.79\n 5 FALSE           40.4                4      0.504  2.02\n 6 TRUE            48.7                3      0.483  2.07\n 7 FALSE           45.8                3      0.474  1.90\n 8 FALSE           60.0                3      0.517  2.07\n 9 FALSE           50.8                4      0.535  2.15\n10 FALSE           48.1                3      0.481  1.93\n# … with 9,990 more rows\n\n\nImagine a case of an old employee with a very bad initial health status who chose to participate in the health program, i.e. \\(D_i=1\\). Based on his/her covariates, it was very likely that he choose to participate and consequently, his propensity score will be rather high, let’s assume it was 0.8, for demonstration. Then his/her weight would equal \\(w_i = \\frac{1}{0.8} = 1.25\\).\nCompared to that, what weight would a young and healthy person that choose to participate in the program obtain? Let’s say his/her probability of participating would be 0.2. Then, his/her weight would be \\(w_i = \\frac{1}{0.2} = 5\\). So we see, he/she would obtain a significantly higher weight.\nIn general, IPW weights atypical observations, like a young and healthy person deciding to participate, higher than typical observations. The same applies for both treatment and control group. If you want, you can check it out yourself.\n\n# (2) Estimation\nmodel_ipw <- lm(sick_days ~ health_program,\n                data = df_ipw, \n                weights = ipw)\nbroom::tidy(model_ipw)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic       p.value\n  <chr>                 <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)           7.85     0.0367    214.   0            \n2 health_programTRUE    0.308    0.0521      5.92 0.00000000341\n\n\nSome propensity values are very extreme and therefore obtain an extreme weight. We should probably filter them out to improve our estimate.\n\ndf_ipw %>% \n  select(health_program, age, sick_days_before, propensity, ipw) %>% \n  arrange(desc(ipw))\n\n# A tibble: 10,000 × 5\n   health_program   age sick_days_before propensity   ipw\n   <lgl>          <dbl>            <dbl>      <dbl> <dbl>\n 1 FALSE           61.4               19      0.953 21.3 \n 2 FALSE           45.6               18      0.933 15.0 \n 3 FALSE           57.7               17      0.931 14.5 \n 4 FALSE           62.7               16      0.923 12.9 \n 5 FALSE           45.9               17      0.921 12.7 \n 6 FALSE           56.4               16      0.917 12.0 \n 7 FALSE           59.5               15      0.905 10.5 \n 8 FALSE           59.0               15      0.905 10.5 \n 9 FALSE           57.0               15      0.903 10.3 \n10 FALSE           41.3               15      0.885  8.68\n# … with 9,990 more rows\n\nmodel_ipw_trim <- lm(sick_days ~ health_program,\n                data = df_ipw %>% filter(propensity %>% between(0.15, 0.85)),\n                weights = ipw)\nbroom::tidy(model_ipw_trim)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           7.63     0.0323     236.  0       \n2 health_programTRUE    0.492    0.0457      10.8 8.16e-27\n\n\nOpposed to other methods, IPW, which is specifically designed for use with propensity scores, allows us to use all data in terms of number of observations and dimensions and the only decision we need to take is how to estimate the propensity score. It is important to note that the model does not need to predict as accurate as possible but it is more crucial that it accounts for all confounders.\n\n\nComparison\nYou see, that there many ways backdoors can be closed.\n\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                #\"All\"   = model_adj,\n                                \"CEM1\"  = model_cem,\n                                \"CEM2\"  = model_cem_coars,\n                                \"NN\"    = model_nn,\n                                \"IPW1\"  = model_ipw,\n                                \"IPW2\"  = model_ipw_trim))\n\n\n\n \n  \n      \n    Naive \n    CEM1 \n    CEM2 \n    NN \n    IPW1 \n    IPW2 \n  \n \n\n  \n    (Intercept) \n    7.260 \n    7.603 \n    7.517 \n    8.003 \n    7.853 \n    7.633 \n  \n  \n     \n    (0.033) \n    (0.033) \n    (0.033) \n    (0.047) \n    (0.037) \n    (0.032) \n  \n  \n    health_programTRUE \n    1.252 \n    0.522 \n    0.730 \n    0.510 \n    0.308 \n    0.492 \n  \n  \n     \n    (0.046) \n    (0.046) \n    (0.047) \n    (0.058) \n    (0.052) \n    (0.046) \n  \n  \n    Num.Obs. \n    10000 \n    9960 \n    9996 \n    7775 \n    10000 \n    9956 \n  \n  \n    R2 \n    0.068 \n    0.013 \n    0.024 \n    0.010 \n    0.003 \n    0.011 \n  \n  \n    R2 Adj. \n    0.068 \n    0.013 \n    0.024 \n    0.010 \n    0.003 \n    0.011 \n  \n  \n    AIC \n    45249.3 \n    44933.4 \n    45510.1 \n    36480.9 \n    47740.0 \n    44855.3 \n  \n  \n    BIC \n    45270.9 \n    44955.0 \n    45531.8 \n    36501.7 \n    47761.6 \n    44876.9 \n  \n  \n    Log.Lik. \n    −22621.658 \n    −22463.706 \n    −22752.070 \n    −18237.432 \n    −23866.986 \n    −22424.643 \n  \n  \n    F \n    725.685 \n    130.395 \n    243.514 \n    76.090 \n    34.994 \n    115.600 \n  \n  \n    RMSE \n    2.32 \n    2.28 \n    2.34 \n    2.45 \n    3.69 \n    3.22"
  },
  {
    "objectID": "content/toolbox/07_did.html",
    "href": "content/toolbox/07_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "The most popular research design in quantitative and social sciences is the difference-in-differences (DiD) approach. As it name suggests, the method captures differences by observing a treatment and a control group over time to estimate causal average effects.\nDiD provides a nonexperimental technique that, in its simplest form, compares two groups (control and treatment) at two points in time (before treatment and after) by observing if and how different both group’s outcome evolve.\nBy taking two differences, two different kind of biases should be avoided. First, by comparing both groups at both points in time, any external effect that affects the outcome through time should play no role as it affects both groups. Secondly, taking only the difference of change in consideration, selection bias is eliminated and potential outcomes can vary.\nAs can be seen in the table, the difference in outcome for the treatment group before and after treatment is \\(D + T\\), while for the control group it is only \\(T\\). The difference of these two differences then reduces to only \\(D\\), which is the treatment effect we want to estimate.\n\n\n\n\n\n\n\n\n\n\nGroup\nTime\nOutcome\n1st Difference\nDiD\n\n\n\n\nTreatment (D=1)\n0\n\\(Y= Y_{T=0, D=1}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0,D=1} + T + D\\)\n\\(T +D\\)\n\n\n\n\n\n\n\n\\(D\\)\n\n\nControl (D=0)\n0\n\\(Y = Y_{T=0, D=0}\\)\n\n\n\n\n\n1\n\\(Y = Y_{T=0, D=0} + T\\)\n\\(T\\)\n\n\n\n\nWe can also break it down in our known notation:\n\\[\nATE = \\bigg(E[Y_{D=1}|T=1] - E[Y_{D=1}|T=0] \\bigg)- \\bigg(E[Y_{D=0}|T=1] - E[Y_{D=0}|T=0]\\bigg)\n\\]\nBecause there are a lot of subscripts, it can also help to write down the formula in pseudo-math:\n\\[\nATE = (Y_{Treatment, After} - Y_{Treatment, before}) - (Y_{Control, After} - Y_{Control, Before})\n\\]\nOpposed to methods where we just know one outcome - the “after” outcome, regardless of whether a unit received or did not receive treatment - we do not have to assume that the potential otucomes \\(E[Y_0|D=1] = E[Y_0|D=1]\\) are equal. That is a big difference, because do not have to assume that observation units are similar in all their characteristics.\nInstead DiD hinges on a different assumption, the parallel trends assumption. It says that, in absence of treatment for both groups, they would be expected to evolve similarly over time. In other words, we do not expect the potential outcome to be similar, but the change of outcomes from before to after. It implies that there is no other factor that has only an impact on just one of the groups. If units differ in characteristics, they are only allowed to have a constant effect. If the effect varies with time, the paralell trends assumption is violated."
  },
  {
    "objectID": "content/toolbox/07_did.html#parallel-trends",
    "href": "content/toolbox/07_did.html#parallel-trends",
    "title": "Difference-in-Differences",
    "section": "Parallel trends",
    "text": "Parallel trends\n\nScenario A\nTo compute an estimated treatment effect, we filter the data to the two periods just around treatment and implement the formulas as in the introduction. Not surprisingly, we get an estimate that is very close to our true treatment effect.\n\n# [1.1.1] (a) Fulfillment ----\n# Scenario (a)\n# Only show last data point before and first data point after treatment.\ndf_zoom_in_a <- df %>% \n  filter(period %in% (P/2):(P/2+1)) %>%\n  mutate(store_discrete = as.factor(store)) %>% \n  rename(sales = sales_a)\n\n# Manually compute differences\n# Difference between treatment and control group BEFORE treatment\nbefore_control_a <- df_zoom_in_a %>%\n  filter(store == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_a <- df_zoom_in_a %>%\n  filter(store == 1, after == 0) %>% \n  pull(sales)\n\ndiff_before_a <- before_treatment_a - before_control_a\n\n# Difference between treatment and control group AFTER treatment\nafter_control_a <- df_zoom_in_a %>%\n  filter(store == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_a <- df_zoom_in_a %>%\n  filter(store == 1, after == 1) %>% \n  pull(sales)\n\ndiff_after_a <- after_treatment_a - after_control_a\n\n# Difference-in-differences. Unbiased estimate if parallel trends is correctly\n# assumed and there is no hidden confounding. Estimate may vary from true\n# treatment effect, as we also include some noise in the data generating \n# process.\ndiff_diff_a <- diff_after_a - diff_before_a\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_a, D)\n\n[1] \"Estimate: 1.05, True Effect: 1.00\"\n\n\nLooking at the last period before and the first period after treatment, the impact of treatment can clearly be seen. The dashed line represents the counterfactual value for the treated group, i.e. the value it would have if it had not been treated. This value is not observed, but by the paralell trends assumptions, it would have developed like the value for the untreated group.\n\n# Plot\nggplot(df_zoom_in_a, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name =\"\", \n                   breaks=c(5, 5.5, 6),\n                   labels = c(\"Before Treatment\", \n                              \"Treatment\",\n                              \"After Treatment\")) +\n  scale_y_continuous(name = \"Sales\", labels = scales::number_format(accuracy = 0.1)) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2), xend = (P/2+1),\n           y = before_treatment_a, yend = after_treatment_a - diff_diff_a,\n           linetype = \"dashed\", color = ggthemr::swatch()[2], size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_a, yend = after_treatment_a - diff_diff_a,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.98), y = after_treatment_a - (diff_diff_a / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.6, y = before_control_a + 1.1*diff_before_a + .1, \n            label = \"Counterfactual\", size = 4, \n           #angle = atan(after_control_a - before_control_a) * 180/pi\n           angle = 3) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")\n\n\n\n\n\n\n\n\n\n\nScenario B\nRepeating the steps for scenario B yields an unexpected result. The estimated treatment effect is different from what we would have expected.\n\n# [1.1.2] (b) Violation ----\n# Scenario (b)\n# Only show last data point before and first data point after treatment.\ndf_zoom_in_b <- df %>% \n  filter(period %in% (P/2):(P/2+1)) %>%\n  mutate(store_discrete = as.factor(store)) %>% \n  rename(sales = sales_b)\n\n# Manually compute differences\n# Difference between treatment and control group BEFORE treatment\nbefore_control_b <- df_zoom_in_b %>%\n  filter(store == 0, after == 0) %>% \n  pull(sales)\nbefore_treatment_b <- df_zoom_in_b %>%\n  filter(store == 1, after == 0) %>% \n  pull(sales)\ndiff_before_b <- before_treatment_b - before_control_b\n\n# Difference between treatment and control group AFTER treatment\nafter_control_b <- df_zoom_in_b %>%\n  filter(store == 0, after == 1) %>% \n  pull(sales)\nafter_treatment_b <- df_zoom_in_b %>%\n  filter(store == 1, after == 1) %>% \n  pull(sales)\ndiff_after_b <- after_treatment_b - after_control_b\n\n# Difference-in-differences. Unbiased estimate if parallel trends is correctly\n# assumed and there is no hidden confounding. Estimate varies from true\n# treatment effect due to confounding and added noise.\ndiff_diff_b <- diff_after_b - diff_before_b\nsprintf(\"Estimate: %.2f, True Effect: %.2f\", diff_diff_b, D)\n\n[1] \"Estimate: 1.45, True Effect: 1.00\"\n\n\nAgain, the picture is very similar. Having only four data points, treatment before and after and control before and after, there is no way to test the paralell trends assumption which leaves room for doubt. So how can we check whether we made a mistake or the paralell trends assumption is violated?\n\n# Plot\nggplot(df_zoom_in_b, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name =\"\", \n                     breaks=c(5, 5.5, 6),\n                     labels = c(\"Before Treatment\", \n                                \"Treatment\",\n                                \"After Treatment\")) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2), xend = (P/2+1),\n           y = before_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.98), y = after_treatment_b - (diff_diff_b / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 0.6, y = before_control_b + 1.1*diff_before_b, \n           label = \"Counterfactual\", size = 4, \n           #angle = atan(after_control_b - before_control_b) * 180/pi\n           angle = 3) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")"
  },
  {
    "objectID": "content/toolbox/07_did.html#event-study",
    "href": "content/toolbox/07_did.html#event-study",
    "title": "Difference-in-Differences",
    "section": "Event Study",
    "text": "Event Study\n\nScenario A\nMany researchers therefore try to increase the validity of their results by providing an event study. Not surprisingly (as we created the data ourselves), both groups develop same before the treatment. It cannot rule out all unobserved behavior but an event study lends credibility to the causal interpretation of treatment effects.\n\n# [1.2.1] (a) Fulfillment ----\n# Zoom out and show that parallel trend assumption is fulfilled in scenario (a)\ndf_zoom_out_a <- df %>% \n  mutate(store_discrete = as.factor(store)) %>% \n  filter(period <= (P/2 + 1)) %>% \n  rename(sales = sales_a)\n\ndiff_control <- after_control_a - before_control_a\n\nggplot(df_zoom_out_a, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name   = \"Period\", breaks = 1:(P/2+1)) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2),\n           xend = (P/2 + 1),\n           y = before_treatment_a,\n           yend = before_treatment_a + 1*(diff_control),\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_a, yend = after_treatment_a - diff_diff_a,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.78), y = after_treatment_a - (diff_diff_a / 2), \n           label = \"Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + .5, y = before_control_a + 1.1*diff_before_a - .1, \n           label = \"Counterfactual\", size = 4, \n           angle = atan(after_control_a - before_control_a) * 180/pi) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")\n\n\n\n\n\n\n\n\n\n\nScenario B\nPerforming the same steps for scenario B, you see the usefulness of an event study. Other than in scenario A, the paralell trends assumption does not seem to hold. It can be seen from the plot, that the estimated treatment effect is larger than the actual treatment effect. This is due to different trends in both groups. The treatment group has a more positive trend even without treatment and the groups would have further diverged after treatment (see green line). Some of the increase in sales after treatment is therefore attributable to this trends but not the treatment effect.\n\n# [1.2.2] (b) Violation----\n# Zoom out and show that parallel trend assumption is violated in scenario (b)\ndf_zoom_out_b <- df %>% \n  mutate(store_discrete = as.factor(store)) %>% \n  #filter(period <= (P/2 + 1)) %>% \n  rename(sales = sales_b)\n\ntreatment_period_1 <- df_zoom_out_b %>% \n  filter(store == 1, period == 1) %>% \n  pull(sales)\ntreatment_period_5 <- df_zoom_out_b %>% \n  filter(store == 1, period == P/2) %>%\n  pull(sales)\n\ndiff_treatment <- (treatment_period_5 - treatment_period_1) / (P/2)\n\nggplot(df_zoom_out_b, aes(x = period, y = sales, color = store_discrete)) +\n  geom_line(size = .95) +\n  scale_x_continuous(name   = \"Period\", breaks = 1:P) +\n  scale_y_continuous(name = \"Sales\") +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  guides(colour = guide_legend(reverse = T)) +\n  geom_vline(xintercept = P/2 + .5, color = ggthemr::swatch()[4]) + \n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_b,\n           yend = before_treatment_b + (P/2)*(after_control_b - before_control_b),\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2),\n           xend = P,\n           y = before_treatment_b,\n           yend = before_treatment_b + (P/2)*(diff_treatment),\n           linetype = \"dashed\", color = \"green\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2), xend = (P/2+1),\n           y = before_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"#00bfc4\", size = .95) +\n  annotate(geom = \"segment\", x = (P/2+1), xend = (P/2+1),\n           y = after_treatment_b, yend = after_treatment_b - diff_diff_b,\n           linetype = \"dashed\", color = \"black\") +\n  annotate(geom = \"label\", x = (P/2+.98), y = after_treatment_b - (diff_diff_b / 2),\n           label = \"Estimated Treatment effect\", size = 3) +\n  annotate(geom = \"text\", x = (P/2) + 3, y = before_control_b + 1*diff_before_b,\n           label = \"Counterfactual\", size = 4,\n           #angle = atan(after_control_b - before_control_b) * 180/pi\n           angle = 3) +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank(), legend.position = \"bottom\") +\n  ggtitle(\"Parallel Trends Assumption\")"
  },
  {
    "objectID": "content/toolbox/07_did.html#modeling",
    "href": "content/toolbox/07_did.html#modeling",
    "title": "Difference-in-Differences",
    "section": "Modeling",
    "text": "Modeling\nA more typical situation is usually that there is more than one unit in the treatment and control group. You could e.g. imagine that you are managing more than two stores and are implementing an ad campaign in a specific region.\nTo simulate such a scenario, we generate data for 3’000 stores that are split evenly into two regions. In one region, the ad campaign will be run (treatment region) and in the other there will be no campaign (control region). The variable relationships as defined in the previous section still hold.\n\n# [1.4] Linear regression ----\n# Now assume that there are more than two stores and treatment is performed\n# e.g. in a specific region which are, depending on scenario (a) and (b) \n# different.\n# Generate a bunch of samples and combine in one table. Here, we choose a higher\n# standard deviation.\nn_stores <- 3e+3\ndf_lm    <- lapply(1:n_stores, function(R) generate_data(sd = 1)) %>% bind_rows()\ndf_lm    <- df_lm %>% filter(period %in% (P/2):(P/2+1))\n\n\nScenario A\nSo how do we compute the average treatment effect? Previously in this chapter, we just used basic math calculations (particularly subtraction). But there is an easier way: we can use regression again. This is because the average treatment effect is the coefficient of the interaction of group and time.\n\\[\ny_i = \\beta_0 + \\beta_1 * Period_i + \\beta_2 * Treatment_i + \\beta_3 * (Time_i * Treatment_i) + \\epsilon_i\n\\]\n\\(Time\\) indicates whether the period is before or after the treatment and \\(Treatment\\) whether an observation was treated or not. Then, the coefficient we are interested in is \\(\\beta_3\\), because its term is only active for the treated group after treatment.\n!!! x1: maybe purchase power in region\nFor scenario A, we can see that there is no need to adjust for the covariate \\(x1\\). If you check the formulas again, your will notice that \\(x1\\) has a constant and time-invariant effect on sales and therefore it does not violate the paralell trends assumption.\nIncluding or leaving out \\(x1\\) in the regression yields the a similar unbiased estimate (close to defined true size) for our variable of interest \\(store:after\\), the parameter of interest.\n\n# [1.4.1] (a) ----\n# (a): Due to the construction of the data set, we expect interaction\n# coefficient to be significant as well as the covariate and period. However, as\n# the covariate does not have a time-varying effect, it is not a confounder and\n# interaction coefficient should be unbiased even if not adjusting for the\n# covariate.\nsummary(lm(sales_a ~ store * after , data = df_lm))\n\n\nCall:\nlm(formula = sales_a ~ store * after, data = df_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.177 -0.952  0.003  0.956  6.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  50.9620     0.0257 1981.04  < 2e-16 ***\nstore         1.0367     0.0364   28.50  < 2e-16 ***\nafter         0.2637     0.0364    7.25  4.5e-13 ***\nstore:after   0.9816     0.0514   19.08  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.4 on 11996 degrees of freedom\nMultiple R-squared:  0.284, Adjusted R-squared:  0.283 \nF-statistic: 1.58e+03 on 3 and 11996 DF,  p-value: <2e-16\n\nsummary(lm(sales_a ~ store * after + x1, data = df_lm))\n\n\nCall:\nlm(formula = sales_a ~ store * after + x1, data = df_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.947 -0.680  0.001  0.673  4.205 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 50.98165    0.01821 2799.89   <2e-16 ***\nstore        0.03882    0.02732    1.42     0.16    \nafter        0.22829    0.02575    8.86   <2e-16 ***\nx1           0.98742    0.00903  109.32   <2e-16 ***\nstore:after  0.97851    0.03642   26.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11995 degrees of freedom\nMultiple R-squared:  0.641, Adjusted R-squared:  0.641 \nF-statistic: 5.36e+03 on 4 and 11995 DF,  p-value: <2e-16\n\n\n\n\nScenario B\nIn scenario B, the effect of x1 is different because it has a time-varying effect. Therefore it violates the parallel trends assumption, leading to a biased estimate if x1 is not included (e.g. because it is unobserved).\nBecause we constructed the data set ourselves, we are able to see that the bias in fact is quite large and the treatment effect seems to include the actual treatment effect plus the effect of x1. Even with including \\(x1\\) and as a main effect and moderator, we cannot fully reconstruct the true treatment effect.\n\n# [1.4.2] (b) ----\n# (b): Due to the construction of the data set, we expect interaction coefficient\n# to be significant and accurate only when adjusting for the time-varying effect\n# of the covariate and main effects for period and covariate.\nsummary(lm(sales_b ~ store*after, data = df))\n\n\nCall:\nlm(formula = sales_b ~ store * after, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.059 -0.419  0.011  0.383  1.050 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   50.591      0.282  179.15  < 2e-16 ***\nstore          2.985      0.399    7.47  1.3e-06 ***\nafter          1.006      0.399    2.52  0.02275 *  \nstore:after    2.683      0.565    4.75  0.00022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.63 on 16 degrees of freedom\nMultiple R-squared:  0.953, Adjusted R-squared:  0.945 \nF-statistic:  109 on 3 and 16 DF,  p-value: 7.44e-11\n\nsummary(lm(sales_b ~ store*after + after*x1 + store*x1, data = df_lm))\n\n\nCall:\nlm(formula = sales_b ~ store * after + after * x1 + store * x1, \n    data = df_lm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.227 -0.684  0.003  0.683  3.646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 50.99628    0.01833 2782.65  < 2e-16 ***\nstore       -0.00499    0.03024   -0.16     0.87    \nafter        0.19101    0.02592    7.37  1.8e-13 ***\nx1           2.63604    0.01576  167.23  < 2e-16 ***\nstore:after  0.99312    0.04103   24.21  < 2e-16 ***\nafter:x1     0.35500    0.01819   19.52  < 2e-16 ***\nstore:x1     1.03268    0.01819   56.77  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 11993 degrees of freedom\nMultiple R-squared:  0.943, Adjusted R-squared:  0.943 \nF-statistic: 3.31e+04 on 6 and 11993 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/toolbox/08_iv.html",
    "href": "content/toolbox/08_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "The method we introduce in this chapter is called instrumental variables estimation (IV) and is a non-experimental methods used to identify causal effects in observational studies with unobserved confounders.\nHeavily used in economics and probably one of the most important research designs, it controls for omitted variable bias. Not being able to close the backdoor path through the omitted variable between treatment and outcome due to the omitted variable not being measured, IV introduces an additional variable, the so called instrument or instrumental variable,which affects the outcome only through the treatment variable.\nThe instrumental variable is exogenous, i.e. there is no other variable in the mode influencing the value of the instrument. Thus, it mimics an experiment by exploiting the exogenous variation in treatment due to the instrument and disregarding endogenous variation from unobserved confounders.\nIV can be illustrated using DAGs. On the left, there is a potential initial situation you could find yourself in: you want to examine the effect of \\(D\\) on $Y$, but unfortunately, there is an unobserved confounder (=omitted variable) that you would have to adjust for to identify the direct effect. As \\(U\\) is unobserved, there is no way to close the backdoor path by methods like matching, regression etc. This is where IV comes to rescue.\nAs you can see on the right, now \\(D\\) mediates between an instrument \\(Z\\) and outcome \\(Y\\). There is no direct path between \\(Z\\) and \\(Y\\) and therefore \\(Z\\) affects \\(Y\\) only through \\(D\\). For the instrument validity, there are a few assumptions that need to be fulfilled, which we discuss later in detail but summarizing, we need\n\nRelevance: \\(Z \\rightarrow D, \\,\\,\\, Cor(Z,D) \\neq 0\\) (testable)\nExcludability: \\(Z \\rightarrow D \\rightarrow Y,\\,\\, Z \\not\\to Y, \\,\\,\\, Cor(Z, Y|D) =0\\) (partly testable)\nExogeneity: \\(U \\not\\to Z, \\,\\,\\, Cor(Z, U)=0\\) (not testable)\n\n\n\n\n\n\n\n\n\n\nThat are the main requirements of IV. To go through them in-depth and also explain how to estimate the effects, we’ll go through an example in the following section.."
  },
  {
    "objectID": "content/toolbox/08_iv.html#exploration",
    "href": "content/toolbox/08_iv.html#exploration",
    "title": "Instrumental Variables",
    "section": "Exploration",
    "text": "Exploration\nFor the sake of explanation, we generate a synthetic data set with the variables \\(Z\\), \\(D\\) and \\(Y\\) as defined above. We also include the unobserved variable \\(U\\). This way, we can better explain where the bias comes from and how it affects the estimated treatment effect. The true treatment effect, which is the direct effect of \\(D\\) on \\(Y\\), is set to 1. We also add some random noise, so relationships are not perfect and what you will see is close to practice.\n\nhead(df)\n\n# A tibble: 6 × 4\n  distance program motivation    kpi\n     <dbl>   <dbl>      <dbl>  <dbl>\n1    0.741       0      0.612 -0.626\n2    0.622       1      0.588  2.44 \n3    0.638       1      0.822  2.92 \n4    0.687       0      0.588  1.93 \n5    0.535       1      0.484  0.777\n6    0.622       0      0.611  0.386\n\n\nFrom the table, you already have an idea what the data structure and types look like. But when developing our analysis strategy, we are mainly interested in relationships between the variables, so let’s have a look at the correlation matrix. Some requirements for a valid IV strategy can already be checked with it. Please also note, that we include \\(U\\) here, but in general, you do not observe \\(U\\), which is the reason why researchers came up with the idea of IV in the first place. It is just for the purpose of explanation that we include it here.\nFrom the correlation matrix, we can take a few important insights. There is a significant negative correlation between distance and program participation, which is also called first-stage and confirms the relevance of our instrument. The distance to the next training location does affect decision to participate in a training. This is assumption (4) from above.\nHaving a synthetic data set, we can also see that our instrument is uncorrelated with the unobserved variable motivation, which means there is no confounding as stated in assumption (2). Usually, however, we would not be able to test this assumption due to an unobservable variable not being observed. Moreover, there could be additional confounders. With merely statistical concepts, we cannot prove that the effect of \\(Z\\) on \\(Y\\) goes only through \\(D\\).\nCorresponding to our DAG, we also see that there both program participation and KPI are correlated with motivation, which thus opens another path between treatment and outcome. Being unable to close this path, we actually need an instrument in this situation.\n\n# Correlation matrix\ncor(df) %>% round(2)\n\n           distance program motivation   kpi\ndistance       1.00   -0.56      -0.02 -0.23\nprogram       -0.56    1.00       0.41  0.48\nmotivation    -0.02    0.41       1.00  0.30\nkpi           -0.23    0.48       0.30  1.00\n\n\nIn general, it is very useful to also plot the data to check relationships between variables. It confirms what we have seen in the correlation matrix."
  },
  {
    "objectID": "content/toolbox/08_iv.html#modeling",
    "href": "content/toolbox/08_iv.html#modeling",
    "title": "Instrumental Variables",
    "section": "Modeling",
    "text": "Modeling\n\nConfounding\nWhen you plausibly argued that your instrument is valid, you would usually perform 2SLS, which is short for Two Stage Least Squares. We’ll come to that shortly, but because we have all the data, we can also check what the bias would be in case we would not use an instrument and ignore the confounder.\nRemember, the true treatment effect is 1 in this case. Therefore, when we regress \\(Y\\) on \\(D\\) and \\(U\\), we should be able to recover this effect. And when you look at the regression output, in fact, we do. It is not exactly equal 1, but that is due to sampling noise.\n\n# First of all, let's look at the coefficients of the \"full\" (but unobservable)\n# model. It is unobservable, as it includes motivation, which in reality is \n# a variable that is very hard to collect or measure.\n# Coefficients are expected to be close to what he have defined in the data\n# generation section.\nmodel_full <- lm(kpi ~ program + motivation, data = df)\nsummary(model_full)\n\n\nCall:\nlm(formula = kpi ~ program + motivation, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.572 -0.680 -0.005  0.674  3.491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.0590     0.0564   -1.05      0.3    \nprogram       0.9898     0.0284   34.91   <2e-16 ***\nmotivation    1.1313     0.1086   10.41   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 5997 degrees of freedom\nMultiple R-squared:  0.245, Adjusted R-squared:  0.245 \nF-statistic:  973 on 2 and 5997 DF,  p-value: <2e-16\n\n\nBut what would happen if we ignored the confounder \\(motivation\\) and only regress \\(Y\\) on \\(D\\)? We do not close the backdoor path and consequently, the effect is overestimated.\nWe can see that the coefficient for program participation is higher than expected, i.e. it has an upward bias. This is because, it takes some of the variation that is actually attributable to motivation into the coefficient of program participation. This confirms the need to include an instrument to model causal effects when there is no way to include the confounder.\n\n# Modeling the data without the unobservable variable, i.e. only including \n# program participation in this case, returns a biased coefficient as the \n# relationship between program and the outcome is biased by a collider.\nmodel_biased <- lm(kpi ~ program, data = df)\nsummary(model_biased)\n\n\nCall:\nlm(formula = kpi ~ program, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.631 -0.688 -0.007  0.688  3.658 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4963     0.0185    26.9   <2e-16 ***\nprogram       1.1101     0.0261    42.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 5998 degrees of freedom\nMultiple R-squared:  0.231, Adjusted R-squared:  0.231 \nF-statistic: 1.8e+03 on 1 and 5998 DF,  p-value: <2e-16\n\n\n\n\n2SLS\nTwo Stage Least Square is the estimation technique for IV and consists, as it names suggest, of two stages. In the first stage, the treatment variable is regressed on the instrument and in the second stage, the estimated values of the first stage are used as a regressors for the outcome. It sounds a little bit confusing, so let’s write it down. Remember, \\(D\\) is our treatment, \\(Z\\) the instrument and \\(Y\\) the outcome.\nFirst stage:\n\\[\nd_i = \\gamma_0 + \\gamma_1z_i + \\nu_i\n\\]\nSecond stage:\n\\[\ny_i = \\beta_0 +\\beta_1\\widehat{d_i} +\\epsilon_i\n\\]\nLower cases indicate single observations, so \\(i\\) indicates for example the row in our data set. What is important to note, is that in the second stage, we do not use \\(d_i\\), but instead \\(\\widehat{d_i}\\). The hat indicates, that these are fitted values from the first stage.\nWe can do 2SLS manually in R, but for reasons I will get to later, it is recommended to use libraries built to run 2SLS. However, for purpose of explanation, we’ll do it also manually here.\nFirst stage: As already discussed, regress treatment variable on instrument and obtain the fitted model. The model coefficient returned by the model summary should be significant, otherwise there is reason to doubt the relevance and validity of the instrument. You can also look at the F-statistic, which should be above 10.\nHere, the instrument is highly significant. The higher the distance, the lower the likelihood of participation.\n!!! log reg?\n\n# First stage\nfirst_stage <- lm(program ~ distance, data = df)\nsummary(first_stage)\n\n\nCall:\nlm(formula = program ~ distance, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2004 -0.3334 -0.0373  0.3509  1.0397 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.4553     0.0192    75.8   <2e-16 ***\ndistance     -1.5999     0.0309   -51.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.42 on 5998 degrees of freedom\nMultiple R-squared:  0.309, Adjusted R-squared:  0.309 \nF-statistic: 2.68e+03 on 1 and 5998 DF,  p-value: <2e-16\n\n\nLet’s look at the fitted values from the first stage. The fitted values is what you get when you use the calculated coefficients and for each observation compute what the model predicts as an expected value. So, they are most likely a different from the actual values. How different they are depends on the goodness of fit of your model. This is why it is important that your instrument has a good explanatory value for the treatment variable.\n!!! Plot residuals?\n\nhist(first_stage$fitted.values)\n\n\n\n\n\n\n\n\n\nggplot(tibble(first_stage_fitted = first_stage$fitted.values), \n       aes(x = first_stage_fitted)) + \n  geom_histogram(color = \"white\", alpha = .8)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow we continue to use the fitted values from the first stage and plug it in the second stage to get the local average treatment effect. We see that the coefficient for the effect is close to one as constructed and we were able to eliminate the omitted variable bias.\n\n# Second stage\nsecond_stage <- lm(df$kpi ~ first_stage$fitted.values)\nsummary(second_stage)\n\n\nCall:\nlm(formula = df$kpi ~ first_stage$fitted.values)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.051 -0.757 -0.009  0.753  4.257 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 0.5669     0.0298    19.0   <2e-16 ***\nfirst_stage$fitted.values   0.9689     0.0521    18.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.1 on 5998 degrees of freedom\nMultiple R-squared:  0.0545,    Adjusted R-squared:  0.0543 \nF-statistic:  345 on 1 and 5998 DF,  p-value: <2e-16\n\n\nHowever, it is recommended to use functions, like e.g. iv_robust() from the estimatr package, as it yields correct standard errors. You see that the coefficient is the same but the standard errors slightly differ.\n\n# Using our instrument (distance to training location), we try to eliminate the\n# bias induced by the omitted variable. If all assumptions regarding the validity\n# of our instrument are met, the resulting coefficient should be\n# close to what we have defined above.\nlibrary(estimatr)\nmodel_iv <- iv_robust(kpi ~ program | distance, data = df)\nsummary(model_iv)\n\n\nCall:\niv_robust(formula = kpi ~ program | distance, data = df)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper   DF\n(Intercept)    0.567     0.0267    21.2 1.13e-96    0.515    0.619 5998\nprogram        0.969     0.0466    20.8 7.23e-93    0.878    1.060 5998\n\nMultiple R-squared:  0.228 ,    Adjusted R-squared:  0.227 \nF-statistic:  433 on 1 and 5998 DF,  p-value: <2e-16"
  },
  {
    "objectID": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "href": "content/fundamentals/03_caus.html#fundamental-problem-of-causal-inference",
    "title": "Causal Inference",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\nNow, let’s think again about the research question. How can we find out what the benefit of having parking spots is? Ideally, we would be able to compute the individual treatment effect (ITE) of each store \\(i\\). That means, for each store, we would know what the sales would be with and without parking spots. Then we could take the difference of those two outcomes and we would know what part of the sales would be only attributable to having parking spots. This is called the individual treatment effect (ITE):\n\\[\n\\text{ITE}_i = Y_{i1} - Y_{i0}\n\\]\n\\(Y_{i1}\\) are sales when there are parking spots at store \\(i\\) and \\(Y_{i0}\\) are sales when there are no parking spots at store \\(i\\). However, observing both outcomes is impossible.\nTo compute the individual treatment effect we would have to know the amount of sales that would have happened in case the treatment was not assigned to e.g. store \\(A\\). Not being able to observe an observation unit in both states (= with and without treatment) is called the fundamental problem of causal inference, essentially a missing data problem.\nThis is why technically the outcomes \\(Y_{i1}\\) and \\(Y_{i0}\\) are potential outcomes. To come from potential outcomes to the observed outcome, we can use the switching equation. For example for store \\(A\\):\n\\[\n\\begin{align}\nY_A &= D_AY_{A1} + (1-D_A)Y_{A0} \\\\\n&= 0*Y_{A1} + 1*Y_{A0} \\\\\n&= Y_{A0}\n\\end{align}\n\\]\nWe are able to observe \\(Y_{A0}\\), the sales for store \\(A\\) having no parking spots, but we are not able to observe \\(Y_{A1}\\), the state in which store \\(A\\) would have parking spots. But to estimate a individual causal effect, we would have to know what happens when we intervene and when we don’t intervene.\n!!! VIZ: Potential outcomes\n\\(Y_{A1}\\) and \\(Y_{A0}\\) are potential outcomes, of which the one actually happened is called factual and the one that did not happen is called counterfactual. Note, that they describe outcomes for the same unit and although we cannot observe one of them, we can still define it mathematically."
  },
  {
    "objectID": "content/fundamentals/03_caus.html#average-treatment-effect",
    "href": "content/fundamentals/03_caus.html#average-treatment-effect",
    "title": "Causal Inference",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nFor now, we will leave the ITE behind and focus on a metric that is more accessible in analyses, the average treatment effect (ATE). The average treatment effect is defined as\n\\[\n\\text{ATE} = E[Y_1 - Y_0] \\,\\,,\n\\]\nthe expected difference in outcomes under both states. So the causal effect is defined as a comparison between two states of the world, the “actual” or “factual” state compared to the never observed “counterfactual” world.\nOther forms of average treatment effects are the average treatment effect on the treated (ATT) and the average effect on the untreated (ATU).\n\\[\n\\begin{align}\nATT = E[Y_1 - Y_0|D = 1] \\\\\nATU = E[Y_1 - Y_0|D = 0]\n\\end{align}\n\\]\nNow let’s ignore the fundamental problem of causal inference for a minute and imagine the impossible scenario that we would be able to observe all outcomes for all stores for all different states. That means, we would be able to magically know the sales of each stores with and without parking spots. Just for illustration, the unobserved outcomes are crossed out, but we’ll still use them for computation.\n\n\n\n\\(i\\)\n\\(Y_{i0}\\)\n\\(Y_{i1}\\)\n\\(D_i\\)\n\\(Y_i\\)\n\\(\\text{ITE}\\)\n\n\n\n\n\\(A\\)\n135\n145\n0\n135\n+10\n\n\n\\(B\\)\n121\n125\n0\n121\n+4\n\n\n\\(C\\)\n74\n102\n1\n102\n+28\n\n\n\\(D\\)\n68\n94\n1\n94\n+26\n\n\n\nKnowing all states, we would be able to easily compute the average treatment effect by averaging the last column \\(\\Delta y\\),\n\\[\n\\text{ATE} = \\frac{1}{4}(28 + 26 + 10 + 4)= 17\n\\]\nWe can already see that for the treated stores, the ones with parking spots, the treatment effect is way higher. We can show that by calculating the average treatment effect for the treated (\\(D_i = 1\\)) and for the untreated (\\(D_i=0\\)).\n\\[\nATT = \\frac{1}{2}(28+26) = 27 \\\\\nATU = \\frac{1}{2}(10+4) = 7\n\\]\nBut again, we cannot see the table as it is shown above but instead, what we would see is the following table.\n\n\n\nStore\n\\(y_0\\)\n\\(y_1\\)\n\\(d\\)\n\\(y\\)\n\\(\\text{ITE}\\)\n\n\n\n\n\\(A\\)\n135\n-\n0\n135\n-\n\n\n\\(B\\)\n121\n-\n0\n121\n-\n\n\n\\(C\\)\n-\n102\n1\n102\n-\n\n\n\\(D\\)\n-\n94\n1\n94\n-\n\n\n\nOne idea you could come up with is to compare the mean of treated units to the mean of untreated units and take the difference as the ATE. Treated units are called the treatment group while untreated units are called control group. Knowing the true average treatment effect from our hypothetical table above, let’s see how it works.\n\\[\n\\text{ATE} = E[Y|D=1] -E[Y|D=0] = \\frac{102+94}{2} - \\frac{135+121}{2} = -30\n\\]\nThis would leave us with an average treatment effect of \\(-30\\), which is is very far away from our true estimate of \\(+27\\). In fact, it even goes in the other direction. This is why we need to be extremely careful when attempting to prove causal effects. Naive estimations and simple methods might not only under- or overestimate the effect or not identify a true effect, but they could get it even completely wrong."
  }
]