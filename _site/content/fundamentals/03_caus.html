<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-10-01">

<title>Causal Data Science for Business Analytics - Causal Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../content/fundamentals/04_dag.html" rel="next">
<link href="../../content/fundamentals/02_reg.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<link href="../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Causal Data Science for Business Analytics</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../content/index.html" aria-current="page">Content</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Causal Inference</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Fundamentals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/fundamentals/01_prob.html" class="sidebar-item-text sidebar-link">Probability Theory and Statistics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/fundamentals/02_reg.html" class="sidebar-item-text sidebar-link">Regression and Statistical Inference</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/fundamentals/03_caus.html" class="sidebar-item-text sidebar-link active">Causal Inference</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/fundamentals/04_dag.html" class="sidebar-item-text sidebar-link">Directed Acyclic Graphs</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Toolbox</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/toolbox/05_rct.html" class="sidebar-item-text sidebar-link">Randomized Controlled Trials</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/toolbox/06_match.html" class="sidebar-item-text sidebar-link">Matching and Subclassification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/toolbox/07_did.html" class="sidebar-item-text sidebar-link">Difference-in-Differences</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/toolbox/08_iv.html" class="sidebar-item-text sidebar-link">Instrumental Variables</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/toolbox/09_rdd.html" class="sidebar-item-text sidebar-link">Regression Discontinuity</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#why-causality" id="toc-why-causality" class="nav-link" data-scroll-target="#why-causality">Why causality?</a></li>
  <li><a href="#illustrative-examples" id="toc-illustrative-examples" class="nav-link" data-scroll-target="#illustrative-examples">Illustrative Examples</a></li>
  </ul></li>
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#notation-and-terminology" id="toc-notation-and-terminology" class="nav-link" data-scroll-target="#notation-and-terminology"><strong>Notation and terminology</strong></a>
  <ul class="collapse">
  <li><a href="#fundamental-problem-of-causal-inference" id="toc-fundamental-problem-of-causal-inference" class="nav-link" data-scroll-target="#fundamental-problem-of-causal-inference">Fundamental Problem of Causal Inference</a></li>
  <li><a href="#average-treatment-effect" id="toc-average-treatment-effect" class="nav-link" data-scroll-target="#average-treatment-effect">Average Treatment Effect</a></li>
  </ul></li>
  <li><a href="#bias" id="toc-bias" class="nav-link" data-scroll-target="#bias"><strong>Bias</strong></a></li>
  <li><a href="#outlook" id="toc-outlook" class="nav-link" data-scroll-target="#outlook">Outlook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Causal Inference</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 1, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="why-causality" class="level2">
<h2 class="anchored" data-anchor-id="why-causality">Why causality?</h2>
<p>Data science has gained extreme popularity in the last years and particularly in the field of machine learning, a large number of new methods and algorithms has been developed. Many of the methods are built to perform well at prediction tasks like predicting whether a customer is likely to churn, natural language processing (extracting sentiments, translating etc.), guiding self-driving cars, recognizing objects and many other applications. Those algorithms belong to the category of supervised learning and are highly data-driven (on historical data) and optimized to predict as accurate as possible. Due to increased computing power, these models have proven to be very successful in many contexts.</p>
<p>However, there are many other <strong>contexts, where prediction is not the main focus but instead making sense of the data, understanding mechanism and processes</strong> or guiding decisions and policies plays the most important role. For example, you are not only interested in whether a customer is likely to churn, but you want to know why he/she is likely to churn. Then, we find ourselves in the realm of causality. Here, many of the newer methods are likely to fail due to their prediction-centric structure</p>
<p>Instead of throwing a lot of data to a black box searching for patterns between independent variables and outcome to get a model that predicts very well, in this course, we will try to <strong>understand characteristics of the data-generating process, i.e.&nbsp;the system of cause and effect and extract useful information from the data</strong>. That is what science is about, explaining why things are happening.</p>
<p>!!! Illustration Prediction vs Explanation</p>
</section>
<section id="illustrative-examples" class="level2">
<h2 class="anchored" data-anchor-id="illustrative-examples">Illustrative Examples</h2>
<p>A simple application, where a data-driven machine learning model would fail to improve our understanding is a naive examination of relationship between hotel room prices and hotel room bookings. Imagine, having a sample of historical data about prices and number of bookings at your hand and you would train/fit a model to that data. A prediction-focused model would now look for correlations and patterns in the data and would conclude that in times of high prices there were more bookings.</p>
<p>But what can we derive from such a model? That higher prices lead to higher bookings? This is most certainly not a correct causal relationship. Because we know that is not true and it is actually the other way around. People are more likely to book when prices are low. There are other factors playing a role like for example tourist seasons, particular events or economic factors. Only if we take these other factors into account, we will be able to obtain a valid estimate of the causal effect. And being able to understand the underlying mechanism will help us in taking the right actions and decisions.</p>
<p>Another example is the direction of causation. In models based solely on correlations, we can’t be sure in which direction the causation works. A classic example is the strong correlation between roosters crowing and the sun rising. Without knowing anything about how the world works, we could come to the conclusion that the rooster causes the sun to rise. Obviously, this is wrong.</p>
<p>Based on these small examples, you should already understand the risk of relying on purely data-driven approaches. In domains, particularly in complex domains that demand a lot of theoretical consideration, data-driven approaches are not sufficient to help us in understanding and guiding our decisions. In business, management and economics, which we put our focus on, wrong conclusions might come with costly consequences. We will therefore explore how putting emphasis on causality is beneficial to business analytics and how we can move from correlation to causation.</p>
</section>
</section>
<section id="definition" class="level1">
<h1>Definition</h1>
<p>So let’s move a step back and think about what causality actually means before we find out how to estimate causal effects.</p>
<p>Hume (1993) defined a causal effect as:</p>
<p>“…if the first object had not been, the second never had existed.”</p>
<p>Mille (2010) approaches it with an example:</p>
<p>“If a person eats of a particular dish, and dies in consequence, that is, would not have died if he had not eaten it, people would be apt to say that eating of that dish was the source of his death.”</p>
<p>Both quotes show that for an action to be the cause of an outcome, the outcome needs to have happened only when there was the cause.</p>
<p>But how do you ensure that this is what happened in a study or analysis task? Unfortunately, <strong>data alone is never sufficient to make causal claims</strong>. In addition, you need to have some kind of domain knowledge to <strong>make necessary (and often non-testable) assumptions</strong>. Thus, causality goes beyond statistical modeling. Using the example from above, the actual problem is not “Are sales high when the price is high?” but rather “What happens to our sales if we decrease/increase the price?”.</p>
<p>Other questions that arise in businesses could be:</p>
<ul>
<li><p>Does the advertising campaign increase sales?</p></li>
<li><p>Does a customer loyalty program reduce customer churn?</p></li>
<li><p>Do job benefits increase the attractiveness of a company for employees?</p></li>
</ul>
<p>Differences in concepts and questions that can be answered are shown by Pearl in his <strong>ladder of causation</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/03_ladder.png" class="img-fluid figure-img" width="600"></p>
</figure>
</div>
<p>The first rung deals with associations based only on observations. Most machine learning models are at the first rung, only observing patterns and predicting the future based on the past.</p>
<p>The second rung also performs exogenous (random or as-if-random) interventions in a system. It actually “does” something by changing the potential cause in e.g.&nbsp;an experiment. It is very important to understand the difference between “seeing” (rung 1) and “doing” (rung 2) to distinguish causation from correlation.</p>
<p>On the third rung, the highest level of causality, we are imagining what would have happened, thus we are in a hypothetical world. It considers an alternate state of a past event.</p>
<p>What is important to do when dealing with causal inference, is to understand what cause and effect really means. <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, if we change <span class="math inline">\(X\)</span> but nothing else and therefore <span class="math inline">\(Y\)</span> changes. You could also imagining it as <span class="math inline">\(Y\)</span> “listening” to <span class="math inline">\(X\)</span> and adapting. A causal effect is temporarily bound, which means that the change of <span class="math inline">\(X\)</span> has to happen before <span class="math inline">\(Y\)</span>. But note, only because <span class="math inline">\(X\)</span> happens before <span class="math inline">\(Y\)</span> does not necessarily prove a causal effect.</p>
<p>In summary, to define a causal effect, we need:</p>
<ul>
<li><p><strong>Temporal sequence</strong>: For <span class="math inline">\(X\)</span> to cause <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span> has to happen before <span class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Non-spurious association</strong>: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> did not just happen by chance alone and there is no other factor that accounts for the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Concomitant variation</strong>: Only if <span class="math inline">\(X\)</span> has happened, it can cause <span class="math inline">\(Y\)</span>. Otherwise, it is no causal effect.</p></li>
</ul>
<p>There is also a difference between probabilistic and deterministic causation. Considering deterministic causation, <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> every time and there are no exemptions. However, when we talk about probabilistic causation, we don’t mean that <span class="math inline">\(X\)</span> definitely causes <span class="math inline">\(Y\)</span> but rather increases the likelihood of <span class="math inline">\(Y\)</span> happening. E.g. a drug generally increases the likelihood of fighting a specific disease but will not work in 100% of the cases. So in almost all applications in business, social sciences, economics and many other domains, causal inference usually deals with probabilistic causation. Also in this course, we will focus only on probabilistic causation.</p>
</section>
<section id="notation-and-terminology" class="level1">
<h1><strong>Notation and terminology</strong></h1>
<p>Now, we will go through an example to introduce the notation and terminology. Both is crucial to understand issues of causality and find solutions to causal problems.</p>
<p>Imagine you are a business owner of four stores and you want to investigate if there is a benefit of having own parking spots in front of your store. And if there is a benefit, you would also like to know what size it has.</p>
<p>At stores <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, which are located in the city center, you don’t have any parking spots but at stores <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> located outside the city you do. Except for location and parking spots, the stores are very similar, i.e.&nbsp;they sell exactly the same goods, have the same size etc. So our research question is:</p>
<p><strong>How do sales increase/decrease, when a store is equipped with parking spots?</strong></p>
<p>In causal inference, we are always interested in analyzing the effect of a specific variable on the <strong>outcome variable</strong>. This specific variable is called <strong>treatment variable</strong> and to understand the effect of this treatment variable, it needs to vary across <strong>observations units</strong>. Otherwise, if the treatment variable was constant across all observation units, we could not see how changing it translates to a change in the outcome. <strong>Manipulating the treatment variable is called treatment or intervention</strong>. It might remind you of a clinical setting, but in causal inference the term has a very general meaning and a treatment can be a lot of things.</p>
<p>Applied to our example:</p>
<ul>
<li><p><strong>Observation unit</strong>: stores</p></li>
<li><p><strong>Treatment variable</strong>: parking spots</p></li>
<li><p><strong>Outcome variable</strong>: sales</p></li>
</ul>
<p>For each store <span class="math inline">\(i\)</span>, we denote whether it has parking spots with <span class="math inline">\(D_i\)</span>, the treatment assignment:</p>
<p><span class="math display">\[
D_i=\begin{cases}1 \ \text{if unit i received treatment}\\0 \ \text{otherwise}\\\end{cases}
\]</span></p>
<p>Here, we focus on a binary variable, but treatment could also be assigned for different levels or sizes (imagine a dose of medicine for example). But here, we just say the variable takes on a value of 1 if a unit receives the treatment and 0 if it does not. A store with <span class="math inline">\(D_i = 1\)</span> has parking spots while a store with <span class="math inline">\(D_i=0\)</span> has no parking spots.</p>
<section id="fundamental-problem-of-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-problem-of-causal-inference">Fundamental Problem of Causal Inference</h2>
<p>Now, let’s think again about the research question. How can we find out what the benefit of having parking spots is? Ideally, we would be able to compute the individual treatment effect (ITE) of each store <span class="math inline">\(i\)</span>. That means, for each store, we would know what the sales would be with and without parking spots. Then we could take the difference of those two outcomes and we would know what part of the sales would be only attributable to having parking spots. This is called the <strong>individual treatment effect (ITE):</strong></p>
<p><span class="math display">\[
\text{ITE}_i = Y_{i1} - Y_{i0}
\]</span></p>
<p><span class="math inline">\(Y_{i1}\)</span> are sales when there are parking spots at store <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_{i0}\)</span> are sales when there are no parking spots at store <span class="math inline">\(i\)</span>. However, observing both outcomes is impossible.</p>
<p>To compute the individual treatment effect we would have to know the amount of sales that would have happened in case the treatment was not assigned to e.g.&nbsp;store <span class="math inline">\(A\)</span>. Not being able to observe an observation unit in both states (= with and without treatment) is called the <strong>fundamental problem of causal inference</strong>, essentially a missing data problem.</p>
<p>This is why technically the outcomes <span class="math inline">\(Y_{i1}\)</span> and <span class="math inline">\(Y_{i0}\)</span> are <strong>potential outcomes</strong>. To come from potential outcomes to the <strong>observed outcome</strong>, we can use the <strong>switching equation</strong>. For example for store <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\begin{align}
Y_A &amp;= D_AY_{A1} + (1-D_A)Y_{A0} \\
&amp;= 0*Y_{A1} + 1*Y_{A0} \\
&amp;= Y_{A0}
\end{align}
\]</span></p>
<p>We are able to observe <span class="math inline">\(Y_{A0}\)</span>, the sales for store <span class="math inline">\(A\)</span> having no parking spots, but we are not able to observe <span class="math inline">\(Y_{A1}\)</span>, the state in which store <span class="math inline">\(A\)</span> would have parking spots. But to estimate a individual causal effect, we would have to know what happens when we intervene and when we don’t intervene.</p>
<p>!!! VIZ: Potential outcomes</p>
<p><span class="math inline">\(Y_{A1}\)</span> and <span class="math inline">\(Y_{A0}\)</span> are <strong>potential outcomes</strong>, of which the one actually happened is called <strong>factual</strong> and the one that did not happen is called <strong>counterfactual</strong>. Note, that they describe outcomes for the same unit and although we cannot observe one of them, we can still define it mathematically.</p>
</section>
<section id="average-treatment-effect" class="level2">
<h2 class="anchored" data-anchor-id="average-treatment-effect">Average Treatment Effect</h2>
<p>For now, we will leave the ITE behind and focus on a metric that is more accessible in analyses, the <strong>average treatment effect (ATE)</strong>. The average treatment effect is defined as</p>
<p><span class="math display">\[
\text{ATE} = E[Y_1 - Y_0] \,\,,
\]</span></p>
<p>the expected difference in outcomes under both states. So the causal effect is defined as a comparison between two states of the world, the “actual” or “factual” state compared to the never observed “counterfactual” world.</p>
<p>Other forms of average treatment effects are the <strong>average treatment effect on the treated (ATT)</strong> and the <strong>average effect on the untreated (ATU)</strong>.</p>
<p><span class="math display">\[
\begin{align}
ATT = E[Y_1 - Y_0|D = 1] \\
ATU = E[Y_1 - Y_0|D = 0]
\end{align}
\]</span></p>
<p>Now let’s ignore the fundamental problem of causal inference for a minute and imagine the impossible scenario that we would be able to observe all outcomes for all stores for all different states. That means, we would be able to magically know the sales of each stores with and without parking spots. Just for illustration, the unobserved outcomes are crossed out, but we’ll still use them for computation.</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(Y_{i0}\)</span></th>
<th><span class="math inline">\(Y_{i1}\)</span></th>
<th><span class="math inline">\(D_i\)</span></th>
<th><span class="math inline">\(Y_i\)</span></th>
<th><span class="math inline">\(\text{ITE}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td>135</td>
<td><del>145</del></td>
<td>0</td>
<td>135</td>
<td><strong>+10</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(B\)</span></td>
<td>121</td>
<td><del>125</del></td>
<td>0</td>
<td>121</td>
<td><strong>+4</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(C\)</span></td>
<td><del>74</del></td>
<td>102</td>
<td>1</td>
<td>102</td>
<td><strong>+28</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(D\)</span></td>
<td><del>68</del></td>
<td>94</td>
<td>1</td>
<td>94</td>
<td><strong>+26</strong></td>
</tr>
</tbody>
</table>
<p>Knowing all states, we would be able to easily compute the average treatment effect by averaging the last column <span class="math inline">\(\Delta y\)</span>,</p>
<p><span class="math display">\[
\text{ATE} = \frac{1}{4}(28 + 26 + 10 + 4)= 17
\]</span></p>
<p>We can already see that for the treated stores, the ones with parking spots, the treatment effect is way higher. We can show that by calculating the average treatment effect for the treated (<span class="math inline">\(D_i = 1\)</span>) and for the untreated (<span class="math inline">\(D_i=0\)</span>).</p>
<p><span class="math display">\[
ATT = \frac{1}{2}(28+26) = 27 \\
ATU = \frac{1}{2}(10+4) = 7
\]</span></p>
<p>But again, we cannot see the table as it is shown above but instead, what we would see is the following table.</p>
<table class="table">
<thead>
<tr class="header">
<th>Store</th>
<th><span class="math inline">\(y_0\)</span></th>
<th><span class="math inline">\(y_1\)</span></th>
<th><span class="math inline">\(d\)</span></th>
<th><span class="math inline">\(y\)</span></th>
<th><span class="math inline">\(\text{ITE}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td>135</td>
<td>-</td>
<td>0</td>
<td>135</td>
<td>-</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B\)</span></td>
<td>121</td>
<td>-</td>
<td>0</td>
<td>121</td>
<td>-</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(C\)</span></td>
<td>-</td>
<td>102</td>
<td>1</td>
<td>102</td>
<td>-</td>
</tr>
<tr class="even">
<td><span class="math inline">\(D\)</span></td>
<td>-</td>
<td>94</td>
<td>1</td>
<td>94</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>One idea you could come up with is to compare the mean of treated units to the mean of untreated units and take the difference as the ATE. Treated units are called the <strong>treatment group</strong> while untreated units are called <strong>control group</strong>. Knowing the true average treatment effect from our hypothetical table above, let’s see how it works.</p>
<p><span class="math display">\[
\text{ATE} = E[Y|D=1] -E[Y|D=0] = \frac{102+94}{2} - \frac{135+121}{2} = -30
\]</span></p>
<p>This would leave us with an average treatment effect of <span class="math inline">\(-30\)</span>, which is is very far away from our true estimate of <span class="math inline">\(+27\)</span>. In fact, it even goes in the other direction. This is why we need to be extremely careful when attempting to prove causal effects. Naive estimations and simple methods might not only under- or overestimate the effect or not identify a true effect, but they could get it even completely wrong.</p>
</section>
</section>
<section id="bias" class="level1">
<h1><strong>Bias</strong></h1>
<p>But what is the reason for this difference and reversing of signs? When we think again about the problem description, there was one more detail we haven’t really cared about so far. It was said that two of the stores are located in the city center and two stores are located outside the city center. What difference could that make for the potential outcomes and the individual treatment effect?</p>
<p>If we think about it, stores outside the city are more dependent on having parking spots as there are no people just walking by and entering spontaneously. In the city, there are people on their shopping trip walking and not by car, so there is no need for a parking sport. Customers visiting stores outside the city center are more likely to have planned their trip and come by car. Also, stores in the city center are more likely to have more customers and therefore more sales.</p>
<p>So there is another factor, namely the location, that plays an important role that we did not include in our analysis which biases our estimate. Such variables are called <strong>confounders</strong> and in many scenarios, they are completely unobserved. In this case, <strong>treated and untreated stores are not comparable</strong>. In potential outcomes notation, we can say that <span class="math inline">\(Y_0\)</span> is different for treated and untreated stores due to their different location causing a different level of sales in the untreated status.</p>
<p>This means, that the <strong>negative association between parking and sales that we observe does not translate into causation</strong>. Mathematically, we can prove that our naive estimate of the ATE can be biased when <span class="math inline">\(Y_0\)</span> differs for both treated and untreated group as we just reasoned. Note that <span class="math inline">\(E[Y_1 - Y_0]\)</span> is the causal average treatment effect.</p>
<p><span class="math display">\[
\begin{align}
E[Y_1 - Y_0] &amp;= E[Y|D=1] - E[Y|D=0] \\ &amp;= E[Y_1|D=1] - E[Y_0|D=0] + E[Y_0|D=1] - E[Y_0|D=1] \\
&amp;= \underbrace{E[Y_1 - Y_0|D=1]}_{ATT} + \underbrace{\{ E[Y_0|D=1] - E[Y_0|D=0] \}}_{BIAS}
\end{align}
\]</span></p>
<p>The formula shows where the <strong>bias</strong> comes from: the <strong>difference in outcomes for both groups had they not been treated</strong>. Mathematically, it looks simple to arrive at an unbiased estimate, we just have to make sure the last part of the formula equals zero: <span class="math inline">\(E[Y_0|D=0] = E[Y_0|D=0]\)</span></p>
<p>But how do we achieve it in practice? How can we go from association to causation by eliminating the bias? We need treatment and control group to be comparable before treatment. This also implies that the ATT is equal to the ATE and the difference in means is equal to the causal effect we want to estimate.</p>
</section>
<section id="outlook" class="level1">
<h1>Outlook</h1>
<p>The most obvious way to achieve similarity between treatment and control group is randomization. Each observation unit needs to be randomly assigned to either control or treatment group, i.e.&nbsp;in our example, we would have to physically randomize whether a store has parking spots or if it has not. The current allocation of parking spots is not random, as the stores outside the city are more likely to have parking spots. There are also other methods that do not require physical randomization which we will get to learn in the following chapters.</p>
<p>But what you should take from this chapter is the problem of confusing statistical estimates with causal estimates. Understanding this problem is essential for understanding the need to find tools and instruments to make treatment and control groups comparable and eliminate biases.</p>
<p>There are very few scenarios where we can actually be confident not to take measures against biases but in most cases, especially when dealing with observational data, biases will arise. In business and management science, economic actors almost always attempt to achieve some optima, which is ultimately based on their potential outcomes and thus rendering simple comparisons improper.</p>
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
If you’re interested: evolution of causal inference.
</div>
</div>
<div class="callout-body-container callout-body">
<p>The history of causal inference dates back to Splawa-Neyman (1923) who proposed the physical randomization of treatments to units as a basis for causal inference and developed a the powerful potential outcomes notation. Fisher (1925) also suggested the explicit use of randomization in experimental design to obtain valid causal estimates. However, it was in 1974 when Rubin promoted the topic again and its importance in social sciences begun to grow.</p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../content/fundamentals/02_reg.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Regression and Statistical Inference</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../content/fundamentals/04_dag.html" class="pagination-link">
        <span class="nav-page-text">Directed Acyclic Graphs</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>